var tipuesearch = {"pages":[{"text":"ParaGEMS A parallel math library for discrete exterior calculus, along with miniApps for various applications in science and engineering. Developer Info Pieter Boom PDRA in the Mechanics and Physics of Solids (MaPoS) research group at the University of Manchester Department of Mechanical, Aerospace and Civil Engineering (MACE)","tags":"home","loc":"index.html","title":" ParaGEMS "},{"text":"Miniapp to to reorder nodes file for 'better' partitioning/load balancing Contents Programs simpleLoadBalance Source Code simpleLoadBalance.f90 Source Code ! !=============================================================================== !-- Simple Load Balancing App !> Miniapp to to reorder nodes file for 'better' partitioning/load balancing !=============================================================================== !/****p* Tools/simpleLoadBalance !* SYNOPSIS PROGRAM simpleLoadBalance !* PURPOSE !*   Miniapp to to reorder nodes file for 'better' partitioning/load balancing !* ASSUMPTION !*   Mesh file exists (TetGen tetrahedral mesh) !* SIDE EFFECTS !*   Mesh files reordered for 'better' partitioning/load balancing !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Miniapp to to reorder nodes file for 'better' partitioning/load balancing !=============================================================================== !-- implicit none -- IMPLICIT NONE !-- real precision -- INTEGER , PARAMETER :: iwp = SELECTED_REAL_KIND ( 15 , 300 ) !-- local variables -- CHARACTER ( LEN = 128 ) :: mesh_prefix ! file name prefix CHARACTER ( LEN = 128 ) :: fname ! file name INTEGER :: unit = 10 ! IO unit INTEGER :: junk ! junk IO variable INTEGER :: offset ! junk IO variable INTEGER :: ie , je , ke , kn ! loop indices INTEGER , ALLOCATABLE :: elems (:,:) ! node indices of elements REAL ( KIND = iwp ), ALLOCATABLE :: nodes (:,:) ! xyz coordinates of nodes REAL ( KIND = iwp ), ALLOCATABLE :: centers (:,:) ! xyz coordinates of element baryceneters INTEGER :: num_elm , num_node ! number of elements and nodes INTEGER :: ier ! error status variable REAL ( KIND = iwp ), ALLOCATABLE :: work (:,:) ! work array for merge sort INTEGER , ALLOCATABLE :: indx (:) ! indx !=============================================================================== ! MAIN EXECUTION !=============================================================================== !-- write miniapp details -- write ( * , * ) 'ParaGEMS utility - reorder_ele: reorder .ele file for better partition/load balancing' !-- get file prefix -- write ( * , * ) 'What is the mesh prefix for the .node and .ele files?' read ( * , * ) mesh_prefix !-- open node file and check for errors -- fname = trim ( mesh_prefix ) // \".node\" write ( * , * ) \"opening file: \" , fname OPEN ( unit , FILE = fname , STATUS = 'OLD' , ACTION = 'READ' , IOSTAT = ier ) IF ( ier /= 0 ) THEN WRITE ( * , '(A,A,A,I5)' ) 'Error opening ' , trim ( fname ), ': errcode = ' , ier STOP END IF !-- read header -- write ( * , * ) \"reading file: \" , fname READ ( unit , * ) num_node , kn !-- get offset -- READ ( unit , * ) offset BACKSPACE ( unit ) !-- read data -- ALLOCATE ( nodes ( num_node , kn + 1 )) DO ie = 1 , num_node READ ( unit , * ) nodes ( ie ,:) END DO !-- close file -- CLOSE ( unit ) !-- sort nodes -- write ( * , * ) \"sorting nodes\" ALLOCATE ( work ( num_node , kn + 1 )) CALL merge_sort_rows ( nodes , num_node , 2 , kn + 1 , work ) DEALLOCATE ( work ) !-- rename files -- fname = trim ( mesh_prefix ) // \".node\" write ( * , * ) \"renaming file: \" , fname CALL RENAME ( trim ( mesh_prefix ) // \".node\" , trim ( mesh_prefix ) // \".node.old\" , ier ) IF ( ier /= 0 ) THEN WRITE ( * , '(A,A,A,I5)' ) 'Error renaming ' , trim ( mesh_prefix ) // \".node\" , ': errcode = ' , ier STOP END IF !-- write new node file -- fname = trim ( mesh_prefix ) // \".node\" write ( * , * ) \"opening file: \" , fname OPEN ( unit , FILE = fname , STATUS = 'UNKNOWN' , ACTION = 'WRITE' , IOSTAT = ier ) IF ( ier /= 0 ) THEN WRITE ( * , '(A,A,A,I5)' ) 'Error opening ' , trim ( fname ), ': errcode = ' , ier STOP END IF !-- write header -- write ( * , * ) \"writing file: \" , fname WRITE ( unit , * ) num_node , kn , 0 , 0 !-- write data -- DO ie = 1 , num_node write ( * , * ) ie , nodes ( ie , 2 : kn + 1 ) WRITE ( unit , * ) ie , nodes ( ie , 2 : kn + 1 ) END DO !-- get inverse node sort index -- write ( * , * ) \"inverse sort\" ALLOCATE ( indx ( num_node )) DO ie = 1 , num_node indx ( NINT ( nodes ( ie , 1 ))) = ie END DO !-- open element file and check for errors -- fname = trim ( mesh_prefix ) // \".ele\" write ( * , * ) \"opening file: \" , fname OPEN ( unit , FILE = fname , STATUS = 'OLD' , ACTION = 'READ' , IOSTAT = ier ) IF ( ier /= 0 ) THEN WRITE ( * , '(A,A,A,I5)' ) 'Error opening ' , trim ( fname ), ': errcode = ' , ier STOP END IF !-- read header -- write ( * , * ) \"reading file: \" , fname READ ( unit , * ) num_elm , ke !-- read data -- ALLOCATE ( elems ( num_elm , ke )) DO ie = 1 , num_elm READ ( unit , * ) junk , elems ( ie ,:) END DO IF ( offset /= 1 ) elems = elems - ( offset - 1 ) !-- close file -- CLOSE ( unit ) !-- update element indicies -- write ( * , * ) \"index update\" DO ie = 1 , num_elm DO je = 1 , ke elems ( ie , je ) = indx ( elems ( ie , je )) END DO END DO !-- rename files -- write ( * , * ) \"renaming file: \" , fname CALL RENAME ( trim ( mesh_prefix ) // \".ele\" , trim ( mesh_prefix ) // \".ele.old\" , ier ) IF ( ier /= 0 ) THEN WRITE ( * , '(A,A,A,I5)' ) 'Error renaming ' , trim ( mesh_prefix ) // \".ele\" , ': errcode = ' , ier STOP END IF !-- write new ele file -- fname = trim ( mesh_prefix ) // \".ele\" write ( * , * ) \"opening file: \" , fname OPEN ( unit , FILE = fname , STATUS = 'UNKNOWN' , ACTION = 'WRITE' , IOSTAT = ier ) IF ( ier /= 0 ) THEN WRITE ( * , '(A,A,A,I5)' ) 'Error opening ' , trim ( fname ), ': errcode = ' , ier STOP END IF !-- write header -- write ( * , * ) \"writing file: \" , fname WRITE ( unit , * ) num_elm , ke , 0 !-- write data -- DO ie = 1 , num_elm WRITE ( unit , * ) ie , elems ( ie ,:) END DO !-- close file  and clean up -- write ( * , * ) \"close and clean\" CLOSE ( unit ) DEALLOCATE ( elems ) !-- open element file and check for errors -- fname = trim ( mesh_prefix ) // \".face\" write ( * , * ) \"opening file: \" , fname OPEN ( unit , FILE = fname , STATUS = 'OLD' , ACTION = 'READ' , IOSTAT = ier ) IF ( ier /= 0 ) THEN WRITE ( * , '(A,A,A,I5)' ) 'Error opening ' , trim ( fname ), ': errcode = ' , ier STOP END IF !-- read header -- write ( * , * ) \"reading file: \" , fname READ ( unit , * ) num_elm , ke ke = 3 !-- read data -- ALLOCATE ( elems ( num_elm , ke + 1 )) DO ie = 1 , num_elm READ ( unit , * ) junk , elems ( ie ,:) END DO IF ( offset /= 1 ) elems (:, 1 : ke ) = elems (:, 1 : ke ) - ( offset - 1 ) !-- close file -- CLOSE ( unit ) !-- update element indicies -- write ( * , * ) \"index update\" DO ie = 1 , num_elm DO je = 1 , ke elems ( ie , je ) = indx ( elems ( ie , je )) END DO END DO !-- rename files -- write ( * , * ) \"renaming file: \" , fname CALL RENAME ( trim ( mesh_prefix ) // \".face\" , trim ( mesh_prefix ) // \".face.old\" , ier ) IF ( ier /= 0 ) THEN WRITE ( * , '(A,A,A,I5)' ) 'Error renaming ' , trim ( mesh_prefix ) // \".face\" , ': errcode = ' , ier STOP END IF !-- write new ele file -- fname = trim ( mesh_prefix ) // \".face\" write ( * , * ) \"opening file: \" , fname OPEN ( unit , FILE = fname , STATUS = 'UNKNOWN' , ACTION = 'WRITE' , IOSTAT = ier ) IF ( ier /= 0 ) THEN WRITE ( * , '(A,A,A,I5)' ) 'Error opening ' , trim ( fname ), ': errcode = ' , ier STOP END IF !-- write header -- write ( * , * ) \"writing file: \" , fname WRITE ( unit , * ) num_elm , 1 !-- write data -- DO ie = 1 , num_elm WRITE ( unit , * ) ie , elems ( ie ,:) END DO !-- close file  and clean up -- write ( * , * ) \"close and clean\" CLOSE ( unit ) DEALLOCATE ( elems ) !-- open element file and check for errors -- fname = trim ( mesh_prefix ) // \".edge\" write ( * , * ) \"opening file: \" , fname OPEN ( unit , FILE = fname , STATUS = 'OLD' , ACTION = 'READ' , IOSTAT = ier ) IF ( ier /= 0 ) THEN WRITE ( * , '(A,A,A,I5)' ) 'Error opening ' , trim ( fname ), ': errcode = ' , ier STOP END IF !-- read header -- write ( * , * ) \"reading file: \" , fname READ ( unit , * ) num_elm , ke ke = 2 !-- read data -- ALLOCATE ( elems ( num_elm , ke + 1 )) DO ie = 1 , num_elm READ ( unit , * ) junk , elems ( ie ,:) END DO IF ( offset /= 1 ) elems (:, 1 : ke ) = elems (:, 1 : ke ) - ( offset - 1 ) !-- close file -- CLOSE ( unit ) !-- update element indicies -- write ( * , * ) \"index update\" DO ie = 1 , num_elm DO je = 1 , ke elems ( ie , je ) = indx ( elems ( ie , je )) END DO END DO !-- rename files -- write ( * , * ) \"renaming file: \" , fname CALL RENAME ( trim ( mesh_prefix ) // \".edge\" , trim ( mesh_prefix ) // \".edge.old\" , ier ) IF ( ier /= 0 ) THEN WRITE ( * , '(A,A,A,I5)' ) 'Error renaming ' , trim ( mesh_prefix ) // \".edge\" , ': errcode = ' , ier STOP END IF !-- write new ele file -- fname = trim ( mesh_prefix ) // \".edge\" write ( * , * ) \"opening file: \" , fname OPEN ( unit , FILE = fname , STATUS = 'UNKNOWN' , ACTION = 'WRITE' , IOSTAT = ier ) IF ( ier /= 0 ) THEN WRITE ( * , '(A,A,A,I5)' ) 'Error opening ' , trim ( fname ), ': errcode = ' , ier STOP END IF !-- write header -- write ( * , * ) \"writing file: \" , fname WRITE ( unit , * ) num_elm , 1 !-- write data -- DO ie = 1 , num_elm WRITE ( unit , * ) ie , elems ( ie ,:) END DO !-- close file  and clean up -- write ( * , * ) \"close and clean\" CLOSE ( unit ) DEALLOCATE ( elems ) !-- clean up -- write ( * , * ) \"clean\" DEALLOCATE ( nodes , indx ) !=============================================================================== CONTAINS !=============================================================================== ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! /****s*/ /src/modules/math_mod|int_merge_sort_rows RECURSIVE SUBROUTINE merge_sort_rows ( A , length , irow , frow , work ) ! ! PURPOSE:  merge sort rows of array in ascending order ! ! PRE:      rows are already sorted in ascending order ! ! UPDATES:  created (PDB) :: 2019/08/23 ! ! ADAPTED FROM: https://rosettacode.org/wiki/Sorting_algorithms/Merge_sort#Fortran ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- REAL ( KIND = iwp ), INTENT ( INOUT ) :: A (:,:) INTEGER , INTENT ( IN ) :: length INTEGER , INTENT ( IN ) :: irow , frow REAL ( KIND = iwp ), INTENT ( INOUT ) :: work (:,:) !-- local variables -- INTEGER :: half INTEGER :: ir , i !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- sort directly if there are two elements -- !-- NOTE: if there is only one row, then it is already sorted! -- IF ( length == 2 ) THEN !-- if out of order, then swap DO ir = irow , frow IF ( A ( 1 , ir ) > A ( 2 , ir )) THEN work ( 1 ,:) = A ( 1 ,:) A ( 1 ,:) = A ( 2 ,:) A ( 2 ,:) = work ( 1 ,:) EXIT ELSEIF ( A ( 1 , ir ) < A ( 2 , ir )) THEN EXIT END IF END DO !-- for more than two rows, split the array and call merge sort on each half -- ELSEIF ( length > 2 ) THEN !-- compute array midpoint -- half = ( length + 1 ) / 2 !-- merge sort half arrays -- CALL merge_sort_rows ( A (: half ,:), half , irow , frow , work ) CALL merge_sort_rows ( A ( half + 1 :,:), length - half , irow , frow , work ) !-- merge sorted half arrays -- DO ir = irow , frow IF ( A ( half , ir ) > A ( half + 1 , ir )) THEN work ( 1 : half ,:) = A ( 1 : half ,:) CALL merge_rows ( work ( 1 : half ,:), A ( half + 1 : length ,:), A , irow , frow ) EXIT ELSEIF ( A ( half , ir ) < A ( half + 1 , ir )) THEN EXIT END IF END DO END IF END SUBROUTINE ! math_mod|int_merge_sort_rows !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! /****s*/ /src/modules/math_mod|int_merge_rows SUBROUTINE merge_rows ( A , B , C , irow , frow ) ! ! PURPOSE:  merge rows of an array in ascending order ! ! PRE:      rows are already sorted in ascending order ! ! UPDATES:  created (PDB) :: 2019/08/23 ! ! ADAPTED FROM: https://rosettacode.org/wiki/Sorting_algorithms/Merge_sort#Fortran ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- REAL ( KIND = iwp ), INTENT ( IN ) :: A (:,:), B (:,:) REAL ( KIND = iwp ), INTENT ( INOUT ) :: C (:,:) INTEGER , INTENT ( IN ) :: irow , frow !-- local variables -- LOGICAL :: is_le INTEGER :: i , j , k INTEGER :: ir !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- dimension check -- IF ( SIZE ( A , DIM = 1 ) + SIZE ( B , DIM = 1 ) > SIZE ( C , DIM = 1 )) THEN STOP END IF !-- loop for all elements of merged array -- i = 1 ; j = 1 DO k = 1 , SIZE ( C , DIM = 1 ) IF ( i <= SIZE ( A , DIM = 1 ) . and . j <= SIZE ( B , DIM = 1 )) THEN is_le = . TRUE . DO ir = irow , frow IF ( A ( i , ir ) < B ( j , ir )) THEN EXIT ELSEIF ( A ( i , ir ) > B ( j , ir )) THEN is_le = . FALSE . EXIT END IF END DO IF ( is_le ) THEN C ( k ,:) = A ( i ,:) i = i + 1 ELSE C ( k ,:) = B ( j ,:) j = j + 1 END IF ELSEIF ( i <= SIZE ( A , DIM = 1 )) THEN C ( k ,:) = A ( i ,:) i = i + 1 ELSEIF ( j <= SIZE ( B , DIM = 1 )) THEN C ( k ,:) = B ( j ,:) j = j + 1 END IF END DO END SUBROUTINE ! math_mod|int_merge_rows !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! END PROGRAM ! simpleLoadBalance !=============================================================================== !","tags":"","loc":"sourcefile/simpleloadbalance.f90.html","title":"simpleLoadBalance.f90 – ParaGEMS"},{"text":"Driver routine for ParaGEMS' unit tests This file depends on sourcefile~~paragems_unit.f90~~EfferentGraph sourcefile~paragems_unit.f90 paragems_unit.f90 sourcefile~common_mod.f90 common_mod.f90 sourcefile~paragems_unit.f90:->sourcefile~common_mod.f90: sourcefile~test_common_mod.f90 test_common_mod.f90 sourcefile~paragems_unit.f90:->sourcefile~test_common_mod.f90: sourcefile~test_common_mod.f90:->sourcefile~common_mod.f90: Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Programs paragems_unit Source Code paragems_unit.f90 Source Code ! !=============================================================================== !-- ParaGEMS Unit testing !> Driver routine for ParaGEMS' unit tests !=============================================================================== !/****p* tests/paragems_unit !* SYNOPSIS PROGRAM paragems_unit !* PURPOSE !*   Driver routine for ParaGEMS' unit tests !* ASSUMPTION !* !* INCLUDES !*   Name                    Purpose !*   common_mod              variable definitions !* SIDE EFFECTS !* !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Driver routine for ParaGEMS' unit tests !=============================================================================== !----------------------------------------------------------------------------- ! modules and implicit none !----------------------------------------------------------------------------- USE common_mod ; USE test_common_mod IMPLICIT NONE !--------------------------------------------------------------------- ! local variables !--------------------------------------------------------------------- INTEGER :: cnt = 0 ! test counter !=============================================================================== ! MAIN EXECUTION !=============================================================================== !----------------------------------------------------------------------- !  1. common_test !----------------------------------------------------------------------- CALL test_vars ( cnt ) END PROGRAM ! paragems_unit !=============================================================================== !","tags":"","loc":"sourcefile/paragems_unit.f90.html","title":"paragems_unit.f90 – ParaGEMS"},{"text":"Miniapp to solve one-field Darcy flow in parallel using PETSc KSP This file depends on sourcefile~~darcy_1f.f90~~EfferentGraph sourcefile~darcy_1f.f90 darcy_1f.f90 sourcefile~common_mod.f90 common_mod.f90 sourcefile~darcy_1f.f90:->sourcefile~common_mod.f90: sourcefile~partition_mod.f90 partition_mod.f90 sourcefile~darcy_1f.f90:->sourcefile~partition_mod.f90: sourcefile~darcy_mod.f90 darcy_mod.f90 sourcefile~darcy_1f.f90:->sourcefile~darcy_mod.f90: sourcefile~solver_mod.f90 solver_mod.f90 sourcefile~darcy_1f.f90:->sourcefile~solver_mod.f90: sourcefile~mpi_mod.f90 mpi_mod.f90 sourcefile~darcy_1f.f90:->sourcefile~mpi_mod.f90: sourcefile~partition_mod.f90:->sourcefile~common_mod.f90: sourcefile~partition_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~dec_mod.f90 dec_mod.f90 sourcefile~partition_mod.f90:->sourcefile~dec_mod.f90: sourcefile~io_mod.f90 io_mod.f90 sourcefile~partition_mod.f90:->sourcefile~io_mod.f90: sourcefile~math_mod.f90 math_mod.f90 sourcefile~partition_mod.f90:->sourcefile~math_mod.f90: sourcefile~darcy_mod.f90:->sourcefile~common_mod.f90: sourcefile~darcy_mod.f90:->sourcefile~solver_mod.f90: sourcefile~darcy_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~darcy_mod.f90:->sourcefile~io_mod.f90: sourcefile~solver_mod.f90:->sourcefile~common_mod.f90: sourcefile~solver_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~mpi_mod.f90:->sourcefile~common_mod.f90: sourcefile~dec_mod.f90:->sourcefile~common_mod.f90: sourcefile~dec_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~dec_mod.f90:->sourcefile~math_mod.f90: sourcefile~io_mod.f90:->sourcefile~common_mod.f90: sourcefile~io_mod.f90:->sourcefile~solver_mod.f90: sourcefile~io_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~io_mod.f90:->sourcefile~dec_mod.f90: sourcefile~math_mod.f90:->sourcefile~common_mod.f90: sourcefile~math_mod.f90:->sourcefile~mpi_mod.f90: Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Programs darcy_1f Source Code darcy_1f.f90 Source Code ! !=============================================================================== !-- Darcy flow - one-field formulation !> Miniapp to solve one-field Darcy flow in parallel using PETSc KSP !=============================================================================== !/****p* programs|darcy_flow/darcy_1f !* SYNOPSIS PROGRAM darcy_1f !* PURPOSE !*   Miniapp to solve one-field Darcy flow in parallel using PETSc KSP !* ASSUMPTION !*   Mesh file exists (TetGen tetrahedral mesh) !*   Input file (default: input.param) !* INCLUDES !*   Name                    Purpose !*   common_mod              variable definitions !*   mpi_mod                 ??? !*   partition_mod !*   darcy_mod !*   solver_mod !* SIDE EFFECTS !*   Solution to Darcy flow computed !*   Solution written to file !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Miniapp to solve one-field Darcy flow in parallel using PETSc KSP !=============================================================================== !-- modules and implicit none -- USE common_mod USE mpi_mod USE partition_mod USE darcy_mod USE solver_mod IMPLICIT NONE !=============================================================================== ! MAIN EXECUTION !=============================================================================== !-- start MPI processes and write miniapp details -- CALL start_mpi () CALL rootwrite_log ( 'ParaGEMS: Solving two field Darcy flow in parallel using PETSc KSP' ) !-- read input -- CALL read_input_darcy () !-- setup partitioning and connectivity -- CALL parallel_setup () !-- initialise geometric information -- CALL initialise_geo () !-- initialise solution -- CALL start_petsc () CALL initialise_darcy () !-- get necessary matrices and vectors (A x = b) -- CALL get_LHS_darcy () CALL get_RHS_darcy () !-- solve linear system -- CALL solve_KSP () !-- write solution (darcy) -- CALL syncwrite_log ( '> Whitney interpolcation()' ); CALL calc_whitney_C2_BC () CALL syncwrite_log ( '> write_solution()' ); CALL write_solution_D0S ( 'pressure' , 'velocity' ) !-- clean up allocated variables and end MPI -- CALL syncwrite_log ( 'ParaGEMS: cleaning up Darcy flow solution' ) CALL clean_up () CALL finalise_darcy () CALL end_petsc () CALL end_mpi () END PROGRAM ! darcy !=============================================================================== !","tags":"","loc":"sourcefile/darcy_1f.f90.html","title":"darcy_1f.f90 – ParaGEMS"},{"text":"Miniapp to solve two-field Darcy flow in parallel using PETSc KSP This file depends on sourcefile~~darcy.f90~~EfferentGraph sourcefile~darcy.f90 darcy.f90 sourcefile~common_mod.f90 common_mod.f90 sourcefile~darcy.f90:->sourcefile~common_mod.f90: sourcefile~partition_mod.f90 partition_mod.f90 sourcefile~darcy.f90:->sourcefile~partition_mod.f90: sourcefile~darcy_mod.f90 darcy_mod.f90 sourcefile~darcy.f90:->sourcefile~darcy_mod.f90: sourcefile~solver_mod.f90 solver_mod.f90 sourcefile~darcy.f90:->sourcefile~solver_mod.f90: sourcefile~mpi_mod.f90 mpi_mod.f90 sourcefile~darcy.f90:->sourcefile~mpi_mod.f90: sourcefile~partition_mod.f90:->sourcefile~common_mod.f90: sourcefile~partition_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~dec_mod.f90 dec_mod.f90 sourcefile~partition_mod.f90:->sourcefile~dec_mod.f90: sourcefile~io_mod.f90 io_mod.f90 sourcefile~partition_mod.f90:->sourcefile~io_mod.f90: sourcefile~math_mod.f90 math_mod.f90 sourcefile~partition_mod.f90:->sourcefile~math_mod.f90: sourcefile~darcy_mod.f90:->sourcefile~common_mod.f90: sourcefile~darcy_mod.f90:->sourcefile~solver_mod.f90: sourcefile~darcy_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~darcy_mod.f90:->sourcefile~io_mod.f90: sourcefile~solver_mod.f90:->sourcefile~common_mod.f90: sourcefile~solver_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~mpi_mod.f90:->sourcefile~common_mod.f90: sourcefile~dec_mod.f90:->sourcefile~common_mod.f90: sourcefile~dec_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~dec_mod.f90:->sourcefile~math_mod.f90: sourcefile~io_mod.f90:->sourcefile~common_mod.f90: sourcefile~io_mod.f90:->sourcefile~solver_mod.f90: sourcefile~io_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~io_mod.f90:->sourcefile~dec_mod.f90: sourcefile~math_mod.f90:->sourcefile~common_mod.f90: sourcefile~math_mod.f90:->sourcefile~mpi_mod.f90: Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Programs darcy Source Code darcy.f90 Source Code ! !=============================================================================== !-- Darcy flow - two-field formulation !> Miniapp to solve two-field Darcy flow in parallel using PETSc KSP !=============================================================================== !/****p* programs|darcy_flow/darcy !* SYNOPSIS PROGRAM darcy !* PURPOSE !*   Miniapp to solve two-field Darcy flow in parallel using PETSc KSP !* ASSUMPTION !*   Mesh file exists (TetGen tetrahedral mesh) !*   Input file (default: input.param) !* INCLUDES !*   Name                    Purpose !*   common_mod              variable definitions !*   mpi_mod                 ??? !*   partition_mod !*   darcy_mod !*   solver_mod !* SIDE EFFECTS !*   Solution to Darcy flow computed !*   Solution written to file !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Miniapp to solve two-field Darcy flow in parallel using PETSc KSP !=============================================================================== !-- modules and implicit none -- USE common_mod USE mpi_mod USE partition_mod USE darcy_mod USE solver_mod IMPLICIT NONE !=============================================================================== ! MAIN EXECUTION !=============================================================================== !-- start MPI processes and write miniapp details -- CALL start_mpi () CALL rootwrite_log ( 'ParaGEMS: Solving two field Darcy flow in parallel using PETSc KSP' ) !-- read input -- CALL read_input_darcy () !-- setup partitioning and connectivity -- CALL parallel_setup () !-- initialise geometric information -- CALL initialise_geo () !-- initialise solution -- CALL start_petsc () CALL initialise_darcy () !-- get necessary matrices and vectors (A x = b) -- CALL get_LHS_darcy () CALL get_RHS_darcy () !-- solve linear system -- CALL solve_KSP () !-- write solution (darcy) -- CALL syncwrite_log ( '> Whitney interpolcation()' ); CALL calc_whitney_C2_BC () CALL syncwrite_log ( '> write_solution()' ); CALL write_solution_D0S ( 'pressure' , 'velocity' ) !-- clean up allocated variables and end MPI -- CALL syncwrite_log ( 'ParaGEMS: cleaning up Darcy flow solution' ) CALL clean_up () CALL finalise_darcy () CALL end_petsc () CALL end_mpi () END PROGRAM ! darcy !=============================================================================== !","tags":"","loc":"sourcefile/darcy.f90.html","title":"darcy.f90 – ParaGEMS"},{"text":"Miniapp to solve two-field Darcy flow in parallel using PETSc KSP with cracking This file depends on sourcefile~~darcy_crkp_2f.f90~~EfferentGraph sourcefile~darcy_crkp_2f.f90 darcy_crkp_2f.f90 sourcefile~common_mod.f90 common_mod.f90 sourcefile~darcy_crkp_2f.f90:->sourcefile~common_mod.f90: sourcefile~partition_mod.f90 partition_mod.f90 sourcefile~darcy_crkp_2f.f90:->sourcefile~partition_mod.f90: sourcefile~darcy_mod.f90 darcy_mod.f90 sourcefile~darcy_crkp_2f.f90:->sourcefile~darcy_mod.f90: sourcefile~solver_mod.f90 solver_mod.f90 sourcefile~darcy_crkp_2f.f90:->sourcefile~solver_mod.f90: sourcefile~mpi_mod.f90 mpi_mod.f90 sourcefile~darcy_crkp_2f.f90:->sourcefile~mpi_mod.f90: sourcefile~partition_mod.f90:->sourcefile~common_mod.f90: sourcefile~partition_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~dec_mod.f90 dec_mod.f90 sourcefile~partition_mod.f90:->sourcefile~dec_mod.f90: sourcefile~io_mod.f90 io_mod.f90 sourcefile~partition_mod.f90:->sourcefile~io_mod.f90: sourcefile~math_mod.f90 math_mod.f90 sourcefile~partition_mod.f90:->sourcefile~math_mod.f90: sourcefile~darcy_mod.f90:->sourcefile~common_mod.f90: sourcefile~darcy_mod.f90:->sourcefile~solver_mod.f90: sourcefile~darcy_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~darcy_mod.f90:->sourcefile~io_mod.f90: sourcefile~solver_mod.f90:->sourcefile~common_mod.f90: sourcefile~solver_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~mpi_mod.f90:->sourcefile~common_mod.f90: sourcefile~dec_mod.f90:->sourcefile~common_mod.f90: sourcefile~dec_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~dec_mod.f90:->sourcefile~math_mod.f90: sourcefile~io_mod.f90:->sourcefile~common_mod.f90: sourcefile~io_mod.f90:->sourcefile~solver_mod.f90: sourcefile~io_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~io_mod.f90:->sourcefile~dec_mod.f90: sourcefile~math_mod.f90:->sourcefile~common_mod.f90: sourcefile~math_mod.f90:->sourcefile~mpi_mod.f90: Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Programs darcy_crkp_2f Source Code darcy_crkp_2f.f90 Source Code ! !=============================================================================== !-- Darcy flow - two-field formulation with cracking !> Miniapp to solve two-field Darcy flow in parallel using PETSc KSP with cracking !=============================================================================== !/****p* programs|darcy_flow/darcy_crkp_2f !* SYNOPSIS PROGRAM darcy_crkp_2f !* PURPOSE !*   Miniapp to solve two-field Darcy flow in parallel using PETSc KSP with cracking !* ASSUMPTION !*   Mesh file exists (TetGen tetrahedral mesh) !*   Input file (default: input.param) !* INCLUDES !*   Name                    Purpose !*   common_mod              variable definitions !*   mpi_mod                 ??? !*   partition_mod !*   darcy_mod !*   solver_mod !* SIDE EFFECTS !*   Solution to Darcy flow computed !*   Solution written to file !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Miniapp to solve two-field Darcy flow in parallel using PETSc KSP with cracking !=============================================================================== !-- modules and implicit none -- USE common_mod USE mpi_mod USE partition_mod USE darcy_mod USE solver_mod IMPLICIT NONE INTEGER :: crck_id , i LOGICAL :: exit_cond CHARACTER ( LEN = slen ) :: msg , fname !=============================================================================== ! MAIN EXECUTION !=============================================================================== !-- start MPI processes and write miniapp details -- CALL start_mpi () CALL rootwrite_log ( 'ParaGEMS: Solving two field Darcy flow in parallel ' // & 'using PETSc KSP' ) !-- read input -- CALL read_input_darcy () !-- setup partitioning and connectivity -- CALL parallel_setup () !-- initialise geometric information -- CALL initialise_geo () !-- initialise solution -- CALL start_petsc (); CALL initialise_darcy2 () !-- get necessary matrices and vectors (A x = b) -- CALL get_LHS_darcy2 (); CALL get_RHS_darcy2 () !-- solve linear system -- CALL solve_KSP2 () !-- MATLAB output of solution and pressures IF ( sol_output ) THEN !-- extract solution data from PETSc arrays -- CALL extract_sol_KSP2 () !-- write MATLAB unsteady solution (darcy) -- CALL write_solution_MATLAB ( 0 ); CALL write_pressure_MATLAB ( 0 ) !-- MATLAB output of centers and volumes CALL write_centers_MATLAB (); CALL write_prml_volumes_MATLAB () !-- write vtk solution (darcy) -- CALL syncwrite_log ( '> Whitney interpolcation()' ); CALL calc_whitney_C2_BC () CALL syncwrite_log ( '> write_solution()' ) CALL write_unsteady_D0S2 ( 'pressure' , 'velocity' , 0 ) END IF !-- CALL open_unsteady_log () !-- DO crck_id = 1 , max_crcks write ( msg , * ) 'Crack propagation iteration: ' , crck_id CALL syncwrite_log ( msg ) !-- identify the faces with maximum velocity and crack -- IF ( crck_type == 0 ) THEN write ( msg , * ) '>>> no cracking' ; CALL syncwrite_log ( msg ); EXIT ELSEIF ( crck_type == 1 ) THEN CALL identify_crack5 ( crck_id , exit_cond ) ELSEIF ( crck_type == 2 ) THEN CALL identify_crack4 ( crck_id , exit_cond ) ELSEIF ( crck_type == 3 ) THEN DO i = 1 , crcks_pstep CALL identify_crack6 ( crck_id , exit_cond ) END DO ELSEIF ( crck_type == 4 ) THEN CALL identify_crack7 ( crck_id , exit_cond ) END IF IF ( exit_cond ) EXIT !-- solve linear system -- CALL solve_KSP2 () !-- MATLAB output of solution and pressures IF ( sol_output . and . MOD ( crck_id , output_frqcy ) == 0 ) THEN !-- extract solution data from PETSc arrays -- CALL extract_sol_KSP2 () !-- write MATLAB unsteady solution (darcy) -- CALL write_solution_MATLAB ( crck_id ) CALL write_pressure_MATLAB ( crck_id ) !-- write vtk unsteady solution (darcy) -- CALL syncwrite_log ( '> Whitney interpolcation()' ) CALL calc_whitney_C2_BC () CALL syncwrite_log ( '> write_solution()' ) CALL write_unsteady_D0S2 ( 'pressure' , 'velocity' , crck_id ) END IF END DO !-- clean up allocated variables and end MPI -- CALL syncwrite_log ( 'ParaGEMS: cleaning up Darcy flow solution' ) CALL finalise_darcy (); CALL close_unsteady_log () CALL clean_up () CALL end_petsc (); CALL end_mpi () END PROGRAM ! darcy !=============================================================================== !","tags":"","loc":"sourcefile/darcy_crkp_2f.f90.html","title":"darcy_crkp_2f.f90 – ParaGEMS"},{"text":"Miniapp to solve two-field Darcy flow in parallel using PETSc KSP with cracking This file depends on sourcefile~~darcy_crkp_2f_old.f90~~EfferentGraph sourcefile~darcy_crkp_2f_old.f90 darcy_crkp_2f_old.f90 sourcefile~common_mod.f90 common_mod.f90 sourcefile~darcy_crkp_2f_old.f90:->sourcefile~common_mod.f90: sourcefile~partition_mod.f90 partition_mod.f90 sourcefile~darcy_crkp_2f_old.f90:->sourcefile~partition_mod.f90: sourcefile~darcy_mod.f90 darcy_mod.f90 sourcefile~darcy_crkp_2f_old.f90:->sourcefile~darcy_mod.f90: sourcefile~solver_mod.f90 solver_mod.f90 sourcefile~darcy_crkp_2f_old.f90:->sourcefile~solver_mod.f90: sourcefile~mpi_mod.f90 mpi_mod.f90 sourcefile~darcy_crkp_2f_old.f90:->sourcefile~mpi_mod.f90: sourcefile~partition_mod.f90:->sourcefile~common_mod.f90: sourcefile~partition_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~dec_mod.f90 dec_mod.f90 sourcefile~partition_mod.f90:->sourcefile~dec_mod.f90: sourcefile~io_mod.f90 io_mod.f90 sourcefile~partition_mod.f90:->sourcefile~io_mod.f90: sourcefile~math_mod.f90 math_mod.f90 sourcefile~partition_mod.f90:->sourcefile~math_mod.f90: sourcefile~darcy_mod.f90:->sourcefile~common_mod.f90: sourcefile~darcy_mod.f90:->sourcefile~solver_mod.f90: sourcefile~darcy_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~darcy_mod.f90:->sourcefile~io_mod.f90: sourcefile~solver_mod.f90:->sourcefile~common_mod.f90: sourcefile~solver_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~mpi_mod.f90:->sourcefile~common_mod.f90: sourcefile~dec_mod.f90:->sourcefile~common_mod.f90: sourcefile~dec_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~dec_mod.f90:->sourcefile~math_mod.f90: sourcefile~io_mod.f90:->sourcefile~common_mod.f90: sourcefile~io_mod.f90:->sourcefile~solver_mod.f90: sourcefile~io_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~io_mod.f90:->sourcefile~dec_mod.f90: sourcefile~math_mod.f90:->sourcefile~common_mod.f90: sourcefile~math_mod.f90:->sourcefile~mpi_mod.f90: Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Programs darcy_crkp_2f Source Code darcy_crkp_2f_old.f90 Source Code ! !=============================================================================== !-- Darcy flow - two-field formulation with cracking !> Miniapp to solve two-field Darcy flow in parallel using PETSc KSP with cracking !=============================================================================== !/****p* programs|darcy_flow/darcy_crkp_2f !* SYNOPSIS PROGRAM darcy_crkp_2f !* PURPOSE !*   Miniapp to solve two-field Darcy flow in parallel using PETSc KSP with cracking !* ASSUMPTION !*   Mesh file exists (TetGen tetrahedral mesh) !*   Input file (default: input.param) !* INCLUDES !*   Name                    Purpose !*   common_mod              variable definitions !*   mpi_mod                 ??? !*   partition_mod !*   darcy_mod !*   solver_mod !* SIDE EFFECTS !*   Solution to Darcy flow computed !*   Solution written to file !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Miniapp to solve two-field Darcy flow in parallel using PETSc KSP with cracking !=============================================================================== !-- modules and implicit none -- USE common_mod USE mpi_mod USE partition_mod USE darcy_mod USE solver_mod IMPLICIT NONE INTEGER :: crck_id LOGICAL :: exit_cond CHARACTER ( LEN = slen ) :: msg , fname !=============================================================================== ! MAIN EXECUTION !=============================================================================== !-- start MPI processes and write miniapp details -- CALL start_mpi () CALL rootwrite_log ( 'ParaGEMS: Solving two field Darcy flow in parallel ' // & 'using PETSc KSP' ) !-- read input -- CALL read_input_darcy () !-- setup partitioning and connectivity -- CALL parallel_setup () !-- initialise geometric information -- CALL initialise_geo () !-- initialise solution -- CALL start_petsc (); CALL initialise_darcy () !-- get necessary matrices and vectors (A x = b) -- CALL get_LHS_darcy (); CALL get_RHS_darcy () !-- solve linear system -- CALL solve_KSP () !-- MATLAB output of solution and pressures IF ( sol_output ) THEN !-- extract solution data from PETSc arrays -- CALL extract_sol_KSP () !-- write MATLAB unsteady solution (darcy) -- CALL write_solution_MATLAB2 ( 0 ) CALL write_pressure_MATLAB2 ( 0 ) !-- MATLAB output of centers and volumes CALL write_centers_MATLAB2 (); CALL write_prml_volumes_MATLAB2 () !-- write vtk solution (darcy) -- CALL syncwrite_log ( '> Whitney interpolcation()' ); CALL calc_whitney_C2_BC () CALL syncwrite_log ( '> write_solution()' ) CALL write_unsteady_D0S ( 'pressure' , 'velocity' , 0 ) END IF !-- CALL open_unsteady_log () !-- DO crck_id = 1 , max_crcks write ( msg , * ) 'Crack propagation iteration: ' , crck_id CALL syncwrite_log ( msg ) !-- identify the faces with maximum velocity and crack -- IF ( crck_type == 0 ) THEN write ( msg , * ) '>>> no cracking' ; CALL syncwrite_log ( msg ); EXIT ELSEIF ( crck_type == 1 ) THEN !CALL identify_crack5(crck_id,exit_cond) CALL identify_crack3 ( crck_id , exit_cond ) ELSEIF ( crck_type == 2 ) THEN !CALL identify_crack4(crck_id,exit_cond) CALL identify_crack ( exit_cond ) END IF IF ( exit_cond ) EXIT !-- solve linear system -- CALL solve_KSP () !-- MATLAB output of solution and pressures IF ( sol_output . and . MOD ( crck_id , output_frqcy ) == 0 ) THEN !-- extract solution data from PETSc arrays -- CALL extract_sol_KSP () !-- write MATLAB unsteady solution (darcy) -- CALL write_solution_MATLAB2 ( crck_id ) CALL write_pressure_MATLAB2 ( crck_id ) !-- write vtk unsteady solution (darcy) -- CALL syncwrite_log ( '> Whitney interpolcation()' ) CALL calc_whitney_C2_BC () CALL syncwrite_log ( '> write_solution()' ) CALL write_unsteady_D0S ( 'pressure' , 'velocity' , crck_id ) END IF END DO !-- clean up allocated variables and end MPI -- CALL syncwrite_log ( 'ParaGEMS: cleaning up Darcy flow solution' ) CALL finalise_darcy (); CALL close_unsteady_log () CALL clean_up () CALL end_petsc (); CALL end_mpi () END PROGRAM ! darcy !=============================================================================== !","tags":"","loc":"sourcefile/darcy_crkp_2f_old.f90.html","title":"darcy_crkp_2f_old.f90 – ParaGEMS"},{"text":"Miniapp to solve one-field Darcy flow in parallel using PETSc KSP with cracking This file depends on sourcefile~~darcy_crkp_1f.f90~~EfferentGraph sourcefile~darcy_crkp_1f.f90 darcy_crkp_1f.f90 sourcefile~common_mod.f90 common_mod.f90 sourcefile~darcy_crkp_1f.f90:->sourcefile~common_mod.f90: sourcefile~partition_mod.f90 partition_mod.f90 sourcefile~darcy_crkp_1f.f90:->sourcefile~partition_mod.f90: sourcefile~darcy_mod.f90 darcy_mod.f90 sourcefile~darcy_crkp_1f.f90:->sourcefile~darcy_mod.f90: sourcefile~solver_mod.f90 solver_mod.f90 sourcefile~darcy_crkp_1f.f90:->sourcefile~solver_mod.f90: sourcefile~mpi_mod.f90 mpi_mod.f90 sourcefile~darcy_crkp_1f.f90:->sourcefile~mpi_mod.f90: sourcefile~partition_mod.f90:->sourcefile~common_mod.f90: sourcefile~partition_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~dec_mod.f90 dec_mod.f90 sourcefile~partition_mod.f90:->sourcefile~dec_mod.f90: sourcefile~io_mod.f90 io_mod.f90 sourcefile~partition_mod.f90:->sourcefile~io_mod.f90: sourcefile~math_mod.f90 math_mod.f90 sourcefile~partition_mod.f90:->sourcefile~math_mod.f90: sourcefile~darcy_mod.f90:->sourcefile~common_mod.f90: sourcefile~darcy_mod.f90:->sourcefile~solver_mod.f90: sourcefile~darcy_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~darcy_mod.f90:->sourcefile~io_mod.f90: sourcefile~solver_mod.f90:->sourcefile~common_mod.f90: sourcefile~solver_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~mpi_mod.f90:->sourcefile~common_mod.f90: sourcefile~dec_mod.f90:->sourcefile~common_mod.f90: sourcefile~dec_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~dec_mod.f90:->sourcefile~math_mod.f90: sourcefile~io_mod.f90:->sourcefile~common_mod.f90: sourcefile~io_mod.f90:->sourcefile~solver_mod.f90: sourcefile~io_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~io_mod.f90:->sourcefile~dec_mod.f90: sourcefile~math_mod.f90:->sourcefile~common_mod.f90: sourcefile~math_mod.f90:->sourcefile~mpi_mod.f90: Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Programs darcy_crkp_1f Source Code darcy_crkp_1f.f90 Source Code ! !=============================================================================== !-- Darcy flow - one-field formulation with cracking !> Miniapp to solve one-field Darcy flow in parallel using PETSc KSP with cracking !=============================================================================== !/****p* programs|darcy_flow/darcy_crkp_1f !* SYNOPSIS PROGRAM darcy_crkp_1f !* PURPOSE !*   Miniapp to solve one-field Darcy flow in parallel using PETSc KSP with cracking !* ASSUMPTION !*   Mesh file exists (TetGen tetrahedral mesh) !*   Input file (default: input.param) !* INCLUDES !*   Name                    Purpose !*   common_mod              variable definitions !*   mpi_mod                 ??? !*   partition_mod !*   darcy_mod !*   solver_mod !* SIDE EFFECTS !*   Solution to Darcy flow computed !*   Solution written to file !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Miniapp to solve one-field Darcy flow in parallel using PETSc KSP with cracking !=============================================================================== !-- modules and implicit none -- USE common_mod USE mpi_mod USE partition_mod USE darcy_mod USE solver_mod IMPLICIT NONE !=============================================================================== ! MAIN EXECUTION !=============================================================================== !-- start MPI processes and write miniapp details -- CALL start_mpi () CALL rootwrite_log ( 'ParaGEMS: Solving two field Darcy flow in parallel using PETSc KSP' ) !-- read input -- CALL read_input_darcy () !-- setup partitioning and connectivity -- CALL parallel_setup () !-- initialise geometric information -- CALL initialise_geo () !-- initialise solution -- CALL start_petsc () CALL initialise_darcy () !-- get necessary matrices and vectors (A x = b) -- CALL get_LHS_darcy () CALL get_RHS_darcy () !-- solve linear system -- CALL solve_KSP () !-- write solution (darcy) -- CALL syncwrite_log ( '> Whitney interpolcation()' ); CALL calc_whitney_C2_BC () CALL syncwrite_log ( '> write_solution()' ); CALL write_solution_D0S ( 'pressure' , 'velocity' ) !-- clean up allocated variables and end MPI -- CALL syncwrite_log ( 'ParaGEMS: cleaning up Darcy flow solution' ) CALL clean_up () CALL finalise_darcy () CALL end_petsc () CALL end_mpi () END PROGRAM ! darcy !=============================================================================== !","tags":"","loc":"sourcefile/darcy_crkp_1f.f90.html","title":"darcy_crkp_1f.f90 – ParaGEMS"},{"text":"Miniapp to solve two-field Darcy flow in parallel using PETSc KSP This file depends on sourcefile~~darcy_2f.f90~~EfferentGraph sourcefile~darcy_2f.f90 darcy_2f.f90 sourcefile~common_mod.f90 common_mod.f90 sourcefile~darcy_2f.f90:->sourcefile~common_mod.f90: sourcefile~partition_mod.f90 partition_mod.f90 sourcefile~darcy_2f.f90:->sourcefile~partition_mod.f90: sourcefile~darcy_mod.f90 darcy_mod.f90 sourcefile~darcy_2f.f90:->sourcefile~darcy_mod.f90: sourcefile~solver_mod.f90 solver_mod.f90 sourcefile~darcy_2f.f90:->sourcefile~solver_mod.f90: sourcefile~mpi_mod.f90 mpi_mod.f90 sourcefile~darcy_2f.f90:->sourcefile~mpi_mod.f90: sourcefile~partition_mod.f90:->sourcefile~common_mod.f90: sourcefile~partition_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~dec_mod.f90 dec_mod.f90 sourcefile~partition_mod.f90:->sourcefile~dec_mod.f90: sourcefile~io_mod.f90 io_mod.f90 sourcefile~partition_mod.f90:->sourcefile~io_mod.f90: sourcefile~math_mod.f90 math_mod.f90 sourcefile~partition_mod.f90:->sourcefile~math_mod.f90: sourcefile~darcy_mod.f90:->sourcefile~common_mod.f90: sourcefile~darcy_mod.f90:->sourcefile~solver_mod.f90: sourcefile~darcy_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~darcy_mod.f90:->sourcefile~io_mod.f90: sourcefile~solver_mod.f90:->sourcefile~common_mod.f90: sourcefile~solver_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~mpi_mod.f90:->sourcefile~common_mod.f90: sourcefile~dec_mod.f90:->sourcefile~common_mod.f90: sourcefile~dec_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~dec_mod.f90:->sourcefile~math_mod.f90: sourcefile~io_mod.f90:->sourcefile~common_mod.f90: sourcefile~io_mod.f90:->sourcefile~solver_mod.f90: sourcefile~io_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~io_mod.f90:->sourcefile~dec_mod.f90: sourcefile~math_mod.f90:->sourcefile~common_mod.f90: sourcefile~math_mod.f90:->sourcefile~mpi_mod.f90: Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Programs darcy_2f Source Code darcy_2f.f90 Source Code ! !=============================================================================== !-- Darcy flow - two-field formulation !> Miniapp to solve two-field Darcy flow in parallel using PETSc KSP !=============================================================================== !/****p* programs|darcy_flow/darcy_2f !* SYNOPSIS PROGRAM darcy_2f !* PURPOSE !*   Miniapp to solve two-field Darcy flow in parallel using PETSc KSP !* ASSUMPTION !*   Mesh file exists (TetGen tetrahedral mesh) !*   Input file (default: input.param) !* INCLUDES !*   Name                    Purpose !*   common_mod              variable definitions !*   mpi_mod                 general mpi routines (start, end, syncwrite,etc) !*   partition_mod           parallel partitioning !*   darcy_mod               Darcy flow specific routines !*   solver_mod              Solver routines (PETSc) !* SIDE EFFECTS !*   Solution to Darcy flow computed !*   Solution written to file !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2020/09/16: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2020/09/16 !> Miniapp to solve two-field Darcy flow in parallel using PETSc KSP !=============================================================================== !-- modules and implicit none -- USE common_mod USE mpi_mod USE partition_mod USE darcy_mod USE solver_mod IMPLICIT NONE !=============================================================================== ! MAIN EXECUTION !=============================================================================== !-- start MPI processes and write miniapp details -- CALL start_mpi () CALL rootwrite_log ( 'ParaGEMS: Solving two field Darcy flow in parallel ' // & 'using PETSc KSP' ) !-- read input -- CALL read_input_darcy () !-- setup partitioning and connectivity -- CALL parallel_setup () !-- initialise geometric information -- CALL initialise_geo () !-- initialise solution -- CALL start_petsc () CALL initialise_darcy2 () !-- get necessary matrices and vectors (A x = b) -- CALL get_LHS_darcy2 () CALL get_RHS_darcy2 () !-- solve linear system -- CALL solve_KSP2 () !-- MATLAB output of solution and pressures CALL extract_sol_KSP2 () CALL write_solution_MATLAB () CALL write_pressure_MATLAB () !-- MATLAB output of centers and volumes CALL write_centers_MATLAB () CALL write_prml_volumes_MATLAB () !-- write solution (darcy) -- CALL syncwrite_log ( '> Whitney interpolcation()' ); CALL calc_whitney_C2_BC () CALL syncwrite_log ( '> write_solution()' ) CALL write_solution_D0S2 ( 'pressure' , 'velocity' ) !-- clean up allocated variables and end MPI -- CALL syncwrite_log ( 'ParaGEMS: cleaning up Darcy flow solution' ) CALL clean_up () CALL finalise_darcy () CALL end_petsc () CALL end_mpi () END PROGRAM ! darcy !=============================================================================== !","tags":"","loc":"sourcefile/darcy_2f.f90.html","title":"darcy_2f.f90 – ParaGEMS"},{"text":"Module contains routines for partitioning parallel workload This file depends on sourcefile~~partition_mod.f90~~EfferentGraph sourcefile~partition_mod.f90 partition_mod.f90 sourcefile~common_mod.f90 common_mod.f90 sourcefile~partition_mod.f90:->sourcefile~common_mod.f90: sourcefile~dec_mod.f90 dec_mod.f90 sourcefile~partition_mod.f90:->sourcefile~dec_mod.f90: sourcefile~io_mod.f90 io_mod.f90 sourcefile~partition_mod.f90:->sourcefile~io_mod.f90: sourcefile~math_mod.f90 math_mod.f90 sourcefile~partition_mod.f90:->sourcefile~math_mod.f90: sourcefile~mpi_mod.f90 mpi_mod.f90 sourcefile~partition_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~dec_mod.f90:->sourcefile~common_mod.f90: sourcefile~dec_mod.f90:->sourcefile~math_mod.f90: sourcefile~dec_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~io_mod.f90:->sourcefile~common_mod.f90: sourcefile~io_mod.f90:->sourcefile~dec_mod.f90: sourcefile~io_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~solver_mod.f90 solver_mod.f90 sourcefile~io_mod.f90:->sourcefile~solver_mod.f90: sourcefile~math_mod.f90:->sourcefile~common_mod.f90: sourcefile~math_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~mpi_mod.f90:->sourcefile~common_mod.f90: sourcefile~solver_mod.f90:->sourcefile~common_mod.f90: sourcefile~solver_mod.f90:->sourcefile~mpi_mod.f90: Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Files dependent on this one sourcefile~~partition_mod.f90~~AfferentGraph sourcefile~partition_mod.f90 partition_mod.f90 sourcefile~darcy.f90 darcy.f90 sourcefile~darcy.f90:->sourcefile~partition_mod.f90: sourcefile~darcy_1f.f90 darcy_1f.f90 sourcefile~darcy_1f.f90:->sourcefile~partition_mod.f90: sourcefile~darcy_crkp_2f_old.f90 darcy_crkp_2f_old.f90 sourcefile~darcy_crkp_2f_old.f90:->sourcefile~partition_mod.f90: sourcefile~darcy_crkp_2f.f90 darcy_crkp_2f.f90 sourcefile~darcy_crkp_2f.f90:->sourcefile~partition_mod.f90: sourcefile~test_partition_mod.f90 test_partition_mod.f90 sourcefile~test_partition_mod.f90:->sourcefile~partition_mod.f90: sourcefile~darcy_crkp_1f.f90 darcy_crkp_1f.f90 sourcefile~darcy_crkp_1f.f90:->sourcefile~partition_mod.f90: sourcefile~darcy_2f.f90 darcy_2f.f90 sourcefile~darcy_2f.f90:->sourcefile~partition_mod.f90: Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules partition_mod Source Code partition_mod.f90 Source Code ! !=============================================================================== !-- DEC Module !> Module contains routines for partitioning parallel workload !=============================================================================== !/****/h* modules|mpi/partition_mod MODULE partition_mod !* PURPOSE !*   Module contains routines for partitioning parallel workload !* INCLUDES !*   Name                    Purpose !*   common_mod              variable definitions !*   mpi_mod                 general mpi routines (start, end, syncwrite,etc) !*   io_mod                  file IO routines !*   math_mod                basic math functions !*   dec_mod                 DEC related functions !* CONTAINS !*   Subroutine              Purpose !*   parallel_setup          sets up partitioning, read mesh and nodal !*                           locations, and sets connectivity !*   partition_dual          partitions dual volumes across available processes !*   get_lcl_prml_elms       ??? get local node indices of highest order primal elements !*   get_connectivity !*   get_glb_indx_dual_vlm !*   get_glb_indx !*   exchange_prml_nodes !*   exchange_glb_indx !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2020/09/16: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2020/09/16 !> Module contains routines for partitioning parallel workload !=============================================================================== ! TO DO: ! - MPI + OpenMP? !=============================================================================== !-- use statements and implicit none -- USE common_mod USE mpi_mod USE io_mod USE math_mod USE dec_mod IMPLICIT NONE !=============================================================================== CONTAINS !=============================================================================== ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* partition_mod/parallel_setup !* SYNOPSIS SUBROUTINE parallel_setup () !* PURPOSE !*   Sets up partitioning, read mesh and nodal locations, and sets connectivity !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2020/09/16: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2020/09/16 !> Sets up partitioning, read mesh and nodal locations, and sets connectivity !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- local variables -- INTEGER :: i !> counter REAL ( KIND = IWP ) :: tmp_time , tmp_time_int !> temporary timing variable !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ CALL syncwrite_log ( '> parallel_setup()' ); tmp_time = curnt_time !-- partition dual elements (eq. primal nodes) across available processes -- CALL syncwrite_log ( '> parallel_setup() - partition_dual()' ) CALL partition_dual (); CALL syncwrite_log_time () !-- get local node indices of highest order primal elements -- CALL syncwrite_log ( '> parallel_setup() - read_prml_elms()' ); CALL read_prml_elms2 (); CALL syncwrite_log_time () !-- get local connectivity and setup parallel connectivity -- CALL syncwrite_log ( '> parallel_setup() - calc_bndry_cobndry()' ) tmp_time_int = curnt_time ; CALL calc_bndry_cobndry () curnt_time = tmp_time_int ; CALL syncwrite_log_time () !-- get parallel connectivity -- CALL syncwrite_log ( '> parallel_setup() - get parallel connectivity ' // & '- get_glb_indx_dual_vlm' ) CALL get_glb_indx_dual_vlm (); CALL syncwrite_log_time () CALL syncwrite_log ( '> parallel_setup() - get parallel connectivity ' // & '- get_connectivity()' ) CALL get_connectivity (); CALL syncwrite_log_time () CALL syncwrite_log ( '> parallel_setup() - get parallel connectivity ' // & '- get_glb_indx' ) CALL get_glb_indx (); CALL syncwrite_log_time () !-- read primal node locations -- CALL syncwrite_log ( '> parallel_setup() - read primal node locations' ) CALL read_nodes_prml (); CALL exchange_prml_nodes () CALL syncwrite_log_time () !-- log time to complete parallel setup -- curnt_time = tmp_time ; CALL syncwrite_log_time () RETURN END SUBROUTINE ! partition_mod|parallel_setup !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* partition_mod/partition_dual !* SYNOPSIS SUBROUTINE partition_dual () !* PURPOSE !*   Partitions dual volumes across available processes !* ASSUMPTION !*   Mesh is a simplicial complex in TetGen format !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2020/09/16: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2020/09/16 !> Partitions dual volumes across available processes !> Assumption: Mesh is a simplicial complex in TetGen format !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- local variables -- CHARACTER ( LEN = slen ) :: fname !> file name INTEGER :: idx !> index variable INTEGER :: k !> simplicial order INTEGER :: int_buffer ( 6 ) !> integer buffer for MPI comms INTEGER :: junk !> junk IO variable LOGICAL :: err = . FALSE . !> error variable !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- get mesh data on root process -- IF ( rank == root ) THEN !-- Open primary element file and check for errors -- fname = trim ( mesh_prefix ) // \".ele\" OPEN ( mesh_unit , FILE = fname , STATUS = 'OLD' , ACTION = 'READ' , IOSTAT = ier ) IF ( ier /= 0 ) THEN WRITE ( log_unit , '(A,A,A,I5)' ) '***Error: failed to open ' , trim ( fname ),& ': errcode = ' , ier err = . TRUE . ELSE !-- read header for number of primary elements and complex dimension -- READ ( mesh_unit , * ) junk , k dim_cmplx = k - 1 glb_num_elm ( k ) = junk !-- first element to get index offset READ ( mesh_unit , * ) junk indx_offset ( k ) = 1 - junk !-- Close file -- CLOSE ( mesh_unit , IOSTAT = ier ) IF ( ier /= 0 ) THEN WRITE ( log_unit , '(A,A,A,I5)' ) '***Error: failed to close ' , trim ( fname ),& ': errcode = ' , ier err = . TRUE . END IF !-- check for minimum dimension of simplicial complex -- IF ( dim_cmplx < 1 ) THEN WRITE ( log_unit , '(A,A,A,I5)' ) '***Error: in element file, ' ,& trim ( fname ), ': dimension of simplicial complex less than one: ' ,& dim_cmplx err = . TRUE . END IF END IF !-- Open primal node (eq. dual element) file and check for errors -- fname = trim ( mesh_prefix ) // \".node\" OPEN ( mesh_unit , FILE = fname , STATUS = 'OLD' , ACTION = 'READ' , IOSTAT = ier ) IF ( ier /= 0 ) THEN WRITE ( log_unit , '(A,A,A,I5)' ) '***Error: failed to open ' , trim ( fname ),& ': errcode = ' , ier err = . TRUE . ELSE !-- read header for number of dual elements and embedding dimension -- READ ( mesh_unit , * ) glb_num_elm ( 1 ), dim_embbd !-- first node to get index offset READ ( mesh_unit , * ) junk indx_offset ( 1 ) = 1 - junk !-- Close file -- CLOSE ( mesh_unit , IOSTAT = ier ) IF ( ier /= 0 ) THEN WRITE ( log_unit , '(A,A,A,I5)' ) '***Error: failed to close ' , trim ( fname ),& ': errcode = ' , ier err = . TRUE . END IF !-- Check parallel compatibility -- IF ( glb_num_elm ( 1 ) < num_procs ) THEN WRITE ( log_unit , '(A,I5,A,A,I5)' ) '***Error: number of available ' // & 'processes, ' , num_procs , ', greater than the number of dual ' // & 'elements (eq. primary nodes), ' , glb_num_elm ( 1 ) err = . TRUE . END IF END IF !-- pack MPI buffer -- int_buffer ( 1 ) = k int_buffer ( 2 ) = dim_embbd int_buffer ( 3 ) = glb_num_elm ( 1 ) int_buffer ( 4 ) = glb_num_elm ( k ) int_buffer ( 5 ) = indx_offset ( 1 ) int_buffer ( 6 ) = indx_offset ( k ) END IF !-- check for errors CALL MPI_BCAST ( err , 1 , MPI_LOGICAL , 0 , MPI_COMM_WORLD , ier ) IF ( err ) CALL end_mpi () !-- broadcast and unpack data -- CALL MPI_BCAST ( int_buffer , 6 , MPI_INTEGER , 0 , MPI_COMM_WORLD , ier ) IF ( rank /= root ) THEN k = int_buffer ( 1 ) dim_embbd = int_buffer ( 2 ) glb_num_elm ( 1 ) = int_buffer ( 3 ) glb_num_elm ( k ) = int_buffer ( 4 ) indx_offset ( 1 ) = int_buffer ( 5 ) indx_offset ( k ) = int_buffer ( 6 ) dim_cmplx = k - 1 END IF !-- calculate number of local dual elements per process, and the indices of !   the dual elements on each process -- !-- partition dual elements (base number per node) -- num_elm ( 1 ) = glb_num_elm ( 1 ) / num_procs !-- compute additional dual elements and partition also compute offset for !   ranges -- extra_pelm = mod ( glb_num_elm ( 1 ), num_procs ) IF ( rank < extra_pelm ) THEN num_elm ( 1 ) = num_elm ( 1 ) + 1 glb_offset = rank * num_elm ( 1 ) ELSE glb_offset = extra_pelm + rank * num_elm ( 1 ) END IF !-- gather the number of elements per_node -- IF ( ALLOCATED ( num_pelm_pp )) DEALLOCATE ( num_pelm_pp ) ALLOCATE ( num_pelm_pp ( num_procs )) CALL MPI_ALLGATHER ( num_elm ( 1 ), 1 , MPI_INTEGER , num_pelm_pp , 1 , MPI_INTEGER ,& MPI_COMM_WORLD , ier ) !-- allocate local variables -- ALLOCATE ( lcl_complex ( k )) ALLOCATE ( lcl_complex ( 1 )% glb_indx ( num_elm ( 1 ))) lcl_complex ( 1 )% glb_indx = ( / ( idx , idx = 1 , num_elm ( 1 )) / ) + glb_offset RETURN END SUBROUTINE ! partition_mod|partition_dual !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* partition_mod/get_connectivity !* SYNOPSIS SUBROUTINE get_connectivity () !* PURPOSE !*   Determine cross-process entity connections for all geometric orders !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2020/09/16: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2020/09/16 !> Determine cross-process entity connections for all geometric orders !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- local variables -- INTEGER :: k , km !> simplicial order INTEGER :: i , j !> loop indices INTEGER :: junk !> junk variable INTEGER :: ptr , ptr2 , ptr3 !> pointers INTEGER , ALLOCATABLE :: req (:) !> request var (non-blking comms) INTEGER , ALLOCATABLE :: status (:,:) !> status var (non-blking comms) INTEGER , ALLOCATABLE :: sbuffer (:) !> boundary index INTEGER , ALLOCATABLE :: rbuffer (:) !> boundary index CHARACTER ( LEN = slen ) :: msg !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- send expected number of boundaries from adjacent processes (for all !    geometric orders) then send boundary indices -- !-- get number of adjacent processes -- num_adj_proc = 0 ; DO j = 1 , lcl_complex ( 2 )% num_recv IF ( j == 1 . OR . lcl_complex ( 2 )% recv_indx ( j , 1 ) /= & lcl_complex ( 2 )% recv_indx ( max ( 1 , j - 1 ), 1 )) & num_adj_proc = num_adj_proc + 1 END DO ALLOCATE ( adj_proc ( num_adj_proc )) !-- allocate MPI arrays -- ALLOCATE (& num_send ( dim_cmplx , num_adj_proc ),& num_recv ( dim_cmplx , num_adj_proc ),& req ( dim_cmplx * num_adj_proc ),& status ( MPI_STATUS_SIZE , dim_cmplx * num_adj_proc )) num_send = 0 ; num_recv = 0 !-- pack send buffer with number of required number of boundaries for each !   geometric order -- ptr = 0 ; DO j = 1 , lcl_complex ( 2 )% num_recv IF ( j == 1 . OR . lcl_complex ( 2 )% recv_indx ( j , 1 ) /= & lcl_complex ( 2 )% recv_indx ( max ( 1 , j - 1 ), 1 )) THEN ptr = ptr + 1 adj_proc ( ptr ) = lcl_complex ( 2 )% recv_indx ( j , 1 ) num_recv ( 1 , ptr ) = 1 ELSE num_recv ( 1 , ptr ) = num_recv ( 1 , ptr ) + 1 END IF END DO ptr2 = lcl_complex ( 2 )% num_recv DO k = 3 , dim_cmplx + 1 km = k - 1 DO j = 1 , lcl_complex ( k )% num_recv IF ( j == 1 . OR . lcl_complex ( k )% recv_indx ( j , 1 ) /= & lcl_complex ( k )% recv_indx ( max ( 1 , j - 1 ), 1 )) THEN ptr = index_in_list ( lcl_complex ( k )% recv_indx ( j , 1 ), adj_proc , ptr ) num_recv ( km , ptr ) = 1 ELSE num_recv ( km , ptr ) = num_recv ( km , ptr ) + 1 END IF END DO ptr2 = ptr2 + lcl_complex ( k )% num_recv * ( k - 1 ) END DO ALLOCATE ( sbuffer ( ptr2 )) !-- send/receive expected number of boundaries from adjacent processes !   then send boundary indices (for all geometric orders) -- DO j = 1 , num_adj_proc ptr = ( j - 1 ) * dim_cmplx ; ptr2 = ptr + dim_cmplx CALL MPI_ISEND ( num_recv (:, j ), dim_cmplx , MPI_INTEGER , adj_proc ( j ), 0 ,& MPI_COMM_WORLD , junk , ier ); CALL MPI_REQUEST_FREE ( junk , ier ) CALL MPI_IRECV ( num_send (:, j ), dim_cmplx , MPI_INTEGER , adj_proc ( j ), 0 ,& MPI_COMM_WORLD , req ( j ), ier ) END DO ptr = 1 DO k = 2 , dim_cmplx + 1 km = k - 1 ptr2 = ptr DO j = 1 , lcl_complex ( k )% num_recv ptr3 = ptr + km sbuffer ( ptr : ptr3 - 1 ) = & lcl_complex ( km )% node_indx ( lcl_complex ( k )% recv_indx ( j , 2 ),:) ptr = ptr3 END DO DO j = 1 , num_adj_proc IF ( num_recv ( km , j ) == 0 ) CYCLE ptr3 = ptr2 + km * num_recv ( km , j ) CALL MPI_ISEND ( sbuffer ( ptr2 : ptr3 - 1 ), km * num_recv ( km , j ), MPI_INTEGER ,& adj_proc ( j ), k , MPI_COMM_WORLD , junk , ier ) CALL MPI_REQUEST_FREE ( junk , ier ) ptr2 = ptr3 END DO END DO !-- receive receive boundary indices (for all geometric orders) -- CALL MPI_WAITALL ( num_adj_proc , req ( 1 : num_adj_proc ), status (:, 1 : num_adj_proc ),& ier ) req = 0 status = 0 ptr = 0 DO k = 2 , dim_cmplx + 1 km = k - 1 lcl_complex ( k )% num_send = sum ( num_send ( km ,:)) ALLOCATE ( lcl_complex ( k )% send_indx ( lcl_complex ( k )% num_send , 2 )) ptr = ptr + lcl_complex ( k )% num_send * km END DO ALLOCATE ( rbuffer ( ptr )) ptr = 1 !-- recv buffer pointer -- DO k = 2 , dim_cmplx + 1 km = k - 1 ptr2 = 1 !-- lcl_complex(k)%recv_indx pointer -- DO j = 1 , num_adj_proc IF ( num_send ( km , j ) == 0 ) THEN req (( k - 2 ) * num_adj_proc + j ) = MPI_REQUEST_NULL CYCLE END IF ptr3 = ptr2 + num_send ( km , j ) lcl_complex ( k )% send_indx ( ptr2 : ptr3 - 1 , 1 ) = adj_proc ( j ) ptr2 = ptr3 ptr3 = ptr + km * num_send ( km , j ) CALL MPI_IRECV ( rbuffer ( ptr : ptr3 - 1 ), km * num_send ( km , j ), MPI_INTEGER ,& adj_proc ( j ), k , MPI_COMM_WORLD , req (( k - 2 ) * num_adj_proc + j ), ier ) ptr = ptr3 END DO END DO !-- get local indices for send_indx -- CALL MPI_WAITALL ( dim_cmplx * num_adj_proc , req , status , ier ) ptr = 1 !-- send buffer pointer DO k = 2 , dim_cmplx + 1 km = k - 1 DO j = 1 , lcl_complex ( k )% num_send ptr2 = ptr + km - 1 DO i = 1 , num_elm ( k - 1 ) IF ( ALL ( rbuffer ( ptr : ptr2 ) == lcl_complex ( k - 1 )% node_indx ( i , 1 : km ))) THEN lcl_complex ( k )% send_indx ( j , 2 ) = i EXIT END IF END DO ptr = ptr2 + 1 END DO END DO !-- clean up -- DEALLOCATE ( req , status ) DO k = 2 , dim_cmplx + 1 CALL MPI_REDUCE ( lcl_complex ( k )% num_send , i , 1 , MPI_INTEGER , MPI_SUM , 0 ,& MPI_COMM_WORLD , ier ) CALL MPI_REDUCE ( lcl_complex ( k )% num_recv , j , 1 , MPI_INTEGER , MPI_SUM , 0 ,& MPI_COMM_WORLD , ier ) write ( msg , '(A,I1,A,I8,A,I8)' ) '   - k: ' , k , ' :number of sends: ' , i ,& ' :number of receives: ' , j CALL rootwrite_log ( msg ) END DO !-- make sure all comms have completed -- CALL MPI_BARRIER ( MPI_COMM_WORLD , ier ) RETURN END SUBROUTINE ! partition_mod|get_connectivity !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* partition_mod/get_glb_indx_dual_vlm !* SYNOPSIS SUBROUTINE get_glb_indx_dual_vlm () !* PURPOSE !*   Set global index of nodes from predetermined node index !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2020/09/16: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2020/09/16 !> Set global index of nodes from predetermined node index !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- local variables -- INTEGER :: k !> simplicial order !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- on root process: -- IF ( ALLOCATED ( lcl_complex ( 1 )% glb_indx )) DEALLOCATE ( lcl_complex ( 1 )% glb_indx ) ALLOCATE ( lcl_complex ( 1 )% glb_indx ( num_elm ( 1 ))) lcl_complex ( 1 )% glb_indx = lcl_complex ( 1 )% node_indx (:, 1 ) RETURN END SUBROUTINE ! partition_mod|get_glb_indx_dual_vlm !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* partition_mod/get_glb_indx !* SYNOPSIS SUBROUTINE get_glb_indx () !* PURPOSE !*   Get global entity indices from file !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2020/09/16: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2020/09/16 !> Get global entity indices from file !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- local variables -- INTEGER :: k !> simplicial order !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- on root process: -- DO k = 2 , dim_cmplx IF (. NOT . ALLOCATED ( lcl_complex ( k )% glb_indx )) & ALLOCATE ( lcl_complex ( k )% glb_indx ( num_elm ( k ))) CALL read_glb_indx2 ( k ) CALL exchange_glb_indx ( k ) !CALL read_glb_indx2(k) END DO RETURN END SUBROUTINE ! partition_mod|get_glb_indx !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* partition_mod/exchange_prml_nodes !* SYNOPSIS SUBROUTINE exchange_prml_nodes () !* PURPOSE !*   Exchange nodal locations to adjacent process (ghost nodes) !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2020/09/16: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2020/09/16 !> Exchange nodal locations to adjacent process (ghost nodes) !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- local variables -- INTEGER :: ptr !> pointer INTEGER :: j !> loop index INTEGER :: junk !> junk comm variable INTEGER :: buffer_size !> size of comm buffer REAL ( KIND = iwp ), ALLOCATABLE :: sbuffer (:) !> send buffer REAL ( KIND = iwp ), ALLOCATABLE :: rbuffer (:) !> send buffer INTEGER , ALLOCATABLE :: req (:) !> request var (non-blking comm) INTEGER , ALLOCATABLE :: status (:,:) !> size of MPI comm buffer !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- pack send buffer -- ALLOCATE ( sbuffer ( lcl_complex ( 2 )% num_send * dim_embbd )) DO j = 1 , lcl_complex ( 2 )% num_send sbuffer (( j - 1 ) * dim_embbd + 1 : j * dim_embbd ) = & lcl_complex ( 1 )% centers ( lcl_complex ( 2 )% send_indx ( j , 2 ), 1 : dim_embbd ) END DO !-- send data to adjacent processes -- ptr = 1 DO j = 1 , num_adj_proc IF ( num_send ( 1 , j ) == 0 ) CYCLE buffer_size = num_send ( 1 , j ) * dim_embbd CALL MPI_ISEND ( sbuffer ( ptr : ptr + buffer_size - 1 ), buffer_size ,& MPI_DOUBLE_PRECISION , adj_proc ( j ), 0 , MPI_COMM_WORLD , junk , ier ) CALL MPI_REQUEST_FREE ( junk , ier ) ptr = ptr + buffer_size END DO !-- receive data from adjacent processes -- ALLOCATE (& req ( num_adj_proc ),& status ( MPI_STATUS_SIZE , num_adj_proc ),& rbuffer ( lcl_complex ( 2 )% num_recv * dim_embbd )) ptr = 1 DO j = 1 , num_adj_proc IF ( num_recv ( 1 , j ) == 0 ) THEN req ( j ) = MPI_REQUEST_NULL CYCLE END IF buffer_size = num_recv ( 1 , j ) * dim_embbd CALL MPI_IRECV ( rbuffer ( ptr : ptr + buffer_size - 1 ), buffer_size ,& MPI_DOUBLE_PRECISION , adj_proc ( j ), 0 , MPI_COMM_WORLD , req ( j ), ier ) ptr = ptr + buffer_size END DO !-- unpack receive buffer -- CALL MPI_WAITALL ( num_adj_proc , req , status , ier ) DO j = 1 , lcl_complex ( 2 )% num_recv lcl_complex ( 1 )% centers ( lcl_complex ( 2 )% recv_indx ( j , 2 ), 1 : dim_embbd ) = & rbuffer (( j - 1 ) * dim_embbd + 1 : j * dim_embbd ) END DO !-- clean up -- DEALLOCATE ( req , status , sbuffer , rbuffer ) !-- make sure all comms have completed -- CALL MPI_BARRIER ( MPI_COMM_WORLD , ier ) RETURN END SUBROUTINE ! io_mod|exchange_prml_nodes !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* partition_mod/exchange_glb_indx !* SYNOPSIS SUBROUTINE exchange_glb_indx ( k ) !* PURPOSE !*   Exchange global indices to adjacent process (ghost nodes) !* INPUTS !*   Name                    Description !*   k                       simplicial order !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2020/09/16: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2020/09/16 !> Exchange global indices to adjacent process (ghost nodes) !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- INTEGER , INTENT ( IN ) :: k !> simplicial order !-- local variables -- INTEGER :: kp !> simplicial order INTEGER :: ptr , ptrp !> pointer INTEGER :: i , j !> loop index INTEGER :: junk !> junk comm variable INTEGER :: buffer_size !> size of comm buffer INTEGER , ALLOCATABLE :: sbuffer (:) !> send buffer INTEGER , ALLOCATABLE :: rbuffer (:) !> receive buffer INTEGER , ALLOCATABLE :: req (:) !> request var (non-blking comm) INTEGER , ALLOCATABLE :: status (:,:) !> size of MPI comm buffer !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- pack send buffer -- kp = k + 1 ALLOCATE ( sbuffer ( lcl_complex ( kp )% num_send )) DO j = 1 , lcl_complex ( kp )% num_send sbuffer ( j ) = lcl_complex ( k )% glb_indx ( lcl_complex ( kp )% send_indx ( j , 2 )) END DO !-- send data to adjacent processes -- ptr = 1 DO j = 1 , num_adj_proc IF ( num_send ( k , j ) == 0 ) CYCLE buffer_size = num_send ( k , j ) CALL MPI_ISEND ( sbuffer ( ptr : ptr + buffer_size - 1 ), buffer_size ,& MPI_INTEGER , adj_proc ( j ), 0 , MPI_COMM_WORLD , junk , ier ) CALL MPI_REQUEST_FREE ( junk , ier ) ptr = ptr + buffer_size END DO !-- receive data from adjacent processes -- ALLOCATE (& req ( num_adj_proc ),& status ( MPI_STATUS_SIZE , num_adj_proc ),& rbuffer ( lcl_complex ( kp )% num_recv )) ptr = 1 DO j = 1 , num_adj_proc IF ( num_recv ( k , j ) == 0 ) THEN req ( j ) = MPI_REQUEST_NULL CYCLE END IF buffer_size = num_recv ( k , j ) CALL MPI_IRECV ( rbuffer ( ptr : ptr + buffer_size - 1 ), buffer_size ,& MPI_INTEGER , adj_proc ( j ), 0 , MPI_COMM_WORLD , req ( j ), ier ) ptr = ptr + buffer_size END DO !-- unpack receive buffer -- CALL MPI_WAITALL ( num_adj_proc , req , status , ier ) DO j = 1 , lcl_complex ( kp )% num_recv lcl_complex ( k )% glb_indx ( lcl_complex ( kp )% recv_indx ( j , 2 )) = rbuffer ( j ) END DO !-- clean up -- DEALLOCATE ( req , status ) !-- make sure all comms have completed -- CALL MPI_BARRIER ( MPI_COMM_WORLD , ier ) RETURN END SUBROUTINE ! io_mod|exchange_glb_indx !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! END MODULE ! partition_mod !=============================================================================== !","tags":"","loc":"sourcefile/partition_mod.f90.html","title":"partition_mod.f90 – ParaGEMS"},{"text":"This file depends on sourcefile~~test_mpi.f90~~EfferentGraph sourcefile~test_mpi.f90 test_mpi.f90 sourcefile~common_mod.f90 common_mod.f90 sourcefile~test_mpi.f90:->sourcefile~common_mod.f90: sourcefile~mpi_mod.f90 mpi_mod.f90 sourcefile~test_mpi.f90:->sourcefile~mpi_mod.f90: sourcefile~mpi_mod.f90:->sourcefile~common_mod.f90: Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules test_mpi Source Code test_mpi.f90 Source Code ! !=============================================================================== ! /****m*/ test_mpi MODULE test_mpi ! ! PURPOSE:    Module contains tests for mpi_mod module ! ! CONTAINS:   Subroutine            Purpose !             t_start_mpi()         - start MPI execution environment & get details !             t_end_mpi()           - end MPI execution environment & stop ParaGEMS !             t_parallel_setup()    - sets up partitioning, read mesh and nodal !                                   locations, and sets connectivity !             t_partition()         - partitions work across available processes !             t_get_connectivity()  - gets process connectivity ! ! UPDATES:  created (PDB) :: 2019/08/21 ! !=============================================================================== !----------------------------------------------------------------------------- ! use statements and implicit none !----------------------------------------------------------------------------- USE common_mod USE mpi_mod IMPLICIT NONE !=============================================================================== CONTAINS !=============================================================================== ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! /****s*/ /src/mpi/modules/mpi_test|t_start_mpi SUBROUTINE t_start_mpi () ! ! PURPOSE:  Start MPI execution environment & get details ! ! TESTS:    Number  Purpose ! ! UPDATES:  created (PDB) :: 2019/08/21 ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !--------------------------------------------------------------------- !  1. start MPI !--------------------------------------------------------------------- CALL MPI_INIT ( ier ) IF ( ier /= MPI_SUCCESS ) THEN WRITE ( * , '(A,A,I5)' ) 'Error in MPI_INIT: errcode = ' , ier STOP \"ParaGEMS: shutdown\" END IF !--------------------------------------------------------------------- !  2. get local rank !--------------------------------------------------------------------- CALL MPI_COMM_RANK ( MPI_COMM_WORLD , rank , ier ) IF ( ier /= MPI_SUCCESS ) THEN WRITE ( * , '(A,A,I5)' ) 'Error in MPI_COMM_RANK: errcode = ' , ier CALL end_mpi () END IF !--------------------------------------------------------------------- !  3. get the global number of processes !--------------------------------------------------------------------- CALL MPI_COMM_SIZE ( MPI_COMM_WORLD , num_procs , ier ) IF ( ier /= MPI_SUCCESS ) THEN WRITE ( * , '(A,A,I5)' ) 'Error in MPI_COMM_SIZE: errcode = ' , ier CALL end_mpi () END IF RETURN END SUBROUTINE ! mpi_test|t_start_mpi !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! /****s*/ /src/modules/mpi/mpi_test|t_end_mpi SUBROUTINE t_end_mpi () ! ! PURPOSE:  End MPI execution environment & stop ParaGEMS ! ! TESTS: ! ! UPDATES:  created (PDB) :: 2019/08/21 ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !--------------------------------------------------------------------- !  1. end MPI execution environment and stop ParaGEMS !--------------------------------------------------------------------- CALL MPI_FINALIZE ( ier ) STOP \"ParaGEMS: shutdown: the program terminated successfully\" RETURN END SUBROUTINE ! mpi_test|t_end_mpi !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! /****s*/ /src/modules/common/common_test|t_parallel_setup SUBROUTINE t_parallel_setup () ! ! PURPOSE:  sets up partitioning, read mesh and nodal locations, and sets !           connectivity ! ! TESTS: ! ! UPDATES:  created (PDB) :: 2019/08/21 ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ RETURN END SUBROUTINE ! common_test|t_parallel_setup !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! /****s*/ /src/modules/common/common_test|t_partition SUBROUTINE t_partition () ! ! PURPOSE: ! ! TESTS: ! ! UPDATES:  created (PDB) :: 2019/08/21 ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !--------------------------------------------------------------------- ! local variables !--------------------------------------------------------------------- CHARACTER ( LEN = slen ) :: fname ! file name INTEGER :: extras ! modulus of the number of volumes ! and available processes INTEGER :: int_buffer ( 2 ) ! integer buffer for MPI comms INTEGER :: junk ! junk IO variable !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ RETURN END SUBROUTINE ! common_test|t_partition !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! END MODULE ! mpi_test !=============================================================================== !","tags":"","loc":"sourcefile/test_mpi.f90.html","title":"test_mpi.f90 – ParaGEMS"},{"text":"This file depends on sourcefile~~test_partition_mod.f90~~EfferentGraph sourcefile~test_partition_mod.f90 test_partition_mod.f90 sourcefile~partition_mod.f90 partition_mod.f90 sourcefile~test_partition_mod.f90:->sourcefile~partition_mod.f90: sourcefile~common_mod.f90 common_mod.f90 sourcefile~partition_mod.f90:->sourcefile~common_mod.f90: sourcefile~dec_mod.f90 dec_mod.f90 sourcefile~partition_mod.f90:->sourcefile~dec_mod.f90: sourcefile~io_mod.f90 io_mod.f90 sourcefile~partition_mod.f90:->sourcefile~io_mod.f90: sourcefile~math_mod.f90 math_mod.f90 sourcefile~partition_mod.f90:->sourcefile~math_mod.f90: sourcefile~mpi_mod.f90 mpi_mod.f90 sourcefile~partition_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~dec_mod.f90:->sourcefile~common_mod.f90: sourcefile~dec_mod.f90:->sourcefile~math_mod.f90: sourcefile~dec_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~io_mod.f90:->sourcefile~common_mod.f90: sourcefile~io_mod.f90:->sourcefile~dec_mod.f90: sourcefile~io_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~solver_mod.f90 solver_mod.f90 sourcefile~io_mod.f90:->sourcefile~solver_mod.f90: sourcefile~math_mod.f90:->sourcefile~common_mod.f90: sourcefile~math_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~mpi_mod.f90:->sourcefile~common_mod.f90: sourcefile~solver_mod.f90:->sourcefile~common_mod.f90: sourcefile~solver_mod.f90:->sourcefile~mpi_mod.f90: Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules test_partition_mod Source Code test_partition_mod.f90 Source Code ! !=============================================================================== ! /****m*/ /src/modules/common/common_test MODULE test_partition_mod ! ! PURPOSE:    Tests for common_mod module ! ! TESTS: ! ! UPDATES:  created (PDB) :: 2019/08/21 ! !=============================================================================== !----------------------------------------------------------------------------- ! use statements and implicit none !----------------------------------------------------------------------------- USE partition_mod IMPLICIT NONE !=============================================================================== CONTAINS !=============================================================================== ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! /****s*/ /src/common/common_test|test_vars SUBROUTINE test_vars ( cnt ) ! ! PURPOSE: ! ! UPDATES:  created (PDB) :: 2019/08/21 ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! TO DO: ! - global variables? ! - primary element data structure ! - clean up !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !--------------------------------------------------------------------- ! arguments !--------------------------------------------------------------------- INTEGER , INTENT ( INOUT ) :: cnt ! test counter !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !--------------------------------------------------------------------- ! 1. precision !--------------------------------------------------------------------- cnt = cnt + 1 IF ( 1.d0 - 9.999999999999999d-1 > 0.d0 ) THEN WRITE ( * , '(A,I5,A)' ) 'common_test : ' , cnt ,& ' : v/      PASSED : precision accurate to 16 decimal places' ELSE WRITE ( * , '(A,I5,A)' ) 'common_test : ' , cnt ,& ' : XXXXXX  FAILED : precision NOT accurate to 16 decimal places' END IF !--------------------------------------------------------------------- ! 2. global variables ! 3. global mpi ! 4. global io ! 5. global mesh ! 6. primary element ! 7. variable clean up !--------------------------------------------------------------------- END SUBROUTINE ! common_test|test_vars !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! END MODULE ! common_test !=============================================================================== !","tags":"","loc":"sourcefile/test_partition_mod.f90.html","title":"test_partition_mod.f90 – ParaGEMS"},{"text":"Module contains routines for controlling MPI execution environment This file depends on sourcefile~~mpi_mod.f90~~EfferentGraph sourcefile~mpi_mod.f90 mpi_mod.f90 sourcefile~common_mod.f90 common_mod.f90 sourcefile~mpi_mod.f90:->sourcefile~common_mod.f90: Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Files dependent on this one sourcefile~~mpi_mod.f90~~AfferentGraph sourcefile~mpi_mod.f90 mpi_mod.f90 sourcefile~darcy.f90 darcy.f90 sourcefile~darcy.f90:->sourcefile~mpi_mod.f90: sourcefile~darcy_mod.f90 darcy_mod.f90 sourcefile~darcy.f90:->sourcefile~darcy_mod.f90: sourcefile~solver_mod.f90 solver_mod.f90 sourcefile~darcy.f90:->sourcefile~solver_mod.f90: sourcefile~partition_mod.f90 partition_mod.f90 sourcefile~darcy.f90:->sourcefile~partition_mod.f90: sourcefile~darcy_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~io_mod.f90 io_mod.f90 sourcefile~darcy_mod.f90:->sourcefile~io_mod.f90: sourcefile~darcy_mod.f90:->sourcefile~solver_mod.f90: sourcefile~io_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~io_mod.f90:->sourcefile~solver_mod.f90: sourcefile~dec_mod.f90 dec_mod.f90 sourcefile~io_mod.f90:->sourcefile~dec_mod.f90: sourcefile~darcy_crkp_2f_old.f90 darcy_crkp_2f_old.f90 sourcefile~darcy_crkp_2f_old.f90:->sourcefile~mpi_mod.f90: sourcefile~darcy_crkp_2f_old.f90:->sourcefile~darcy_mod.f90: sourcefile~darcy_crkp_2f_old.f90:->sourcefile~solver_mod.f90: sourcefile~darcy_crkp_2f_old.f90:->sourcefile~partition_mod.f90: sourcefile~darcy_crkp_2f.f90 darcy_crkp_2f.f90 sourcefile~darcy_crkp_2f.f90:->sourcefile~mpi_mod.f90: sourcefile~darcy_crkp_2f.f90:->sourcefile~darcy_mod.f90: sourcefile~darcy_crkp_2f.f90:->sourcefile~solver_mod.f90: sourcefile~darcy_crkp_2f.f90:->sourcefile~partition_mod.f90: sourcefile~solver_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~math_mod.f90 math_mod.f90 sourcefile~math_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~darcy_crkp_1f.f90 darcy_crkp_1f.f90 sourcefile~darcy_crkp_1f.f90:->sourcefile~mpi_mod.f90: sourcefile~darcy_crkp_1f.f90:->sourcefile~darcy_mod.f90: sourcefile~darcy_crkp_1f.f90:->sourcefile~solver_mod.f90: sourcefile~darcy_crkp_1f.f90:->sourcefile~partition_mod.f90: sourcefile~partition_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~partition_mod.f90:->sourcefile~io_mod.f90: sourcefile~partition_mod.f90:->sourcefile~math_mod.f90: sourcefile~partition_mod.f90:->sourcefile~dec_mod.f90: sourcefile~test_mpi.f90 test_mpi.f90 sourcefile~test_mpi.f90:->sourcefile~mpi_mod.f90: sourcefile~darcy_2f.f90 darcy_2f.f90 sourcefile~darcy_2f.f90:->sourcefile~mpi_mod.f90: sourcefile~darcy_2f.f90:->sourcefile~darcy_mod.f90: sourcefile~darcy_2f.f90:->sourcefile~solver_mod.f90: sourcefile~darcy_2f.f90:->sourcefile~partition_mod.f90: sourcefile~dec_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~dec_mod.f90:->sourcefile~math_mod.f90: sourcefile~darcy_1f.f90 darcy_1f.f90 sourcefile~darcy_1f.f90:->sourcefile~mpi_mod.f90: sourcefile~darcy_1f.f90:->sourcefile~darcy_mod.f90: sourcefile~darcy_1f.f90:->sourcefile~solver_mod.f90: sourcefile~darcy_1f.f90:->sourcefile~partition_mod.f90: sourcefile~test_math_mod.f90 test_math_mod.f90 sourcefile~test_math_mod.f90:->sourcefile~math_mod.f90: sourcefile~test_io_mod.f90 test_io_mod.f90 sourcefile~test_io_mod.f90:->sourcefile~io_mod.f90: sourcefile~test_solver_mod.f90 test_solver_mod.f90 sourcefile~test_solver_mod.f90:->sourcefile~solver_mod.f90: sourcefile~test_dec_mod.f90 test_dec_mod.f90 sourcefile~test_dec_mod.f90:->sourcefile~dec_mod.f90: sourcefile~test_darcy_mod.f90 test_darcy_mod.f90 sourcefile~test_darcy_mod.f90:->sourcefile~darcy_mod.f90: sourcefile~test_partition_mod.f90 test_partition_mod.f90 sourcefile~test_partition_mod.f90:->sourcefile~partition_mod.f90: Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules mpi_mod Source Code mpi_mod.f90 Source Code ! !=============================================================================== !-- MPI Module !> Module contains routines for controlling MPI execution environment !=============================================================================== !/****/h* modules|mpi/mpi_mod !* SYNOPSIS MODULE mpi_mod !* PURPOSE !*   Module contains routines for controlling MPI execution environment !* INCLUDES !*   Name                    Purpose !*   common_mod              variable definitions !* CONTAINS !*   Subroutine              Purpose !*   start_mpi()             start MPI execution environment & get details !*   end_mpi()               end MPI execution environment & stop ParaGEMS !*   elm2proc()              convert partitioning element to process num (rank) !*   open_log                Open log file from root process & write header !*   close_log               Close log file from root process !*   rootwrite_log           Write unsynchronised message to log from root !*   write_log               Write unsynchronised messages to log from all ranks !*   syncwrite_log           Write synchronised message to log from root !*   syncwrite_log_mpidata   Write a synchronised message to log with num ranks !*   syncwrite_log_time      Write a synchronised message to log with timings !*   open_unsteady_log       Open log file for unsteady simulations from root !*                           and write header !*   close_unsteady_log      Close log file for unsteady simulations from root !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2020/09/16: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2020/09/16 !> Module contains routines for controlling MPI execution environment !=============================================================================== !-- use statements and implicit none -- USE common_mod IMPLICIT NONE !=============================================================================== CONTAINS !=============================================================================== ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* mpi_mod/start_mpi !* SYNOPSIS SUBROUTINE start_mpi () !* PURPOSE !*   Start MPI execution environment & get details !* SIDE EFFECTS !*   - MPI execution environment started, rank identified, and number of !*     processes evaluated !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2020/09/16: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2020/09/16 !> Start MPI execution environment & get details !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- start MPI -- CALL MPI_INIT ( ier ) IF ( ier /= MPI_SUCCESS ) THEN WRITE ( * , '(A,A,I5)' ) 'Error in MPI_INIT: errcode = ' , ier STOP 'ParaGEMS: shutdown' END IF !-- get local rank -- CALL MPI_COMM_RANK ( MPI_COMM_WORLD , rank , ier ) IF ( ier /= MPI_SUCCESS ) THEN WRITE ( * , '(A,A,I5)' ) 'Error in MPI_COMM_RANK: errcode = ' , ier CALL end_mpi () END IF !-- get the global number of processes -- CALL MPI_COMM_SIZE ( MPI_COMM_WORLD , num_procs , ier ) IF ( ier /= MPI_SUCCESS ) THEN WRITE ( * , '(A,A,I5)' ) 'Error in MPI_COMM_SIZE: errcode = ' , ier CALL end_mpi () END IF !-- record simumation start time -- CALL MPI_BARRIER ( MPI_COMM_WORLD , ier ) start_time = MPI_Wtime (); curnt_time = start_time !-- open paragems log file and write mpi data -- CALL open_log (); CALL syncwrite_log_mpidata () RETURN END SUBROUTINE ! mpi_mod/start_mpi !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* mpi_mod/end_mpi !* SYNOPSIS SUBROUTINE end_mpi () !* PURPOSE !*   End MPI execution environment & stop ParaGEMS !* SIDE EFFECTS !*   - MPI execution environment ended and ParaGEMS stoped !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2020/09/16: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2020/09/16 !> End MPI execution environment & stop ParaGEMS !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- local variables -- CHARACTER ( LEN = slen ) :: str_out !> STOP output string !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- end MPI execution environment and stop ParaGEMS -- WRITE ( str_out , '(A,A)' ) & 'ParaGEMS: shutdown: the program terminated successfully' CALL syncwrite_log ( str_out ); CALL syncwrite_log_time (); CALL close_log () CALL MPI_FINALIZE ( ier ) IF ( ier /= MPI_SUCCESS ) & WRITE ( * , '(A,A,I5)' ) 'Error in MPI_COMM_SIZE: errcode = ' , ier STOP RETURN END SUBROUTINE ! mpi_mod/end_mpi !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/f* mpi_mod/elm2proc !* SYNOPSIS FUNCTION elm2proc ( id ) !* PURPOSE !*   Convert partitioning element to process number (rank) !* INPUTS !*   Name                    Description !*   id                      global vertex index !* OUTPUTS !*   Name                    Description !*   id                      global vertex index !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2020/09/16: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2020/09/16 !> Convert partitioning element to process number (rank) !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- INTEGER , INTENT ( IN ) :: id !> INTEGER :: elm2proc !> !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- identify process associated with given primary index -- IF ( extra_pelm == 0 . OR . id < extra_pelm * num_pelm_pp ( 1 )) THEN elm2proc = ( id - 1 ) / num_pelm_pp ( 1 ) ELSE elm2proc = ( id - extra_pelm - 1 ) / ( num_pelm_pp ( 1 ) - 1 ) END IF RETURN END FUNCTION ! mpi_mod/elm2proc !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* mpi_mod/open_log !* SYNOPSIS SUBROUTINE open_log () !* PURPOSE !*   Open log file from root process and write ParaGEMS header !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2020/09/16: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2020/09/16 !> Open log file from root process and write ParaGEMS header !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- local variables -- CHARACTER ( LEN = slen ) :: fname !> file name LOGICAL :: err = . FALSE . !> error variable integer :: i = 0 !> counter LOGICAL :: fexists !> logical for file existence !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- on root process -- IF ( rank == root ) THEN !-- Open log file and check for errors -- fname = trim ( log_prefix ) // '.log' INQUIRE ( FILE = fname , EXIST = fexists ) IF ( fexists ) THEN i = 1 DO WHILE ( fexists ) IF ( i < 10 ) THEN WRITE ( fname , '(A,I1)' ) trim ( log_prefix ) // '.log.' , i ELSEIF ( i < 100 ) THEN WRITE ( fname , '(A,I2)' ) trim ( log_prefix ) // '.log.' , i ELSEIF ( i < 100 ) THEN WRITE ( fname , '(A,I3)' ) trim ( log_prefix ) // '.log.' , i END IF INQUIRE ( FILE = fname , EXIST = fexists ) i = i + 1 END DO END IF OPEN ( log_unit , FILE = fname , STATUS = 'NEW' , ACTION = 'WRITE' , IOSTAT = ier ) IF ( ier /= 0 ) THEN WRITE ( * , '(A,A,A,I5)' ) 'Error opening ' , trim ( fname ), ': errcode = ' , ier err = . TRUE . END IF !-- write header -- WRITE ( log_unit , * ) '======================================================' WRITE ( log_unit , * ) 'ParaGEMS: Parallel GEometric Mechanics of Solids' WRITE ( log_unit , * ) '------------------------------------------------------' WRITE ( log_unit , * ) 'Developed by: Pieter Boom' WRITE ( log_unit , * ) 'University of Manchester' WRITE ( log_unit , * ) '======================================================' CALL FLUSH ( log_unit ) END IF !-- check for errors CALL MPI_BCAST ( err , 1 , MPI_LOGICAL , 0 , MPI_COMM_WORLD , ier ) IF ( err ) CALL end_mpi () RETURN END SUBROUTINE ! mpi_mod/open_log !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* mpi_mod/close_log !* SYNOPSIS SUBROUTINE close_log () !* PURPOSE !*   Close log file from root process !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2020/09/16: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2020/09/16 !> Close log file from root process !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- on root process: close log file -- IF ( rank == root ) CLOSE ( log_unit ) RETURN END SUBROUTINE ! mpi_mod/close_log !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* mpi_mod/rootwrite_log !* SYNOPSIS SUBROUTINE rootwrite_log ( msg ) !* PURPOSE !*   Write unsynchronised message from root process to log file !* INPUTS !*   Name                    Description !*   msg                     message string to be written to file !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2020/09/16: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2020/09/16 !> Write unsynchronised message from root process to log file !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- CHARACTER ( LEN =* ), INTENT ( IN ) :: msg !> message string !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- write message to log file -- IF ( rank == root ) THEN WRITE ( log_unit , * ) msg ; CALL FLUSH ( log_unit ) END IF RETURN END SUBROUTINE ! mpi_mod/rootwrite_log !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* mpi_mod/write_log !* SYNOPSIS SUBROUTINE write_log ( msg ) !* PURPOSE !*   Write unsynchronised messages collated from processes to log file !* INPUTS !*   Name                    Description !*   msg                     message string to be written to file !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2020/09/16: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2020/09/16 !> Write unsynchronised messages collated from processes to log file !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- CHARACTER ( LEN = slen ), INTENT ( IN ) :: msg !> message string !-- local variables -- INTEGER :: i !> counter INTEGER :: status ( MPI_STATUS_SIZE ) !> mpi status variable !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IF ( rank == root ) THEN !-- write message to log file -- WRITE ( log_unit , '(A,I8,A,A)' ) 'rank: ' , rank , ' : ' , msg DO i = 1 , num_procs - 1 CALL MPI_RECV ( msg , slen , MPI_CHARACTER , i , i , MPI_COMM_WORLD , status , ier ) WRITE ( log_unit , '(A,I8,A,A)' ) 'rank: ' , i , ' : ' , msg END DO CALL FLUSH ( log_unit ) ELSE CALL MPI_SEND ( msg , slen , MPI_CHARACTER , 0 , rank , MPI_COMM_WORLD , ier ) END IF RETURN END SUBROUTINE ! mpi_mod/write_log !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* mpi_mod/syncwrite_log !* SYNOPSIS SUBROUTINE syncwrite_log ( msg ) !* PURPOSE !*   Write synchronised message from root process to log file !* INPUTS !*   Name                    Description !*   msg                     message string to be written to file !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2020/09/16: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2020/09/16 !> Write uynchronised message from root process to log file !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- CHARACTER ( LEN =* ), INTENT ( IN ) :: msg !> message string !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- synchronise processes -- CALL MPI_BARRIER ( MPI_COMM_WORLD , ier ); !-- write message from root process -- IF ( rank == root ) THEN WRITE ( log_unit , * ) msg ; CALL FLUSH ( log_unit ) END IF RETURN END SUBROUTINE ! mpi_mod/syncwrite_log !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* mpi_mod/syncwrite_log_mpidata !* SYNOPSIS SUBROUTINE syncwrite_log_mpidata () !* PURPOSE !*   Write a synchronised message with number of mpi ranks to log file !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2020/09/16: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2020/09/16 !> Write a synchronised message with number of mpi ranks to log file !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- synchronise processes -- CALL MPI_BARRIER ( MPI_COMM_WORLD , ier ); !-- write message from root process -- IF ( rank == root ) THEN WRITE ( log_unit , * ) 'ParaGEMS: running: ' , num_procs , ' processes' CALL FLUSH ( log_unit ) END IF END SUBROUTINE ! mpi_mod/syncwrite_log_mpidata !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* mpi_mod/syncwrite_log_time !* SYNOPSIS SUBROUTINE syncwrite_log_time () !* PURPOSE !*   Write a synchronised message to log file with timings !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2020/09/16: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2020/09/16 !> Write a synchronised message to log file with timings !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- local variables -- CHARACTER ( LEN = slen ) :: log_msg !> message string REAL ( KIND = iwp ) :: time !> time variable !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- synchronise processes and get current time -- CALL MPI_BARRIER ( MPI_COMM_WORLD , ier ) time = MPI_WTIME () !-- write msg with difference in times -- WRITE ( log_msg , '(A,E10.3)' ) '   - TIME for process:     ' , time - curnt_time CALL rootwrite_log ( log_msg ) WRITE ( log_msg , '(A,E10.3)' ) '   - TIME from MPI start:  ' , time - start_time CALL rootwrite_log ( log_msg ) !-- update current time -- curnt_time = time RETURN END SUBROUTINE ! mpi_mod/syncwrite_log_time !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* mpi_mod/open_unsteady_log !* SYNOPSIS SUBROUTINE open_unsteady_log () !* PURPOSE !*   Open log file for unsteady simulations from root process and write header !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2020/09/16: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2020/09/16 !> Open log file for unsteady simulations from root process and write header !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- local variables -- CHARACTER ( LEN = slen ) :: fname !> file name LOGICAL :: err = . FALSE . !> error variable !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- on root process -- IF ( rank == root ) THEN !-- Open log file and check for errors -- fname = trim ( unstdy_prefix ) // '.log' OPEN ( ulog_unit , FILE = fname , STATUS = 'UNKNOWN' , ACTION = 'WRITE' , IOSTAT = ier ) IF ( ier /= 0 ) THEN WRITE ( * , '(A,A,A,I5)' ) 'Error opening ' , trim ( fname ), ': errcode = ' , ier err = . TRUE . END IF WRITE ( ulog_unit , * ) 'iter  max_flx indx   area   area&#94;(3/2)   csum_area   csum_area&#94;(3/2)' CALL FLUSH ( ulog_unit ) END IF !-- check for errors CALL MPI_BCAST ( err , 1 , MPI_LOGICAL , 0 , MPI_COMM_WORLD , ier ) IF ( err ) CALL end_mpi () RETURN END SUBROUTINE ! mpi_mod/open_unsteady_log !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* mpi_mod/close_unsteady_log !* SYNOPSIS SUBROUTINE close_unsteady_log () !* PURPOSE !*   Close log file for unsteady simulations from root !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2020/09/16: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2020/09/16 !> Close log file for unsteady simulations from root !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- on root process: close log file -- IF ( rank == root ) CLOSE ( ulog_unit ) RETURN END SUBROUTINE ! mpi_mod/close_unsteady_log !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! END MODULE ! mpi_mod !=============================================================================== !","tags":"","loc":"sourcefile/mpi_mod.f90.html","title":"mpi_mod.f90 – ParaGEMS"},{"text":"This file depends on sourcefile~~test_io_mod.f90~~EfferentGraph sourcefile~test_io_mod.f90 test_io_mod.f90 sourcefile~io_mod.f90 io_mod.f90 sourcefile~test_io_mod.f90:->sourcefile~io_mod.f90: sourcefile~common_mod.f90 common_mod.f90 sourcefile~io_mod.f90:->sourcefile~common_mod.f90: sourcefile~dec_mod.f90 dec_mod.f90 sourcefile~io_mod.f90:->sourcefile~dec_mod.f90: sourcefile~mpi_mod.f90 mpi_mod.f90 sourcefile~io_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~solver_mod.f90 solver_mod.f90 sourcefile~io_mod.f90:->sourcefile~solver_mod.f90: sourcefile~dec_mod.f90:->sourcefile~common_mod.f90: sourcefile~dec_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~math_mod.f90 math_mod.f90 sourcefile~dec_mod.f90:->sourcefile~math_mod.f90: sourcefile~mpi_mod.f90:->sourcefile~common_mod.f90: sourcefile~solver_mod.f90:->sourcefile~common_mod.f90: sourcefile~solver_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~math_mod.f90:->sourcefile~common_mod.f90: sourcefile~math_mod.f90:->sourcefile~mpi_mod.f90: Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules test_io_mod Source Code test_io_mod.f90 Source Code ! !=============================================================================== ! /****m*/ /src/modules/common/common_test MODULE test_io_mod ! ! PURPOSE:    Tests for common_mod module ! ! TESTS: ! ! UPDATES:  created (PDB) :: 2019/08/21 ! !=============================================================================== !----------------------------------------------------------------------------- ! use statements and implicit none !----------------------------------------------------------------------------- USE io_mod IMPLICIT NONE !=============================================================================== CONTAINS !=============================================================================== ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! /****s*/ /src/common/common_test|test_vars SUBROUTINE test_vars ( cnt ) ! ! PURPOSE: ! ! UPDATES:  created (PDB) :: 2019/08/21 ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! TO DO: ! - global variables? ! - primary element data structure ! - clean up !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !--------------------------------------------------------------------- ! arguments !--------------------------------------------------------------------- INTEGER , INTENT ( INOUT ) :: cnt ! test counter !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !--------------------------------------------------------------------- ! 1. precision !--------------------------------------------------------------------- cnt = cnt + 1 IF ( 1.d0 - 9.999999999999999d-1 > 0.d0 ) THEN WRITE ( * , '(A,I5,A)' ) 'common_test : ' , cnt ,& ' : v/      PASSED : precision accurate to 16 decimal places' ELSE WRITE ( * , '(A,I5,A)' ) 'common_test : ' , cnt ,& ' : XXXXXX  FAILED : precision NOT accurate to 16 decimal places' END IF !--------------------------------------------------------------------- ! 2. global variables ! 3. global mpi ! 4. global io ! 5. global mesh ! 6. primary element ! 7. variable clean up !--------------------------------------------------------------------- END SUBROUTINE ! common_test|test_vars !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! END MODULE ! common_test !=============================================================================== !","tags":"","loc":"sourcefile/test_io_mod.f90.html","title":"test_io_mod.f90 – ParaGEMS"},{"text":"Module contains subroutines for reading from and writing to disk This file depends on sourcefile~~io_mod.f90~~EfferentGraph sourcefile~io_mod.f90 io_mod.f90 sourcefile~common_mod.f90 common_mod.f90 sourcefile~io_mod.f90:->sourcefile~common_mod.f90: sourcefile~dec_mod.f90 dec_mod.f90 sourcefile~io_mod.f90:->sourcefile~dec_mod.f90: sourcefile~mpi_mod.f90 mpi_mod.f90 sourcefile~io_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~solver_mod.f90 solver_mod.f90 sourcefile~io_mod.f90:->sourcefile~solver_mod.f90: sourcefile~dec_mod.f90:->sourcefile~common_mod.f90: sourcefile~dec_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~math_mod.f90 math_mod.f90 sourcefile~dec_mod.f90:->sourcefile~math_mod.f90: sourcefile~mpi_mod.f90:->sourcefile~common_mod.f90: sourcefile~solver_mod.f90:->sourcefile~common_mod.f90: sourcefile~solver_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~math_mod.f90:->sourcefile~common_mod.f90: sourcefile~math_mod.f90:->sourcefile~mpi_mod.f90: Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Files dependent on this one sourcefile~~io_mod.f90~~AfferentGraph sourcefile~io_mod.f90 io_mod.f90 sourcefile~darcy_mod.f90 darcy_mod.f90 sourcefile~darcy_mod.f90:->sourcefile~io_mod.f90: sourcefile~partition_mod.f90 partition_mod.f90 sourcefile~partition_mod.f90:->sourcefile~io_mod.f90: sourcefile~test_io_mod.f90 test_io_mod.f90 sourcefile~test_io_mod.f90:->sourcefile~io_mod.f90: sourcefile~darcy.f90 darcy.f90 sourcefile~darcy.f90:->sourcefile~darcy_mod.f90: sourcefile~darcy.f90:->sourcefile~partition_mod.f90: sourcefile~darcy_1f.f90 darcy_1f.f90 sourcefile~darcy_1f.f90:->sourcefile~darcy_mod.f90: sourcefile~darcy_1f.f90:->sourcefile~partition_mod.f90: sourcefile~darcy_crkp_2f_old.f90 darcy_crkp_2f_old.f90 sourcefile~darcy_crkp_2f_old.f90:->sourcefile~darcy_mod.f90: sourcefile~darcy_crkp_2f_old.f90:->sourcefile~partition_mod.f90: sourcefile~darcy_crkp_2f.f90 darcy_crkp_2f.f90 sourcefile~darcy_crkp_2f.f90:->sourcefile~darcy_mod.f90: sourcefile~darcy_crkp_2f.f90:->sourcefile~partition_mod.f90: sourcefile~test_partition_mod.f90 test_partition_mod.f90 sourcefile~test_partition_mod.f90:->sourcefile~partition_mod.f90: sourcefile~darcy_crkp_1f.f90 darcy_crkp_1f.f90 sourcefile~darcy_crkp_1f.f90:->sourcefile~darcy_mod.f90: sourcefile~darcy_crkp_1f.f90:->sourcefile~partition_mod.f90: sourcefile~darcy_2f.f90 darcy_2f.f90 sourcefile~darcy_2f.f90:->sourcefile~darcy_mod.f90: sourcefile~darcy_2f.f90:->sourcefile~partition_mod.f90: sourcefile~test_darcy_mod.f90 test_darcy_mod.f90 sourcefile~test_darcy_mod.f90:->sourcefile~darcy_mod.f90: Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules io_mod Source Code io_mod.f90 Source Code ! !=============================================================================== !-- IO Module !> Module contains subroutines for reading from and writing to disk !=============================================================================== !/****/h* modules|io/io_mod !* SYNOPSIS MODULE io_mod !* PURPOSE !*   Module contains subroutines for reading from and writing to disk !* INCLUDES !*   Name                    Purpose !*   common_mod              variable definitions !*   mpi_mod                 ??? !*   dec_mod                 DEC functions !*   ieee_arithmetic !*   iso_fortran_env !8   petsc.h !* CONTAINS !*   Subroutine              Purpose !*   set_defaults_io()       set default values for io variables !*   read_prml_elms()        read local node indices of highest order primal elements !*   read_nodes_prml()       read the nodal locations of the primal mesh !*   write_solution()        write solution to disk !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/20: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/20 !> Module contains subroutines for reading from and writing to disk !=============================================================================== ! TO DO: ! - MPI-IO? !=============================================================================== !-- use statements and implicit none -- USE common_mod USE mpi_mod USE dec_mod USE solver_mod USE , INTRINSIC :: ieee_arithmetic , only : IEEE_Value , IEEE_QUIET_NAN USE , INTRINSIC :: iso_fortran_env , only : real32 IMPLICIT NONE !-- include PETSc variables -- #include <petsc/finclude/petsc.h> !=============================================================================== CONTAINS !=============================================================================== ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s*  io_mod/read_prml_elms !* SYNOPSIS SUBROUTINE read_prml_elms () !* PURPOSE !*   Read local node indices of highest order primal elements !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Read local node indices of highest order primal elements !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- local variables -- CHARACTER ( LEN = slen ) :: fname !> file name INTEGER :: junk !> junk IO variable INTEGER :: k !> simplicial order INTEGER :: ie , in !> loop indices (procs,elems) INTEGER :: buffer_size !> size of MPI comm buffer INTEGER , ALLOCATABLE :: int_buffer (:,:) !> integer buffer for MPI comms INTEGER , ALLOCATABLE :: req (:) !> request variable for non-blocking comms INTEGER , ALLOCATABLE :: status (:) !> size of MPI comm buffer INTEGER , ALLOCATABLE :: prml_elm (:,:) !> primal element list INTEGER :: num_dual_elm !> number of local dual element INTEGER , ALLOCATABLE :: dual_elm_in (:) !> node indices of dual element INTEGER :: proc , proc_old !> processor rank (current, past) INTEGER :: ptr !> pointer LOGICAL :: err = . FALSE . !> error variable !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- Open mesh file and check for errors -- fname = trim ( mesh_prefix ) // \".ele\" ; CALL root_open_file_read ( fname , mesh_unit ) IF ( rank == root ) THEN !-- on root process -- !-- read header and check for consistency -- READ ( mesh_unit , * ) junk , k IF ( glb_num_elm ( k ) /= junk ) THEN WRITE ( log_unit , '(A,A,A,I5,A,I5)' ) 'Error: mesh file' , trim ( fname ),& ': inconsistency: glb_num_elm(dim_cmplx+1)=' , glb_num_elm ( k ), '/=' , junk err = . TRUE . ELSEIF ( dim_cmplx /= k - 1 ) THEN WRITE ( log_unit , '(A,A,A,I5,A,I5)' ) 'Error: mesh file' , trim ( fname ),& ': inconsistcy: dim_cmplx=' , dim_cmplx , '/=' , k - 1 err = . TRUE . END IF END IF !-- check for errors CALL MPI_BCAST ( err , 1 , MPI_LOGICAL , 0 , MPI_COMM_WORLD , ier ) IF ( err ) CALL end_mpi () !-- set buffer size and allocate temp. storage for primal elements -- !-- (assume a maximum of 30 dual element neighbours) -- buffer_size = dim_cmplx + 3 ALLOCATE ( prml_elm ( 30 * num_elm ( 1 ), buffer_size )) IF ( rank == root ) THEN !-- on root process -- !-- allocate integer communication buffer -- ALLOCATE ( int_buffer ( buffer_size , glb_num_elm ( k ) + 1 )) !-- loop through primal elements -- ptr = 0 ; DO ie = 1 , glb_num_elm ( k ) !-- read nodal indices of primal element, sort and compute orientation -- READ ( mesh_unit , * ) int_buffer ( 1 : buffer_size - 1 , ie ) int_buffer ( 1 , ie ) = ie int_buffer ( 2 : buffer_size - 1 , ie ) = int_buffer ( 2 : buffer_size - 1 , ie ) + indx_offset ( 1 ) CALL calc_orientation ( int_buffer ( 2 : buffer_size - 1 , ie ), int_buffer ( buffer_size , ie )) !-- loop through dual elements -- proc_old = - 1 DO in = 2 , buffer_size - 1 !-- get process of dual element -- proc = elm2proc ( int_buffer ( in , ie )) IF ( proc == 0 . and . proc /= proc_old ) THEN !-- if local: assign to local array -- ptr = ptr + 1 ; prml_elm ( ptr ,:) = int_buffer (:, ie ) ELSEIF ( proc /= proc_old ) THEN !-- otherwise: send data to neighbouring process -- CALL MPI_ISEND ( int_buffer (:, ie ), buffer_size , MPI_INTEGER , proc , 0 ,& MPI_COMM_WORLD , junk , ier ) CALL MPI_REQUEST_FREE ( junk , ier ) END IF proc_old = proc END DO END DO !-- send communication termination message -- int_buffer (:, ie ) = - 1 DO proc = 1 , num_procs - 1 CALL MPI_ISEND ( int_buffer (:, ie ), buffer_size , MPI_INTEGER , proc , 0 ,& MPI_COMM_WORLD , junk , ier ) CALL MPI_REQUEST_FREE ( junk , ier ) END DO ELSE !-- on neighbouring process -- !-- allocate integer communication buffer and status variable -- ALLOCATE ( int_buffer ( buffer_size , 1 ), status ( MPI_STATUS_SIZE )) !-- poll for incoming MPI communications -- ptr = 0 DO !-- receive nodal indices (blocking) -- CALL MPI_RECV ( int_buffer (:, 1 ), buffer_size , MPI_INTEGER , 0 , 0 ,& MPI_COMM_WORLD , status , ier ) IF ( ALL ( int_buffer (:, 1 ) ==- 1 )) THEN !-- received termination message: stop polling for communication -- EXIT ELSE !-- assign to local array -- ptr = ptr + 1 ; prml_elm ( ptr ,:) = int_buffer (:, 1 ) END IF END DO END IF !-- allocate and assign primal element data to local structures -- ALLOCATE (& lcl_complex ( dim_cmplx + 1 )% glb_indx ( ptr ),& lcl_complex ( dim_cmplx + 1 )% orientation ( ptr ),& lcl_complex ( dim_cmplx + 1 )% node_indx ( ptr , dim_cmplx + 1 )) lcl_complex ( dim_cmplx + 1 )% glb_indx = prml_elm ( 1 : ptr , 1 ) lcl_complex ( dim_cmplx + 1 )% orientation = prml_elm ( 1 : ptr , buffer_size ) lcl_complex ( dim_cmplx + 1 )% node_indx = prml_elm ( 1 : ptr , 2 : buffer_size - 1 ) num_elm ( dim_cmplx + 1 ) = ptr !-- close file and clean up -- IF ( rank == root ) THEN CLOSE ( mesh_unit , IOSTAT = ier ) IF ( ier /= 0 ) THEN WRITE ( log_unit , '(A,A,A,I5)' ) '***Error: failed to close ' , trim ( fname ),& ': errcode = ' , ier !err = .TRUE. END IF DEALLOCATE ( prml_elm , int_buffer ) ELSE DEALLOCATE ( prml_elm , int_buffer , status ) END IF !-- make sure all comms have completed -- CALL MPI_BARRIER ( MPI_COMM_WORLD , ier ) RETURN END SUBROUTINE ! io_mod|read_prml_elms !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s*  io_mod/read_prml_elms2 !* SYNOPSIS SUBROUTINE read_prml_elms2 () !* PURPOSE !*   Read local node indices of highest order primal elements !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Read local node indices of highest order primal elements !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- local variables -- CHARACTER ( LEN = slen ) :: fname !> file name INTEGER :: junk !> junk IO variable INTEGER :: k !> simplicial order INTEGER :: ie , in !> loop indices (procs,elems) INTEGER :: buffer_size !> size of MPI comm buffer INTEGER , ALLOCATABLE :: int_buffer (:) !> integer buffer for MPI comms INTEGER , ALLOCATABLE :: req (:) !> request variable for non-blocking comms INTEGER , ALLOCATABLE :: status (:) !> size of MPI comm buffer INTEGER :: num_dual_elm !> number of local dual element INTEGER , ALLOCATABLE :: dual_elm_in (:) !> node indices of dual element INTEGER :: proc , proc_old !> processor rank (current, past) INTEGER :: ptr !> pointer LOGICAL :: err = . FALSE . !> error variable INTEGER :: l , orient , iib , fib , num_read_iters , i , j , & max_read_size , resid_read_size , read_size , stride , strt_indx !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- Open mesh file and check for errors -- fname = trim ( mesh_prefix ) // \".ele\" ; CALL root_open_file_read ( fname , mesh_unit ) IF ( rank == root ) THEN !-- on root process -- !-- read header and check for consistency -- READ ( mesh_unit , * ) junk , k IF ( glb_num_elm ( k ) /= junk ) THEN WRITE ( log_unit , '(A,A,A,I5,A,I5)' ) 'Error: mesh file' , trim ( fname ),& ': inconsistency: glb_num_elm(dim_cmplx+1)=' , glb_num_elm ( k ), '/=' , junk err = . TRUE . ELSEIF ( dim_cmplx /= k - 1 ) THEN WRITE ( log_unit , '(A,A,A,I5,A,I5)' ) 'Error: mesh file' , trim ( fname ),& ': inconsistcy: dim_cmplx=' , dim_cmplx , '/=' , k - 1 err = . TRUE . END IF END IF !-- check for errors CALL MPI_BCAST ( err , 1 , MPI_LOGICAL , 0 , MPI_COMM_WORLD , ier ) IF ( err ) CALL end_mpi () !-- get some basic information for reuse -- k = dim_cmplx + 1 max_read_size = 1000 ; read_size = max_read_size stride = k ; buffer_size = read_size * stride num_read_iters = glb_num_elm ( k ) / read_size resid_read_size = mod ( glb_num_elm ( k ), read_size ) ptr = 0 ; fib = stride - 1 ALLOCATE ( int_buffer ( read_size * ( stride + 1 ))) IF ( rank == root ) THEN !-- on root process -- !-- DO i = 0 , num_read_iters IF ( i == num_read_iters ) THEN read_size = resid_read_size buffer_size = read_size * stride END IF !-- iib = 1 DO j = 1 , read_size READ ( mesh_unit , * ) junk , int_buffer ( iib : iib + fib ) !-- int_buffer ( iib : iib + fib ) = int_buffer ( iib : iib + fib ) + indx_offset ( 1 ) iib = iib + stride END DO !-- CALL MPI_BCAST ( int_buffer , buffer_size , MPI_INTEGER , 0 , MPI_COMM_WORLD , ier ) !-- iib = 1 DO j = 1 , read_size IF ( minval ( int_buffer ( iib : iib + fib )) <= num_pelm_pp ( 1 )) ptr = ptr + 1 iib = iib + stride END DO END DO ELSE !-- on neighbouring processes -- !-- DO i = 0 , num_read_iters IF ( i == num_read_iters ) THEN read_size = resid_read_size buffer_size = read_size * stride END IF !-- CALL MPI_BCAST ( int_buffer , buffer_size , MPI_INTEGER , 0 , MPI_COMM_WORLD , ier ) !-- iib = 1 DO j = 1 , read_size DO l = 0 , dim_cmplx IF ( elm2proc ( int_buffer ( iib + l )) == rank ) THEN ptr = ptr + 1 EXIT END IF END DO iib = iib + stride END DO END DO END IF !-- allocate and assign primal element data to local structures -- ALLOCATE (& lcl_complex ( dim_cmplx + 1 )% glb_indx ( ptr ),& lcl_complex ( dim_cmplx + 1 )% orientation ( ptr ),& lcl_complex ( dim_cmplx + 1 )% node_indx ( ptr , dim_cmplx + 1 ),& lcl_complex ( dim_cmplx + 1 )% deflt_order ( ptr , dim_cmplx + 1 )) num_elm ( dim_cmplx + 1 ) = ptr !-- get some basic information for reuse -- read_size = max_read_size ; stride = stride + 1 buffer_size = read_size * stride ptr = 0 ; fib = fib + 1 IF ( rank == root ) THEN !-- on root process -- !-- REWIND ( mesh_unit ) READ ( mesh_unit , * ) junk !-- DO i = 0 , num_read_iters IF ( i == num_read_iters ) THEN read_size = resid_read_size buffer_size = read_size * stride END IF !-- iib = 1 DO j = 1 , read_size READ ( mesh_unit , * ) int_buffer ( iib : iib + fib ) !-- int_buffer ( iib ) = i * max_read_size + j ; int_buffer ( iib + 1 : iib + fib ) = & int_buffer ( iib + 1 : iib + fib ) + indx_offset ( 1 ) iib = iib + stride END DO !-- CALL MPI_BCAST ( int_buffer , buffer_size , MPI_INTEGER , 0 , MPI_COMM_WORLD , ier ) !-- iib = 1 DO j = 1 , read_size IF ( minval ( int_buffer ( iib + 1 : iib + fib )) <= num_pelm_pp ( 1 )) THEN ptr = ptr + 1 lcl_complex ( dim_cmplx + 1 )% glb_indx ( ptr ) = int_buffer ( iib ) lcl_complex ( dim_cmplx + 1 )% deflt_order ( ptr ,:) = int_buffer ( iib + 1 : iib + fib ) CALL calc_orientation ( int_buffer ( iib + 1 : iib + fib ), orient ) lcl_complex ( dim_cmplx + 1 )% node_indx ( ptr ,:) = int_buffer ( iib + 1 : iib + fib ) lcl_complex ( dim_cmplx + 1 )% orientation ( ptr ) = orient END IF iib = iib + stride END DO END DO !-- close file -- CLOSE ( mesh_unit , IOSTAT = ier ) IF ( ier /= 0 ) THEN WRITE ( log_unit , '(A,A,A,I5)' ) '***Error: failed to close ' , trim ( fname ),& ': errcode = ' , ier !err = .TRUE. END IF ELSE !-- on neighbouring processes -- !-- DO i = 0 , num_read_iters IF ( i == num_read_iters ) THEN read_size = resid_read_size buffer_size = read_size * stride END IF !-- CALL MPI_BCAST ( int_buffer , buffer_size , MPI_INTEGER , 0 , MPI_COMM_WORLD , ier ) !-- iib = 1 DO j = 1 , read_size DO l = 1 , dim_cmplx + 1 IF ( elm2proc ( int_buffer ( iib + l )) == rank ) THEN ptr = ptr + 1 lcl_complex ( dim_cmplx + 1 )% glb_indx ( ptr ) = int_buffer ( iib ) lcl_complex ( dim_cmplx + 1 )% deflt_order ( ptr ,:) = int_buffer ( iib + 1 : iib + fib ) CALL calc_orientation ( int_buffer ( iib + 1 : iib + fib ), orient ) lcl_complex ( dim_cmplx + 1 )% node_indx ( ptr ,:) = int_buffer ( iib + 1 : iib + fib ) lcl_complex ( dim_cmplx + 1 )% orientation ( ptr ) = orient EXIT END IF END DO iib = iib + stride END DO END DO END IF !-- clean up -- DEALLOCATE ( int_buffer ) !-- make sure all comms have completed -- CALL MPI_BARRIER ( MPI_COMM_WORLD , ier ) RETURN END SUBROUTINE ! io_mod|read_prml_elms2 !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s*  io_mod/read_glb_indx !* SYNOPSIS SUBROUTINE read_glb_indx ( k ) !* PURPOSE !*   Read glbal indices of elements of given geometric order !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Read glbal indices of elements of given geometric order !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- INTEGER , INTENT ( IN ) :: k !> simplicial order !-- local variables -- CHARACTER ( LEN = slen ) :: fname !> file name INTEGER :: junk !> junk IO variable INTEGER :: ie , in , i , j !> loop indices (procs,elems) INTEGER :: buffer_size !> size of MPI comm buffer INTEGER , ALLOCATABLE :: int_buffer (:,:) !> integer buffer for MPI comms INTEGER , ALLOCATABLE :: req (:) !> request variable for non-blocking comms INTEGER , ALLOCATABLE :: status (:) !> size of MPI comm buffer INTEGER , ALLOCATABLE :: prml_elm (:,:) !> primal element list INTEGER :: proc , proc_old !> processor rank (current, past) INTEGER :: ptr !> pointer !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- build file name -- SELECT CASE ( k ) CASE ( 1 ) fname = trim ( mesh_prefix ) // \".node\" CASE ( 2 ) fname = trim ( mesh_prefix ) // \".edge\" CASE ( 3 ) fname = trim ( mesh_prefix ) // \".face\" CASE DEFAULT WRITE ( log_unit , '(A,A,A,I5)' ) '***Error: invalid geometric order (0<k<dim_cmplx+1): ' , k CALL end_mpi () END SELECT !-- open mesh file and check for errors -- CALL root_open_file_read ( fname , mesh_unit ) !-- read header and broadcast global number of entities -- IF ( rank == root ) READ ( mesh_unit , * ) glb_num_elm ( k ) CALL MPI_BCAST ( glb_num_elm ( k ), 1 , MPI_INTEGER , 0 , MPI_COMM_WORLD , ier ) !-- set buffer size and allocate temp. storage for primal elements -- !-- (assume a maximum of 30 dual element neighbours) -- buffer_size = k + 1 ALLOCATE ( prml_elm ( num_elm ( k ), buffer_size )) IF ( rank == root ) THEN !-- on root process -- !-- first element to get index offset READ ( mesh_unit , * ) junk indx_offset ( k ) = 1 - junk BACKSPACE ( mesh_unit ) !-- allocate integer communication buffer -- ALLOCATE ( int_buffer ( buffer_size , glb_num_elm ( k ) + 1 )) !-- loop through primal elements -- ptr = 0 ; DO ie = 1 , glb_num_elm ( k ) !-- read nodal indices of primal element and sort -- READ ( mesh_unit , * ) int_buffer (:, ie ) int_buffer ( 1 , ie ) = ie int_buffer ( 2 : buffer_size , ie ) = int_buffer ( 2 : buffer_size , ie ) + indx_offset ( 1 ) CALL int_insertion_sort ( int_buffer ( 2 : buffer_size , ie )) !-- loop through dual elements -- proc_old = - 1 DO in = 2 , buffer_size !-- get process of dual element -- proc = elm2proc ( int_buffer ( in , ie )) IF ( proc == 0 . and . proc /= proc_old ) THEN !-- if local: assign to local array -- ptr = ptr + 1 ; prml_elm ( ptr ,:) = int_buffer (:, ie ) ELSEIF ( proc /= proc_old ) THEN !-- otherwise: send data to neighbouring process -- CALL MPI_ISEND ( int_buffer (:, ie ), buffer_size , MPI_INTEGER , proc , 0 ,& MPI_COMM_WORLD , junk , ier ) CALL MPI_REQUEST_FREE ( junk , ier ) END IF proc_old = proc END DO END DO !-- send communication termination message -- int_buffer (:, ie ) = - 1 DO proc = 1 , num_procs - 1 CALL MPI_ISEND ( int_buffer (:, ie ), buffer_size , MPI_INTEGER , proc , 0 ,& MPI_COMM_WORLD , junk , ier ) CALL MPI_REQUEST_FREE ( junk , ier ) END DO ELSE !-- on neighbouring process -- !-- allocate integer communication buffer and status variable -- ALLOCATE ( int_buffer ( buffer_size , 1 ), status ( MPI_STATUS_SIZE )) !-- poll for incoming MPI communications -- ptr = 0 DO !-- receive nodal indices (blocking) -- CALL MPI_RECV ( int_buffer (:, 1 ), buffer_size , MPI_INTEGER , 0 , 0 ,& MPI_COMM_WORLD , status , ier ) IF ( ALL ( int_buffer ==- 1 )) THEN !-- received termination message: stop polling for communication -- EXIT ELSE !-- assign to local array -- ptr = ptr + 1 ; prml_elm ( ptr ,:) = int_buffer (:, 1 ) END IF END DO END IF !-- assign primal element data to local structures -- DO i = 1 , ptr j = 1 DO IF ( ALL ( prml_elm ( i , 2 : buffer_size ) == lcl_complex ( k )% node_indx ( j ,:))) THEN lcl_complex ( k )% glb_indx ( j ) = prml_elm ( i , 1 ) EXIT END IF j = j + 1 END DO END DO !-- close file and clean up -- IF ( rank == root ) THEN CLOSE ( mesh_unit , IOSTAT = ier ) IF ( ier /= 0 ) THEN WRITE ( log_unit , '(A,A,A,I5)' ) '***Error: failed to close ' , trim ( fname ),& ': errcode = ' , ier !err = .TRUE. END IF DEALLOCATE ( prml_elm , int_buffer ) ELSE DEALLOCATE ( prml_elm , int_buffer , status ) END IF !-- make sure all comms have completed -- CALL MPI_BARRIER ( MPI_COMM_WORLD , ier ) !-- broadcast index offset variable -- CALL MPI_BCAST ( indx_offset ( k ), 1 , MPI_INTEGER , 0 , MPI_COMM_WORLD , ier ) RETURN END SUBROUTINE ! io_mod|read_glb_indx !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s*  io_mod/read_nodes_prml !* SYNOPSIS SUBROUTINE read_nodes_prml () !* PURPOSE !*   Root process reads the location of primal nodes !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Root process reads the location of primal nodes !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- local variables -- CHARACTER ( LEN = slen ) :: fname !> file name INTEGER :: junk !> junk IO variable INTEGER :: k !> simplicial order INTEGER :: ip , ie !> loop indices (procs,elems) INTEGER :: buffer_size !> size of MPI comm buffer REAL ( KIND = iwp ), ALLOCATABLE :: real_buffer (:) !> buffer for MPI comms INTEGER :: iib , fib !> initial/final index for MPI comm buffer INTEGER , ALLOCATABLE :: status (:) !> size of MPI comm buffer INTEGER :: ptr , ptrp !> LOGICAL :: err = . FALSE . !> error variable !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- open mesh file and check for errors -- fname = trim ( mesh_prefix ) // \".node\" ; CALL root_open_file_read ( fname , mesh_unit ) !-- set buffer size and allocate buffer -- buffer_size = maxval ( num_pelm_pp ) * ( dim_embbd + 1 ) ALLOCATE ( real_buffer ( buffer_size )) !-- allocate storage for nodal locations -- ALLOCATE ( lcl_complex ( 1 )% centers ( num_elm ( 1 ), dim_embbd )) IF ( rank == root ) THEN !-- on root process -- !-- read header and check for consistency -- READ ( mesh_unit , * ) junk , k IF ( glb_num_elm ( 1 ) /= junk ) THEN WRITE ( log_unit , '(A,A,A,I5,A,I5)' ) 'Error: mesh file' , trim ( fname ),& ': inconsistency: glb_num_elm(1)=' , glb_num_elm ( 1 ), '/=' , junk err = . TRUE . ELSEIF ( dim_embbd /= k ) THEN WRITE ( log_unit , '(A,A,A,I5,A,I5)' ) 'Error: mesh file' , trim ( fname ),& ': inconsistcy: dim_embbd=' , dim_embbd , '/=' , k err = . TRUE . END IF END IF !-- check for errors CALL MPI_BCAST ( err , 1 , MPI_LOGICAL , 0 , MPI_COMM_WORLD , ier ) IF ( err ) CALL end_mpi () IF ( rank == root ) THEN !-- on root process -- !-- advance to neighbour data -- DO ie = 1 , num_pelm_pp ( 1 ) READ ( mesh_unit , * ) END DO !-- loop through neighbouring processes -- DO ip = 1 , num_procs - 1 !-- loop through elements on neighbouring process -- iib = 1 ; fib = dim_embbd + 1 DO ie = 1 , num_pelm_pp ( ip + 1 ) !-- read nodal location of elements -- READ ( mesh_unit , * ) real_buffer ( iib : fib ) !-- update indices -- iib = iib + dim_embbd + 1 ; fib = fib + dim_embbd + 1 END DO !-- send nodal locations (non-blocking) -- buffer_size = num_pelm_pp ( ip + 1 ) * ( dim_embbd + 1 ) CALL MPI_SEND ( real_buffer , buffer_size , MPI_DOUBLE_PRECISION , ip , ip ,& MPI_COMM_WORLD , ier ) END DO !-- read local data -- REWIND ( mesh_unit ) READ ( mesh_unit , * ) !-- header -- !-- loop through local primary elements -- ptr = 1 DO ie = 1 , num_pelm_pp ( 1 ) !-- read nodal location of elements -- READ ( mesh_unit , * ) junk , lcl_complex ( 1 )% centers ( ptr ,:) !-- find correct location in data structure -- ptrp = ptr ; DO IF ( junk + indx_offset ( 1 ) == lcl_complex ( 1 )% node_indx ( ptrp , 1 )) EXIT ptrp = ptrp + 1 END DO !-- shuffle data and zero locations from neighbouring processes -- IF ( ptrp > ptr ) THEN lcl_complex ( 1 )% centers ( ptrp ,:) = lcl_complex ( 1 )% centers ( ptr ,:) lcl_complex ( 1 )% centers ( ptr : ptrp - 1 ,:) = 0.d0 END IF ptr = ptrp + 1 END DO !-- zero remaining locations from neighbouring processes -- lcl_complex ( 1 )% centers ( ptr : num_elm ( 1 ),:) = 0.d0 !-- on neighbouring process -- ELSE !-- allocate MPI comm status variable -- ALLOCATE ( status ( MPI_STATUS_SIZE )) !-- receive nodal indices (blocking) -- buffer_size = num_pelm_pp ( rank + 1 ) * ( dim_embbd + 1 ) CALL MPI_RECV ( real_buffer , buffer_size , MPI_DOUBLE_PRECISION , 0 , rank ,& MPI_COMM_WORLD , status , ier ) !-- unpack nodal indices in to local element structure -- iib = 1 ; fib = dim_embbd + 1 ; ptr = 1 DO ie = 1 , num_pelm_pp ( rank + 1 ) !-- read nodal indices of element and temporarily assign to data structure -- junk = IDNINT ( real_buffer ( iib )) lcl_complex ( 1 )% centers ( ptr ,:) = real_buffer ( iib + 1 : fib ) !-- find correct location in data structure -- ptrp = ptr ; DO IF ( junk + indx_offset ( 1 ) == lcl_complex ( 1 )% node_indx ( ptrp , 1 )) EXIT ptrp = ptrp + 1 END DO !-- shuffle data and zero locations from neighbouring processes -- IF ( ptrp > ptr ) THEN lcl_complex ( 1 )% centers ( ptrp ,:) = lcl_complex ( 1 )% centers ( ptr ,:) lcl_complex ( 1 )% centers ( ptr : ptrp - 1 ,:) = 0.d0 END IF !-- update indicies -- iib = iib + dim_embbd + 1 ; fib = fib + dim_embbd + 1 ; ptr = ptrp + 1 END DO !-- zero remaining locations from neighbouring processes -- lcl_complex ( 1 )% centers ( ptr : num_elm ( 1 ),:) = 0.d0 END IF !-- wait for MPI communication to finish, close file and clean up -- IF ( rank == root ) THEN CLOSE ( mesh_unit , IOSTAT = ier ) IF ( ier /= 0 ) THEN WRITE ( log_unit , '(A,A,A,I5)' ) '***Error: failed to close ' , trim ( fname ),& ': errcode = ' , ier !err = .TRUE. END IF DEALLOCATE ( real_buffer ) ELSE DEALLOCATE ( real_buffer , status ) END IF !-- make sure all comms have completed -- !-- all comms are blocking: no need for MPI_BARRIER -- RETURN END SUBROUTINE ! io_mod|read_nodes_prml !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s*  io_mod/read_glb_indx2 !* SYNOPSIS SUBROUTINE read_glb_indx2 ( k ) !* PURPOSE !*   Read global indices of elements of given geometric order !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Read local node indices of highest order primal elements !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- INTEGER , INTENT ( IN ) :: k !> simplicial order !-- local variables -- CHARACTER ( LEN = slen ) :: fname !> file name INTEGER :: junk !> junk IO variable INTEGER :: ie , in , i , j !> loop indices (procs,elems) INTEGER :: buffer_size !> size of MPI comm buffer INTEGER , ALLOCATABLE :: int_buffer (:) !> integer buffer for MPI comms INTEGER , ALLOCATABLE :: req (:) !> request variable for non-blocking comms INTEGER , ALLOCATABLE :: status (:) !> size of MPI comm buffer INTEGER , ALLOCATABLE :: prml_elm (:,:) !> primal element list INTEGER :: proc , proc_old !> processor rank (current, past) INTEGER :: ptr !> pointer INTEGER :: n , l , orient , iib , fib , num_read_iters , & max_read_size , resid_read_size , read_size , stride , strt_indx !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- build file name -- SELECT CASE ( k ) CASE ( 1 ) fname = trim ( mesh_prefix ) // \".node\" CASE ( 2 ) fname = trim ( mesh_prefix ) // \".edge\" CASE ( 3 ) fname = trim ( mesh_prefix ) // \".face\" CASE DEFAULT WRITE ( log_unit , '(A,A,A,I5)' ) '***Error: invalid geometric order (0<k<dim_cmplx+1): ' , k CALL end_mpi () END SELECT !-- open mesh file and check for errors -- CALL root_open_file_read ( fname , mesh_unit ) !-- read header and broadcast global number of entities -- IF ( rank == root ) READ ( mesh_unit , * ) glb_num_elm ( k ) CALL MPI_BCAST ( glb_num_elm ( k ), 1 , MPI_INTEGER , 0 , MPI_COMM_WORLD , ier ) !-- get some basic information for reuse -- max_read_size = 1000 ; read_size = max_read_size stride = k + 1 ; buffer_size = read_size * stride num_read_iters = glb_num_elm ( k ) / read_size resid_read_size = mod ( glb_num_elm ( k ), read_size ) fib = k ALLOCATE ( int_buffer ( buffer_size )) IF ( rank == root ) THEN !-- on root process -- !-- first element to get index offset READ ( mesh_unit , * ) junk indx_offset ( k ) = 1 - junk BACKSPACE ( mesh_unit ) !-- DO i = 0 , num_read_iters IF ( i == num_read_iters ) THEN read_size = resid_read_size buffer_size = read_size * stride END IF !-- iib = 1 DO j = 1 , read_size READ ( mesh_unit , * ) int_buffer ( iib : iib + fib ) !-- int_buffer ( iib ) = i * max_read_size + j ; int_buffer ( iib + 1 : iib + fib ) = & int_buffer ( iib + 1 : iib + fib ) + indx_offset ( 1 ) iib = iib + stride END DO !-- CALL MPI_BCAST ( int_buffer , buffer_size , MPI_INTEGER , 0 , MPI_COMM_WORLD , ier ) !-- iib = 1 DO j = 1 , read_size CALL int_insertion_sort ( int_buffer ( iib + 1 : iib + fib )) IF ( int_buffer ( iib + 1 ) <= num_elm ( 1 )) THEN DO l = 1 , num_elm ( k ) IF ( ALL ( int_buffer ( iib + 1 : iib + fib ) == lcl_complex ( k )% node_indx ( l ,:))) THEN lcl_complex ( k )% glb_indx ( l ) = int_buffer ( iib ) EXIT END IF END DO END IF iib = iib + stride END DO END DO !-- close file -- CLOSE ( mesh_unit , IOSTAT = ier ) IF ( ier /= 0 ) THEN WRITE ( log_unit , '(A,A,A,I5)' ) '***Error: failed to close ' , trim ( fname ),& ': errcode = ' , ier !err = .TRUE. END IF ELSE !-- on neighbouring processes -- !-- DO i = 0 , num_read_iters IF ( i == num_read_iters ) THEN read_size = resid_read_size buffer_size = read_size * stride END IF !-- CALL MPI_BCAST ( int_buffer , buffer_size , MPI_INTEGER , 0 , MPI_COMM_WORLD , ier ) !-- iib = 1 DO j = 1 , read_size CALL int_insertion_sort ( int_buffer ( iib + 1 : iib + fib )) DO n = 1 , k IF ( elm2proc ( int_buffer ( iib + n )) == rank ) THEN DO l = 1 , num_elm ( k ) IF ( ALL ( int_buffer ( iib + 1 : iib + fib ) == lcl_complex ( k )% node_indx ( l ,:))) THEN lcl_complex ( k )% glb_indx ( l ) = int_buffer ( iib ) EXIT END IF END DO END IF END DO iib = iib + stride END DO END DO END IF !-- clean up -- DEALLOCATE ( int_buffer ) !-- make sure all comms have completed -- CALL MPI_BARRIER ( MPI_COMM_WORLD , ier ) !-- broadcast index offset variable -- CALL MPI_BCAST ( indx_offset ( k ), 1 , MPI_INTEGER , 0 , MPI_COMM_WORLD , ier ) RETURN END SUBROUTINE ! io_mod|read_glb_indx2 !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s*  io_mod/root_open_file_read !* SYNOPSIS SUBROUTINE root_open_file_read ( fname , unit ) !* PURPOSE !*   Open given file on root process for reading !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Open given file on root process for reading !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- CHARACTER ( LEN =* ), INTENT ( IN ) :: fname !> file name INTEGER , INTENT ( IN ) :: unit !> file unit LOGICAL :: err = . FALSE . !> error variable !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- on root process: -- IF ( rank == root ) THEN !-- Open mesh file and check for errors -- OPEN ( unit , FILE = fname , STATUS = 'OLD' , ACTION = 'READ' , IOSTAT = ier ) IF ( ier /= 0 ) THEN WRITE ( log_unit , '(A,A,A,I5)' ) 'Error opening ' , trim ( fname ), ': errcode = ' , ier err = . TRUE . END IF END IF !-- check for errors CALL MPI_BCAST ( err , 1 , MPI_LOGICAL , 0 , MPI_COMM_WORLD , ier ) IF ( err ) CALL end_mpi () RETURN END SUBROUTINE ! io_mod|root_open_file_read !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s*  io_mod/root_open_file_write !* SYNOPSIS SUBROUTINE root_open_file_write ( fname , unit ) !* PURPOSE !*   Open given file on root process for writing !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Open given file on root process for writing !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- CHARACTER ( LEN =* ), INTENT ( IN ) :: fname !> file name INTEGER , INTENT ( IN ) :: unit !> file unit LOGICAL :: err = . FALSE . !> error variable !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- on root process: -- IF ( rank == root ) THEN !-- Open mesh file and check for errors -- OPEN ( unit , FILE = fname , STATUS = 'UNKNOWN' , ACTION = 'WRITE' , IOSTAT = ier ) IF ( ier /= 0 ) THEN WRITE ( log_unit , '(A,A,A,I5)' ) 'Error opening ' , trim ( fname ), ': errcode = ' , ier err = . TRUE . END IF END IF !-- check for errors CALL MPI_BCAST ( err , 1 , MPI_LOGICAL , 0 , MPI_COMM_WORLD , ier ) IF ( err ) CALL end_mpi () RETURN END SUBROUTINE ! io_mod|root_open_file_write !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s*  io_mod/write_solution_D0S !* SYNOPSIS SUBROUTINE write_solution_D0S ( sclr_name , vctr_name ) !* PURPOSE !*   write vtk solution at dual 0-cells !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> write vtk solution at dual 0-cells !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- CHARACTER ( LEN =* ), INTENT ( IN ) :: sclr_name !> scalar variable name (label) CHARACTER ( LEN =* ), INTENT ( IN ) :: vctr_name !> vector variable name (label) !-- local variables -- CHARACTER ( LEN = slen ) :: fname !> file name INTEGER :: kmax !> INTEGER :: i , j !> INTEGER :: iib , fib !> INTEGER :: ptr !> INTEGER :: extra_comms !> INTEGER , ALLOCATABLE :: int_buffer (:) !> INTEGER , ALLOCATABLE :: junk_int_buffer (:) !> REAL ( KIND = iwp ), ALLOCATABLE :: real_buffer (:) !> REAL ( KIND = iwp ), ALLOCATABLE :: junk_real_buffer (:) !> INTEGER :: buffer_size !> INTEGER :: junk !> INTEGER , ALLOCATABLE :: req (:) !> request variable for non-blocking comms INTEGER , ALLOCATABLE :: status (:) !> size of MPI comm buffer INTEGER , ALLOCATABLE :: istatus (:,:) !> size of MPI comm buffer !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- set commonly used variable -- kmax = dim_cmplx + 1 !-- open solution file -- fname = trim ( sol_prefix ) // '.vtk' ; CALL root_open_file_write ( fname , sol_unit ) !-- gather total number of primal volumes accross all processes -- CALL MPI_REDUCE ( num_elm ( kmax ), extra_comms , 1 , MPI_INTEGER , MPI_SUM , 0 , MPI_COMM_WORLD , ier ) !-- Open mesh file and check for errors -- fname = trim ( mesh_prefix ) // \".ele\" ; CALL root_open_file_read ( fname , mesh_unit ) IF ( rank == root ) THEN !-- on root process -- !-- compute extra comms that will be received -- extra_comms = extra_comms - glb_num_elm ( kmax ) !-- write header -- write ( sol_unit , '(A)' ) '# vtk DataFile Version 3.0' write ( sol_unit , '(A)' ) 'Unstructured Grid' write ( sol_unit , '(A)' ) 'ASCII' write ( sol_unit , '(A)' ) 'DATASET UNSTRUCTURED_GRID' !------------------------------------------------------------------------- !-- write primal vertex locations -- !-- write header -- write ( sol_unit , '(A,I10,A)' ) 'POINTS' , glb_num_elm ( 1 ), ' double' !-- write local data -- DO i = 1 , num_pelm_pp ( 1 ) write ( sol_unit , * ) lcl_complex ( 1 )% centers ( i ,:) END DO !-- collect data from adjacent processes and write to file -- !-- allocate MPI variables -- ALLOCATE ( real_buffer ( 3 * num_pelm_pp ( 1 )), status ( MPI_STATUS_SIZE )) DO i = 1 , num_procs - 1 !-- recieve data from adjacent process -- buffer_size = dim_embbd * num_pelm_pp ( i + 1 ) CALL MPI_RECV ( real_buffer , buffer_size , MPI_DOUBLE_PRECISION , i , 0 ,& MPI_COMM_WORLD , status , ier ) !-- unpack buffer and write to file -- iib = 1 ; fib = dim_embbd DO j = 1 , num_pelm_pp ( i + 1 ) write ( sol_unit , * ) real_buffer ( iib : fib ) iib = iib + dim_embbd ; fib = fib + dim_embbd END DO END DO !-- clean up -- DEALLOCATE ( real_buffer ) !------------------------------------------------------------------------- !-- write primal volume indices -- !-- write header -- write ( sol_unit , '(A,I10,I10)' ) 'CELLS' , glb_num_elm ( kmax ), ( kmax + 1 ) * glb_num_elm ( kmax ) !-- use input mesh file for node order -- !-- read header -- READ ( mesh_unit , * ) junk !-- process node indices -- ALLOCATE ( int_buffer ( kmax )) DO i = 1 , glb_num_elm ( kmax ) READ ( mesh_unit , * ) junk , int_buffer write ( sol_unit , * ) kmax , int_buffer + indx_offset ( 1 ) - 1 END DO !------------------------------------------------------------------------- !-- write primal volumes types -- !-- write header -- write ( sol_unit , '(A,I10)' ) 'CELL_TYPES' , glb_num_elm ( kmax ) !-- write data -- SELECT CASE ( dim_cmplx ) CASE ( 1 ) DO i = 1 , glb_num_elm ( 2 ); write ( sol_unit , '(I1)' ) 3 ; END DO CASE ( 2 ) DO i = 1 , glb_num_elm ( 3 ); write ( sol_unit , '(I1)' ) 5 ; END DO CASE ( 3 ) DO i = 1 , glb_num_elm ( 4 ); write ( sol_unit , '(I2)' ) 10 ; END DO CASE DEFAULT END SELECT !------------------------------------------------------------------------- !-- write primal volumes scalar data -- !-- write header -- write ( sol_unit , '(A,I10)' ) 'CELL_DATA' , glb_num_elm ( kmax ) write ( sol_unit , '(A,A,A)' ) 'SCALARS ' , sclr_name , ' double 1' write ( sol_unit , '(A)' ) 'LOOKUP_TABLE default' !-- set local data pointer and allocate MPI variables -- ptr = 1 ALLOCATE ( real_buffer ( 1 ), junk_real_buffer ( 1 )) !-- wait for previous communication to end and deallocate old MPI variables -- ! CALL MPI_WAITALL(extra_comms,req,istatus,ier) CALL MPI_BARRIER ( MPI_COMM_WORLD , ier ) ! DEALLOCATE(int_buffer,junk_int_buffer) !-- deallocate old and allocate new MPI variables -- DEALLOCATE ( int_buffer ) ALLOCATE ( req ( extra_comms ), istatus ( MPI_STATUS_SIZE , extra_comms )) !-- loop through all primal volumes and write data to file -- DO i = 1 , glb_num_elm ( kmax ) IF ( lcl_complex ( kmax )% glb_indx ( ptr ) == i ) THEN !-- data is local -- !-- write to file -- write ( sol_unit , * ) lcl_complex ( kmax )% dual_sol ( ptr , 1 ) !-- update local data pointer -- ptr = min ( ptr + 1 , num_elm ( kmax )) ELSE !-- data is non-local -- !-- recieve first communication to arrive for given volume -- CALL MPI_RECV ( real_buffer , 1 , MPI_DOUBLE_PRECISION , MPI_ANY_SOURCE , i ,& MPI_COMM_WORLD , status , ier ) !-- write to file -- write ( sol_unit , * ) real_buffer END IF END DO !-- recieve extras unneeded data to be discarded -- DO i = 1 , extra_comms CALL MPI_IRECV ( junk_real_buffer , 1 , MPI_DOUBLE_PRECISION , MPI_ANY_SOURCE ,& MPI_ANY_TAG , MPI_COMM_WORLD , req ( i ), ier ) END DO !------------------------------------------------------------------------- !-- write cell center vector data -- !-- write header -- write ( sol_unit , '(A,A,A)' ) 'VECTORS ' , vctr_name , ' double' !-- set local data pointer and allocate MPI variables -- ptr = 1 DEALLOCATE ( real_buffer , junk_real_buffer ) ALLOCATE ( real_buffer ( dim_embbd ), junk_real_buffer ( dim_embbd )) !-- wait for previous communication to end and deallocate old MPI variables -- CALL MPI_WAITALL ( extra_comms , req , istatus , ier ) CALL MPI_BARRIER ( MPI_COMM_WORLD , ier ) !-- loop through all primal volumes and write data to file -- DO i = 1 , glb_num_elm ( kmax ) IF ( lcl_complex ( kmax )% glb_indx ( ptr ) == i ) THEN !-- data is local -- !-- write to file -- write ( sol_unit , * ) lcl_complex ( kmax )% whtny_sol ( ptr ,:) !-- update local data pointer -- ptr = min ( ptr + 1 , num_elm ( kmax )) ELSE !-- data is non-local -- !-- recieve first communication to arrive for given volume -- CALL MPI_RECV ( real_buffer , dim_embbd , MPI_DOUBLE_PRECISION , MPI_ANY_SOURCE , i ,& MPI_COMM_WORLD , status , ier ) !-- write to file -- write ( sol_unit , * ) real_buffer END IF END DO !-- recieve extras unneeded data to be discarded -- DO i = 1 , extra_comms CALL MPI_IRECV ( junk_real_buffer , dim_embbd , MPI_DOUBLE_PRECISION , MPI_ANY_SOURCE ,& MPI_ANY_TAG , MPI_COMM_WORLD , req ( i ), ier ) END DO !------------------------------------------------------------------------- !-- wait for previous communication to end and deallocate variables -- CALL MPI_WAITALL ( extra_comms , req , istatus , ier ) CALL MPI_BARRIER ( MPI_COMM_WORLD , ier ) DEALLOCATE ( real_buffer , junk_real_buffer , req , status , istatus ) !-- close files -- CLOSE ( sol_unit , IOSTAT = ier ) IF ( ier /= 0 ) THEN WRITE ( log_unit , '(A,A,A,I5)' ) '***Error: failed to close ' , trim ( fname ),& ': errcode = ' , ier !err = .TRUE. END IF CLOSE ( mesh_unit , IOSTAT = ier ) IF ( ier /= 0 ) THEN WRITE ( log_unit , '(A,A,A,I5)' ) '***Error: failed to close ' , trim ( fname ),& ': errcode = ' , ier !err = .TRUE. END IF ELSE !------------------------------------------------------------------------- !-- find starting index of local primary vertices -- DO i = 1 , num_elm ( 1 ) IF ( lcl_complex ( 1 )% glb_indx ( i ) == glb_offset + 1 ) THEN ptr = i ; EXIT END IF END DO buffer_size = dim_embbd * num_pelm_pp ( rank + 1 ); ALLOCATE ( real_buffer ( buffer_size )) iib = 1 ; fib = dim_embbd DO j = 1 , num_pelm_pp ( rank + 1 ) real_buffer ( iib : fib ) = lcl_complex ( 1 )% centers ( ptr + j - 1 ,:) iib = iib + dim_embbd ; fib = fib + dim_embbd END DO !------------------------------------------------------------------------- !-- send primal vertex locations -- CALL MPI_ISEND ( real_buffer , buffer_size , MPI_DOUBLE_PRECISION , 0 , 0 ,& MPI_COMM_WORLD , junk , ier ) CALL MPI_REQUEST_FREE ( junk , ier ) !------------------------------------------------------------------------- !-- send primal volume indices -- ! DO i = 1,num_elm(kmax) !   CALL MPI_ISEND(lcl_complex(kmax)%node_indx(i,1:kmax),kmax,MPI_INTEGER,0,& !     lcl_complex(kmax)%glb_indx(i),MPI_COMM_WORLD,junk,ier) !   CALL MPI_REQUEST_FREE(junk,ier) ! END DO !-- wait for previous communications to finish (free tags) -- CALL MPI_BARRIER ( MPI_COMM_WORLD , ier ); DEALLOCATE ( real_buffer ) !------------------------------------------------------------------------- !-- send primal volume values -- DO i = 1 , num_elm ( kmax ) CALL MPI_ISEND ( lcl_complex ( kmax )% dual_sol ( i , 1 ), 1 , MPI_DOUBLE_PRECISION , 0 ,& lcl_complex ( kmax )% glb_indx ( i ), MPI_COMM_WORLD , junk , ier ) CALL MPI_REQUEST_FREE ( junk , ier ) END DO !-- wait for previous communications to finish -- CALL MPI_BARRIER ( MPI_COMM_WORLD , ier ) !------------------------------------------------------------------------- !-- send primal volume values -- DO i = 1 , num_elm ( kmax ) CALL MPI_ISEND ( lcl_complex ( kmax )% whtny_sol ( i ,:), dim_embbd , MPI_DOUBLE_PRECISION , 0 ,& lcl_complex ( kmax )% glb_indx ( i ), MPI_COMM_WORLD , junk , ier ) CALL MPI_REQUEST_FREE ( junk , ier ) END DO !-- wait for previous communications to finish -- CALL MPI_BARRIER ( MPI_COMM_WORLD , ier ) END IF RETURN END SUBROUTINE ! io_mod|write_solution_D0S !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s*  io_mod/write_solution_D0S2 !* SYNOPSIS SUBROUTINE write_solution_D0S2 ( sclr_name , vctr_name ) !* PURPOSE !*   write vtk solution at dual 0-cells !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> write vtk solution at dual 0-cells !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- CHARACTER ( LEN =* ), INTENT ( IN ) :: sclr_name !> scalar variable name (label) CHARACTER ( LEN =* ), INTENT ( IN ) :: vctr_name !> vector variable name (label) !-- local variables -- CHARACTER ( LEN = slen ) :: fname !> file name INTEGER :: kmax !> INTEGER :: i , j , k !> INTEGER :: iib , fib !> INTEGER :: ptr !> INTEGER :: extra_comms !> INTEGER , ALLOCATABLE :: int_buffer (:) !> INTEGER , ALLOCATABLE :: junk_int_buffer (:) !> REAL ( KIND = iwp ), ALLOCATABLE :: real_buffer (:) !> REAL ( KIND = iwp ), ALLOCATABLE :: junk_real_buffer (:) !> INTEGER :: buffer_size !> INTEGER :: junk !> INTEGER , ALLOCATABLE :: req (:) !> request variable for non-blocking comms INTEGER , ALLOCATABLE :: status (:) !> size of MPI comm buffer INTEGER , ALLOCATABLE :: istatus (:,:) !> size of MPI comm buffer REAL ( KIND = IWP ) :: tmp_time , tmp_time_int !> temporary timing variable INTEGER :: comm_size , max_comm_size , gindx , num_comm_iters , & strt_indx , resid_comm_size !> !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- set commonly used variable -- kmax = dim_cmplx + 1 !-- open solution file -- fname = trim ( sol_prefix ) // '.vtk' ; CALL root_open_file_write ( fname , sol_unit ) !-- gather total number of primal volumes accross all processes -- CALL MPI_REDUCE ( num_elm ( kmax ), extra_comms , 1 , MPI_INTEGER , MPI_SUM , 0 , MPI_COMM_WORLD , ier ) !-- get some basic information for reuse -- max_comm_size = 10000 IF ( rank == root ) THEN !-- on root process -- !------------------------------------------------------------------------- !-- write header -- write ( sol_unit , '(A)' ) '# vtk DataFile Version 3.0' write ( sol_unit , '(A)' ) 'Unstructured Grid' write ( sol_unit , '(A)' ) 'ASCII' write ( sol_unit , '(A)' ) 'DATASET UNSTRUCTURED_GRID' CALL syncwrite_log ( '>---- header' ) !------------------------------------------------------------------------- !-- write primal vertex locations -- !-- write header -- write ( sol_unit , '(A,I10,A)' ) 'POINTS' , glb_num_elm ( 1 ), ' double' !-- write local data -- DO i = 1 , num_pelm_pp ( 1 ) write ( sol_unit , * ) lcl_complex ( 1 )% centers ( i ,:) END DO !-- collect data from adjacent processes and write to file -- !-- allocate MPI variables -- ALLOCATE ( status ( MPI_STATUS_SIZE )) DO k = 1 , num_procs - 1 num_comm_iters = num_pelm_pp ( k + 1 ) / max_comm_size resid_comm_size = mod ( num_pelm_pp ( k + 1 ), max_comm_size ) comm_size = max_comm_size buffer_size = comm_size * dim_embbd ALLOCATE ( real_buffer ( buffer_size )) DO i = 0 , num_comm_iters IF ( i == num_comm_iters ) THEN comm_size = resid_comm_size buffer_size = comm_size * dim_embbd END IF !-- recieve data from adjacent process -- CALL MPI_RECV ( real_buffer , buffer_size , MPI_DOUBLE_PRECISION , k , i ,& MPI_COMM_WORLD , status , ier ) !-- unpack buffer and write to file -- iib = 1 ; fib = dim_embbd DO j = 1 , comm_size write ( sol_unit , * ) real_buffer ( iib : fib ) iib = iib + dim_embbd ; fib = fib + dim_embbd END DO END DO DEALLOCATE ( real_buffer ) END DO !-- clean up -- DEALLOCATE ( status ) CALL syncwrite_log ( '>---- vertex locations' ) !------------------------------------------------------------------------- !-- write primal volume indices -- !-- write header -- write ( sol_unit , '(A,I10,I10)' ) 'CELLS' , glb_num_elm ( kmax ), ( kmax + 1 ) * glb_num_elm ( kmax ) !-- num_comm_iters = glb_num_elm ( kmax ) / max_comm_size resid_comm_size = mod ( glb_num_elm ( kmax ), max_comm_size ) comm_size = max_comm_size buffer_size = comm_size * kmax ALLOCATE ( int_buffer ( buffer_size ), junk_int_buffer ( buffer_size )) strt_indx = 1 DO i = 0 , num_comm_iters IF ( i == num_comm_iters ) THEN comm_size = resid_comm_size buffer_size = comm_size * kmax END IF int_buffer = 0 DO j = strt_indx , num_elm ( kmax ) gindx = lcl_complex ( kmax )% glb_indx ( j ) - i * max_comm_size IF ( gindx > comm_size ) THEN strt_indx = j EXIT END IF iib = ( gindx - 1 ) * kmax + 1 ; fib = iib + kmax - 1 IF ( lcl_complex ( kmax )% orientation ( j ) == 1 ) THEN int_buffer ( iib : fib ) = lcl_complex ( kmax )% node_indx ( j , 1 : kmax ) - 1 ELSE int_buffer ( iib ) = lcl_complex ( kmax )% node_indx ( j , 2 ) - 1 int_buffer ( iib + 1 ) = lcl_complex ( kmax )% node_indx ( j , 1 ) - 1 int_buffer ( iib + 2 : fib ) = lcl_complex ( kmax )% node_indx ( j , 3 : kmax ) - 1 END IF IF ( j == num_elm ( kmax )) strt_indx = num_elm ( kmax ) + 1 END DO CALL MPI_REDUCE ( int_buffer , junk_int_buffer , buffer_size , MPI_INTEGER , MPI_MAX , 0 , MPI_COMM_WORLD , ier ) iib = 1 ; fib = kmax - 1 DO j = 1 , comm_size WRITE ( sol_unit , * ) kmax , junk_int_buffer ( iib : iib + fib ) iib = iib + kmax END DO END DO !-- clean up -- DEALLOCATE ( int_buffer , junk_int_buffer ) CALL syncwrite_log ( '>---- volume indices' ) !------------------------------------------------------------------------- !-- write primal volumes types -- !-- write header -- write ( sol_unit , '(A,I10)' ) 'CELL_TYPES' , glb_num_elm ( kmax ) !-- write data -- SELECT CASE ( dim_cmplx ) CASE ( 1 ) DO i = 1 , glb_num_elm ( kmax ); write ( sol_unit , '(I1)' ) 3 ; END DO CASE ( 2 ) DO i = 1 , glb_num_elm ( kmax ); write ( sol_unit , '(I1)' ) 5 ; END DO CASE ( 3 ) DO i = 1 , glb_num_elm ( kmax ); write ( sol_unit , '(I2)' ) 10 ; END DO CASE DEFAULT END SELECT CALL syncwrite_log ( '>---- types' ) !------------------------------------------------------------------------- !-- write primal volumes scalar data -- !-- write header -- write ( sol_unit , '(A,I10)' ) 'CELL_DATA' , glb_num_elm ( kmax ) write ( sol_unit , '(A,A,A)' ) 'SCALARS ' , sclr_name , ' double 1' write ( sol_unit , '(A)' ) 'LOOKUP_TABLE default' !-- comm_size = max_comm_size buffer_size = comm_size ALLOCATE ( real_buffer ( buffer_size ), junk_real_buffer ( buffer_size )) strt_indx = 1 DO i = 0 , num_comm_iters IF ( i == num_comm_iters ) THEN comm_size = resid_comm_size buffer_size = comm_size END IF real_buffer =- large DO j = strt_indx , num_elm ( kmax ) gindx = lcl_complex ( kmax )% glb_indx ( j ) - i * max_comm_size IF ( gindx > comm_size ) THEN strt_indx = j EXIT END IF real_buffer ( gindx ) = lcl_complex ( kmax )% dual_sol ( j , 1 ) IF ( j == num_elm ( kmax )) strt_indx = num_elm ( kmax ) + 1 END DO CALL MPI_REDUCE ( real_buffer , junk_real_buffer , buffer_size , MPI_DOUBLE_PRECISION , MPI_MAX , 0 , MPI_COMM_WORLD , ier ) DO j = 1 , comm_size WRITE ( sol_unit , * ) junk_real_buffer ( j ) END DO END DO !-- clean up -- DEALLOCATE ( real_buffer , junk_real_buffer ) CALL syncwrite_log ( '>---- scalar' ) !------------------------------------------------------------------------- !-- write cell center vector data -- !-- write header -- write ( sol_unit , '(A,A,A)' ) 'VECTORS ' , vctr_name , ' double' !-- comm_size = max_comm_size buffer_size = comm_size * dim_embbd ALLOCATE ( real_buffer ( buffer_size ), junk_real_buffer ( buffer_size )) strt_indx = 1 DO i = 0 , num_comm_iters IF ( i == num_comm_iters ) THEN comm_size = resid_comm_size buffer_size = comm_size * dim_embbd END IF real_buffer =- large DO j = strt_indx , num_elm ( kmax ) gindx = lcl_complex ( kmax )% glb_indx ( j ) - i * max_comm_size IF ( gindx > comm_size ) THEN strt_indx = j EXIT END IF iib = ( gindx - 1 ) * dim_embbd + 1 ; fib = iib + dim_embbd - 1 real_buffer ( iib : fib ) = lcl_complex ( kmax )% whtny_sol ( j , 1 : dim_embbd ) IF ( j == num_elm ( kmax )) strt_indx = num_elm ( kmax ) + 1 END DO CALL MPI_REDUCE ( real_buffer , junk_real_buffer , buffer_size , MPI_DOUBLE_PRECISION , MPI_MAX , 0 , MPI_COMM_WORLD , ier ) iib = 1 ; fib = dim_embbd - 1 DO j = 1 , comm_size WRITE ( sol_unit , * ) junk_real_buffer ( iib : iib + fib ) iib = iib + dim_embbd END DO END DO !-- clean up -- DEALLOCATE ( real_buffer , junk_real_buffer ) CALL syncwrite_log ( '>---- vector' ) !------------------------------------------------------------------------- !-- close files -- CLOSE ( sol_unit , IOSTAT = ier ) IF ( ier /= 0 ) THEN WRITE ( log_unit , '(A,A,A,I5)' ) '***Error: failed to close ' , trim ( fname ),& ': errcode = ' , ier !err = .TRUE. END IF ELSE !-- adjacent processes -- !------------------------------------------------------------------------- !-- send header -- CALL syncwrite_log ( '>---- header' ) !------------------------------------------------------------------------- !-- send primal vertex locations -- !-- find starting index of local primary vertices -- DO i = 1 , num_elm ( 1 ) IF ( lcl_complex ( 1 )% glb_indx ( i ) == glb_offset + 1 ) THEN ptr = i - 1 ; EXIT END IF END DO !-- set up and pack buffer -- num_comm_iters = num_pelm_pp ( rank + 1 ) / max_comm_size resid_comm_size = mod ( num_pelm_pp ( rank + 1 ), max_comm_size ) comm_size = max_comm_size buffer_size = comm_size * dim_embbd ALLOCATE ( real_buffer ( buffer_size )) DO i = 0 , num_comm_iters IF ( i == num_comm_iters ) THEN comm_size = resid_comm_size buffer_size = comm_size * dim_embbd END IF iib = 1 ; fib = dim_embbd - 1 DO j = 1 , comm_size real_buffer ( iib : iib + fib ) = lcl_complex ( 1 )% centers ( ptr + j ,:) iib = iib + dim_embbd END DO !-- send primal vertex locations -- CALL MPI_SEND ( real_buffer , buffer_size , MPI_DOUBLE_PRECISION , 0 , i , MPI_COMM_WORLD , ier ) END DO DEALLOCATE ( real_buffer ) CALL syncwrite_log ( '>---- vertex locations' ) !------------------------------------------------------------------------- !-- send primal volume indices -- !-- num_comm_iters = glb_num_elm ( kmax ) / max_comm_size resid_comm_size = mod ( glb_num_elm ( kmax ), max_comm_size ) comm_size = max_comm_size buffer_size = comm_size * kmax ALLOCATE ( int_buffer ( buffer_size ), junk_int_buffer ( buffer_size )) strt_indx = 1 DO i = 0 , num_comm_iters IF ( i == num_comm_iters ) THEN comm_size = resid_comm_size buffer_size = comm_size * kmax END IF int_buffer = 0 DO j = strt_indx , num_elm ( kmax ) gindx = lcl_complex ( kmax )% glb_indx ( j ) - i * max_comm_size IF ( gindx > comm_size ) THEN strt_indx = j EXIT END IF iib = ( gindx - 1 ) * kmax + 1 ; fib = iib + kmax - 1 IF ( lcl_complex ( kmax )% orientation ( j ) == 1 ) THEN int_buffer ( iib : fib ) = lcl_complex ( kmax )% node_indx ( j , 1 : kmax ) - 1 ELSE int_buffer ( iib ) = lcl_complex ( kmax )% node_indx ( j , 2 ) - 1 int_buffer ( iib + 1 ) = lcl_complex ( kmax )% node_indx ( j , 1 ) - 1 int_buffer ( iib + 2 : fib ) = lcl_complex ( kmax )% node_indx ( j , 3 : kmax ) - 1 END IF IF ( j == num_elm ( kmax )) strt_indx = num_elm ( kmax ) + 1 END DO CALL MPI_REDUCE ( int_buffer , junk_int_buffer , buffer_size , MPI_INTEGER , MPI_MAX , 0 , MPI_COMM_WORLD , ier ) END DO !-- clean up -- DEALLOCATE ( int_buffer , junk_int_buffer ) CALL syncwrite_log ( '>---- volume indices' ) !------------------------------------------------------------------------- !-- send primal volumes types -- CALL syncwrite_log ( '>---- types' ) !------------------------------------------------------------------------- !-- send primal volume scalar values -- !-- comm_size = max_comm_size buffer_size = comm_size ALLOCATE ( real_buffer ( buffer_size ), junk_real_buffer ( buffer_size )) strt_indx = 1 DO i = 0 , num_comm_iters IF ( i == num_comm_iters ) THEN comm_size = resid_comm_size buffer_size = comm_size END IF real_buffer =- large DO j = strt_indx , num_elm ( kmax ) gindx = lcl_complex ( kmax )% glb_indx ( j ) - i * max_comm_size IF ( gindx > comm_size ) THEN strt_indx = j EXIT END IF real_buffer ( gindx ) = lcl_complex ( kmax )% dual_sol ( j , 1 ) IF ( j == num_elm ( kmax )) strt_indx = num_elm ( kmax ) + 1 END DO CALL MPI_REDUCE ( real_buffer , junk_real_buffer , buffer_size , MPI_DOUBLE_PRECISION , MPI_MAX , 0 , MPI_COMM_WORLD , ier ) END DO !-- clean up -- DEALLOCATE ( real_buffer , junk_real_buffer ) CALL syncwrite_log ( '>---- scalar' ) !------------------------------------------------------------------------- !-- send cell center vector data -- !-- comm_size = max_comm_size buffer_size = comm_size * dim_embbd ALLOCATE ( real_buffer ( buffer_size ), junk_real_buffer ( buffer_size )) strt_indx = 1 DO i = 0 , num_comm_iters IF ( i == num_comm_iters ) THEN comm_size = resid_comm_size buffer_size = comm_size * dim_embbd END IF real_buffer =- large DO j = strt_indx , num_elm ( kmax ) gindx = lcl_complex ( kmax )% glb_indx ( j ) - i * max_comm_size IF ( gindx > comm_size ) THEN strt_indx = j EXIT END IF iib = ( gindx - 1 ) * dim_embbd + 1 ; fib = iib + dim_embbd - 1 real_buffer ( iib : fib ) = lcl_complex ( kmax )% whtny_sol ( j , 1 : dim_embbd ) IF ( j == num_elm ( kmax )) strt_indx = num_elm ( kmax ) + 1 END DO CALL MPI_REDUCE ( real_buffer , junk_real_buffer , buffer_size , MPI_DOUBLE_PRECISION , MPI_MAX , 0 , MPI_COMM_WORLD , ier ) END DO !-- clean up -- DEALLOCATE ( real_buffer , junk_real_buffer ) CALL syncwrite_log ( '>---- vector' ) END IF !-- wait for previous communications to finish -- CALL MPI_BARRIER ( MPI_COMM_WORLD , ier ) CALL syncwrite_log_time () curnt_time = tmp_time RETURN END SUBROUTINE ! io_mod|write_solution_D0S2 !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s*  io_mod/write_solution_D0S !* SYNOPSIS SUBROUTINE write_unsteady_D0S ( sclr_name , vctr_name , iter ) !* PURPOSE !*   write vtk solution at dual 0-cells for evolving simulations !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> write vtk solution at dual 0-cells for evolving simulations !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- CHARACTER ( LEN =* ), INTENT ( IN ) :: sclr_name !> scalar variable name (label) CHARACTER ( LEN =* ), INTENT ( IN ) :: vctr_name !> vector variable name (label) INTEGER , INTENT ( IN ) :: iter !> integer iteration number !-- local variables -- CHARACTER ( LEN = slen ) :: fname !> file name INTEGER :: prelen !> INTEGER :: kmax !> INTEGER :: i , j !> INTEGER :: iib , fib !> INTEGER :: ptr !> INTEGER :: extra_comms !> INTEGER , ALLOCATABLE :: int_buffer (:) !> INTEGER , ALLOCATABLE :: junk_int_buffer (:) !> REAL ( KIND = iwp ), ALLOCATABLE :: real_buffer (:) !> REAL ( KIND = iwp ), ALLOCATABLE :: junk_real_buffer (:) !> INTEGER :: buffer_size !> INTEGER :: junk !> INTEGER , ALLOCATABLE :: req (:) !> request variable for non-blocking comms INTEGER , ALLOCATABLE :: status (:) !> size of MPI comm buffer INTEGER , ALLOCATABLE :: istatus (:,:) !> size of MPI comm buffer REAL ( KIND = IWP ) :: tmp_time , tmp_time_int !> temporary timing variable !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ CALL syncwrite_log ( '> write_unsteady_D0S()' ); tmp_time = curnt_time curnt_time = MPI_WTIME () !-- set commonly used variable -- kmax = dim_cmplx + 1 !-- open solution file -- prelen = len_trim ( unstdy_prefix ) fname = trim ( unstdy_prefix ) // \"_00000000.vtk\" WRITE ( fname ( prelen + 1 : prelen + 9 ), '(I9)' ) 100000000 + iter fname ( prelen + 1 : prelen + 1 ) = \"_\" CALL root_open_file_write ( fname , unstdy_unit ) !-- gather total number of primal volumes accross all processes -- CALL MPI_REDUCE ( num_elm ( kmax ), extra_comms , 1 , MPI_INTEGER , MPI_SUM , 0 , MPI_COMM_WORLD , ier ) !-- Open mesh file and check for errors -- fname = trim ( mesh_prefix ) // \".ele\" ; CALL root_open_file_read ( fname , mesh_unit ) IF ( rank == root ) THEN !-- on root process -- !-- compute extra comms that will be received -- extra_comms = extra_comms - glb_num_elm ( kmax ) !-- write header -- write ( unstdy_unit , '(A)' ) '# vtk DataFile Version 3.0' write ( unstdy_unit , '(A)' ) 'Unstructured Grid' write ( unstdy_unit , '(A)' ) 'ASCII' write ( unstdy_unit , '(A)' ) 'DATASET UNSTRUCTURED_GRID' !------------------------------------------------------------------------- !-- write primal vertex locations -- !-- write header -- write ( unstdy_unit , '(A,I10,A)' ) 'POINTS' , glb_num_elm ( 1 ), ' double' !-- write local data -- DO i = 1 , num_pelm_pp ( 1 ) write ( unstdy_unit , * ) lcl_complex ( 1 )% centers ( i ,:) END DO !-- collect data from adjacent processes and write to file -- !-- allocate MPI variables -- ALLOCATE ( real_buffer ( 3 * num_pelm_pp ( 1 )), status ( MPI_STATUS_SIZE )) DO i = 1 , num_procs - 1 !-- recieve data from adjacent process -- buffer_size = dim_embbd * num_pelm_pp ( i + 1 ) CALL MPI_RECV ( real_buffer , buffer_size , MPI_DOUBLE_PRECISION , i , 0 ,& MPI_COMM_WORLD , status , ier ) !-- unpack buffer and write to file -- iib = 1 ; fib = dim_embbd DO j = 1 , num_pelm_pp ( i + 1 ) write ( unstdy_unit , * ) real_buffer ( iib : fib ) iib = iib + dim_embbd ; fib = fib + dim_embbd END DO END DO !-- clean up -- DEALLOCATE ( real_buffer ) !------------------------------------------------------------------------- !-- write primal volume indices -- !-- write header -- write ( unstdy_unit , '(A,I10,I10)' ) 'CELLS' , glb_num_elm ( kmax ), ( kmax + 1 ) * glb_num_elm ( kmax ) !-- use input mesh file for node order -- !-- read header -- READ ( mesh_unit , * ) junk !-- process node indices -- ALLOCATE ( int_buffer ( kmax )) DO i = 1 , glb_num_elm ( kmax ) READ ( mesh_unit , * ) junk , int_buffer write ( unstdy_unit , * ) kmax , int_buffer + indx_offset ( 1 ) - 1 END DO ! !-- set local data pointer and allocate MPI variables -- ! ptr = 1 ! ALLOCATE(& !   req(extra_comms),istatus(MPI_STATUS_SIZE,extra_comms),& !   int_buffer(kmax),junk_int_buffer(kmax)) ! !-- wait for previous communication to end -- ! CALL MPI_BARRIER(MPI_COMM_WORLD,ier) ! !-- loop through all primal volumes and write data to file -- ! DO i = 1,glb_num_elm(kmax) !   IF (lcl_complex(kmax)%glb_indx(ptr)==i) THEN !-- data is local -- !     !-- write to file -- !     write(unstdy_unit,*) kmax, lcl_complex(kmax)% node_indx(ptr,:)-1 !     !-- update local data pointer -- !     ptr = min(ptr+1,num_elm(kmax)) !   ELSE !-- non-local data -- !     !-- receive first communication to arrive for given volume -- !     CALL MPI_RECV(int_buffer,kmax,MPI_INTEGER,MPI_ANY_SOURCE,i,& !       MPI_COMM_WORLD,status,ier) !     !-- write to file -- !     write(unstdy_unit,*) kmax, int_buffer-1 !   END IF ! END DO ! ! !-- recieve extras unneeded data to be discarded -- ! DO i = 1,extra_comms !   CALL MPI_IRECV(junk_int_buffer,kmax,MPI_INTEGER,MPI_ANY_SOURCE,MPI_ANY_TAG,& !     MPI_COMM_WORLD,req(i),ier) ! END DO !------------------------------------------------------------------------- !-- write primal volumes types -- !-- write header -- write ( unstdy_unit , '(A,I10)' ) 'CELL_TYPES' , glb_num_elm ( kmax ) !-- write data -- SELECT CASE ( dim_cmplx ) CASE ( 1 ) DO i = 1 , glb_num_elm ( 2 ); write ( unstdy_unit , '(I1)' ) 3 ; END DO CASE ( 2 ) DO i = 1 , glb_num_elm ( 3 ); write ( unstdy_unit , '(I1)' ) 5 ; END DO CASE ( 3 ) DO i = 1 , glb_num_elm ( 4 ); write ( unstdy_unit , '(I2)' ) 10 ; END DO CASE DEFAULT END SELECT !------------------------------------------------------------------------- !-- write primal volumes scalar data -- !-- write header -- write ( unstdy_unit , '(A,I10)' ) 'CELL_DATA' , glb_num_elm ( kmax ) write ( unstdy_unit , '(A,A,A)' ) 'SCALARS ' , sclr_name , ' double 1' write ( unstdy_unit , '(A)' ) 'LOOKUP_TABLE default' !-- set local data pointer and allocate MPI variables -- ptr = 1 ALLOCATE ( real_buffer ( 1 ), junk_real_buffer ( 1 )) !-- wait for previous communication to end and deallocate old MPI variables -- ! CALL MPI_WAITALL(extra_comms,req,istatus,ier) CALL MPI_BARRIER ( MPI_COMM_WORLD , ier ) ! DEALLOCATE(int_buffer,junk_int_buffer) !-- deallocate old and allocate new MPI variables -- DEALLOCATE ( int_buffer ) ALLOCATE ( req ( extra_comms ), istatus ( MPI_STATUS_SIZE , extra_comms )) !-- loop through all primal volumes and write data to file -- DO i = 1 , glb_num_elm ( kmax ) IF ( lcl_complex ( kmax )% glb_indx ( ptr ) == i ) THEN !-- data is local -- !-- write to file -- write ( unstdy_unit , * ) lcl_complex ( kmax )% dual_sol ( ptr , 1 ) !-- update local data pointer -- ptr = min ( ptr + 1 , num_elm ( kmax )) ELSE !-- data is non-local -- !-- recieve first communication to arrive for given volume -- CALL MPI_RECV ( real_buffer , 1 , MPI_DOUBLE_PRECISION , MPI_ANY_SOURCE , i ,& MPI_COMM_WORLD , status , ier ) !-- write to file -- write ( unstdy_unit , * ) real_buffer END IF END DO !-- recieve extras unneeded data to be discarded -- DO i = 1 , extra_comms CALL MPI_IRECV ( junk_real_buffer , 1 , MPI_DOUBLE_PRECISION , MPI_ANY_SOURCE ,& MPI_ANY_TAG , MPI_COMM_WORLD , req ( i ), ier ) END DO !------------------------------------------------------------------------- !-- write cell center vector data -- !-- write header -- write ( unstdy_unit , '(A,A,A)' ) 'VECTORS ' , vctr_name , ' double' !-- set local data pointer and allocate MPI variables -- ptr = 1 !-- wait for previous communication to end and deallocate old MPI variables -- CALL MPI_WAITALL ( extra_comms , req , istatus , ier ) CALL MPI_BARRIER ( MPI_COMM_WORLD , ier ) DEALLOCATE ( real_buffer , junk_real_buffer ) ALLOCATE ( real_buffer ( dim_embbd ), junk_real_buffer ( dim_embbd )) !-- loop through all primal volumes and write data to file -- DO i = 1 , glb_num_elm ( kmax ) IF ( lcl_complex ( kmax )% glb_indx ( ptr ) == i ) THEN !-- data is local -- !-- write to file -- write ( unstdy_unit , * ) lcl_complex ( kmax )% whtny_sol ( ptr ,:) !-- update local data pointer -- ptr = min ( ptr + 1 , num_elm ( kmax )) ELSE !-- data is non-local -- !-- recieve first communication to arrive for given volume -- CALL MPI_RECV ( real_buffer , dim_embbd , MPI_DOUBLE_PRECISION , MPI_ANY_SOURCE , i ,& MPI_COMM_WORLD , status , ier ) !-- write to file -- write ( unstdy_unit , * ) real_buffer END IF END DO !-- recieve extras unneeded data to be discarded -- DO i = 1 , extra_comms CALL MPI_IRECV ( junk_real_buffer , dim_embbd , MPI_DOUBLE_PRECISION , MPI_ANY_SOURCE ,& MPI_ANY_TAG , MPI_COMM_WORLD , req ( i ), ier ) END DO !------------------------------------------------------------------------- !-- wait for previous communication to end and deallocate variables -- CALL MPI_WAITALL ( extra_comms , req , istatus , ier ) CALL MPI_BARRIER ( MPI_COMM_WORLD , ier ) DEALLOCATE ( real_buffer , junk_real_buffer , req , status , istatus ) !-- close files -- CLOSE ( unstdy_unit , IOSTAT = ier ) IF ( ier /= 0 ) THEN WRITE ( log_unit , '(A,A,A,I5)' ) '***Error: failed to close ' , trim ( fname ),& ': errcode = ' , ier !err = .TRUE. END IF CLOSE ( mesh_unit , IOSTAT = ier ) IF ( ier /= 0 ) THEN WRITE ( log_unit , '(A,A,A,I5)' ) '***Error: failed to close ' , trim ( fname ),& ': errcode = ' , ier !err = .TRUE. END IF ELSE !------------------------------------------------------------------------- !-- find starting index of local primary vertices -- DO i = 1 , num_elm ( 1 ) IF ( lcl_complex ( 1 )% glb_indx ( i ) == glb_offset + 1 ) THEN ptr = i ; EXIT END IF END DO buffer_size = dim_embbd * num_pelm_pp ( rank + 1 ); ALLOCATE ( real_buffer ( buffer_size )) iib = 1 ; fib = dim_embbd DO j = 1 , num_pelm_pp ( rank + 1 ) real_buffer ( iib : fib ) = lcl_complex ( 1 )% centers ( ptr + j - 1 ,:) iib = iib + dim_embbd ; fib = fib + dim_embbd END DO !------------------------------------------------------------------------- !-- send primal vertex locations -- CALL MPI_ISEND ( real_buffer , buffer_size , MPI_DOUBLE_PRECISION , 0 , 0 ,& MPI_COMM_WORLD , junk , ier ) CALL MPI_REQUEST_FREE ( junk , ier ) !-- wait for previous communications to finish -- !CALL MPI_BARRIER(MPI_COMM_WORLD,ier) !------------------------------------------------------------------------- !-- send primal volume indices -- ! DO i = 1,num_elm(kmax) !   CALL MPI_ISEND(lcl_complex(kmax)%node_indx(i,1:kmax),kmax,MPI_INTEGER,0,& !     lcl_complex(kmax)%glb_indx(i),MPI_COMM_WORLD,junk,ier) !   CALL MPI_REQUEST_FREE(junk,ier) ! END DO !-- wait for previous communications to finish (free tags) -- CALL MPI_BARRIER ( MPI_COMM_WORLD , ier ); DEALLOCATE ( real_buffer ) !------------------------------------------------------------------------- !-- send primal volume values -- DO i = 1 , num_elm ( kmax ) CALL MPI_ISEND ( lcl_complex ( kmax )% dual_sol ( i , 1 ), 1 , MPI_DOUBLE_PRECISION , 0 ,& lcl_complex ( kmax )% glb_indx ( i ), MPI_COMM_WORLD , junk , ier ) CALL MPI_REQUEST_FREE ( junk , ier ) END DO !-- wait for previous communications to finish -- CALL MPI_BARRIER ( MPI_COMM_WORLD , ier ) !------------------------------------------------------------------------- !-- send cell center vector data -- DO i = 1 , num_elm ( kmax ) CALL MPI_ISEND ( lcl_complex ( kmax )% whtny_sol ( i , 1 : dim_embbd ), dim_embbd , MPI_DOUBLE_PRECISION , 0 ,& lcl_complex ( kmax )% glb_indx ( i ), MPI_COMM_WORLD , junk , ier ) CALL MPI_REQUEST_FREE ( junk , ier ) END DO !-- wait for previous communications to finish -- CALL MPI_BARRIER ( MPI_COMM_WORLD , ier ) END IF CALL syncwrite_log_time () curnt_time = tmp_time RETURN END SUBROUTINE ! io_mod|write_unsteady_D0S !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s*  io_mod/write_solution_D0S2 !* SYNOPSIS SUBROUTINE write_unsteady_D0S2 ( sclr_name , vctr_name , iter ) !* PURPOSE !*   write vtk solution at dual 0-cells for evolving simulations !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> write vtk solution at dual 0-cells for evolving simulations !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- CHARACTER ( LEN =* ), INTENT ( IN ) :: sclr_name !> scalar variable name (label) CHARACTER ( LEN =* ), INTENT ( IN ) :: vctr_name !> vector variable name (label) INTEGER , INTENT ( IN ) :: iter !> integer iteration number !-- local variables -- CHARACTER ( LEN = slen ) :: fname !> file name INTEGER :: prelen !> INTEGER :: kmax !> INTEGER :: i , j , k !> INTEGER :: iib , fib !> INTEGER :: ptr !> INTEGER :: extra_comms !> INTEGER , ALLOCATABLE :: int_buffer (:) !> INTEGER , ALLOCATABLE :: junk_int_buffer (:) !> REAL ( KIND = iwp ), ALLOCATABLE :: real_buffer (:) !> REAL ( KIND = iwp ), ALLOCATABLE :: junk_real_buffer (:) !> INTEGER :: buffer_size !> INTEGER :: junk !> INTEGER , ALLOCATABLE :: req (:) !> request variable for non-blocking comms INTEGER , ALLOCATABLE :: status (:) !> size of MPI comm buffer INTEGER , ALLOCATABLE :: istatus (:,:) !> size of MPI comm buffer REAL ( KIND = IWP ) :: tmp_time , tmp_time_int !> temporary timing variable INTEGER :: comm_size , max_comm_size , gindx , num_comm_iters , & strt_indx , resid_comm_size !> !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ CALL syncwrite_log ( '> write_unsteady_D0S()' ); tmp_time = curnt_time curnt_time = MPI_WTIME () !-- set commonly used variable -- kmax = dim_cmplx + 1 !-- open solution file -- prelen = len_trim ( unstdy_prefix ) fname = trim ( unstdy_prefix ) // \"_00000000.vtk\" WRITE ( fname ( prelen + 1 : prelen + 9 ), '(I9)' ) 100000000 + iter fname ( prelen + 1 : prelen + 1 ) = \"_\" CALL root_open_file_write ( fname , unstdy_unit ) !-- get some basic information for reuse -- max_comm_size = 10000 IF ( rank == root ) THEN !-- on root process -- !------------------------------------------------------------------------- !-- write header -- write ( unstdy_unit , '(A)' ) '# vtk DataFile Version 3.0' write ( unstdy_unit , '(A)' ) 'Unstructured Grid' write ( unstdy_unit , '(A)' ) 'ASCII' write ( unstdy_unit , '(A)' ) 'DATASET UNSTRUCTURED_GRID' !------------------------------------------------------------------------- !-- write primal vertex locations -- !-- write header -- write ( unstdy_unit , '(A,I10,A)' ) 'POINTS' , glb_num_elm ( 1 ), ' double' !-- write local data -- DO i = 1 , num_pelm_pp ( 1 ) write ( unstdy_unit , * ) lcl_complex ( 1 )% centers ( i ,:) END DO !-- collect data from adjacent processes and write to file -- !-- allocate MPI variables -- ALLOCATE ( status ( MPI_STATUS_SIZE )) DO k = 1 , num_procs - 1 num_comm_iters = num_pelm_pp ( k + 1 ) / max_comm_size resid_comm_size = mod ( num_pelm_pp ( k + 1 ), max_comm_size ) comm_size = max_comm_size buffer_size = comm_size * dim_embbd ALLOCATE ( real_buffer ( buffer_size )) DO i = 0 , num_comm_iters IF ( i == num_comm_iters ) THEN comm_size = resid_comm_size buffer_size = comm_size * dim_embbd END IF !-- recieve data from adjacent process -- CALL MPI_RECV ( real_buffer , buffer_size , MPI_DOUBLE_PRECISION , k , i ,& MPI_COMM_WORLD , status , ier ) !-- unpack buffer and write to file -- iib = 1 ; fib = dim_embbd DO j = 1 , comm_size write ( unstdy_unit , * ) real_buffer ( iib : fib ) iib = iib + dim_embbd ; fib = fib + dim_embbd END DO END DO DEALLOCATE ( real_buffer ) END DO !-- clean up -- DEALLOCATE ( status ) !------------------------------------------------------------------------- !-- write primal volume indices -- !-- write header -- write ( unstdy_unit , '(A,I10,I10)' ) 'CELLS' , glb_num_elm ( kmax ), ( kmax + 1 ) * glb_num_elm ( kmax ) !-- num_comm_iters = glb_num_elm ( kmax ) / max_comm_size resid_comm_size = mod ( glb_num_elm ( kmax ), max_comm_size ) comm_size = max_comm_size buffer_size = comm_size * kmax ALLOCATE ( int_buffer ( buffer_size ), junk_int_buffer ( buffer_size )) strt_indx = 1 DO i = 0 , num_comm_iters IF ( i == num_comm_iters ) THEN comm_size = resid_comm_size buffer_size = comm_size * kmax END IF int_buffer = 0 DO j = strt_indx , num_elm ( kmax ) gindx = lcl_complex ( kmax )% glb_indx ( j ) - i * max_comm_size IF ( gindx > comm_size ) THEN strt_indx = j EXIT END IF iib = ( gindx - 1 ) * kmax + 1 ; fib = iib + kmax - 1 IF ( lcl_complex ( kmax )% orientation ( j ) == 1 ) THEN int_buffer ( iib : fib ) = lcl_complex ( kmax )% node_indx ( j , 1 : kmax ) - 1 ELSE int_buffer ( iib ) = lcl_complex ( kmax )% node_indx ( j , 2 ) - 1 int_buffer ( iib + 1 ) = lcl_complex ( kmax )% node_indx ( j , 1 ) - 1 int_buffer ( iib + 2 : fib ) = lcl_complex ( kmax )% node_indx ( j , 3 : kmax ) - 1 END IF IF ( j == num_elm ( kmax )) strt_indx = num_elm ( kmax ) + 1 END DO CALL MPI_REDUCE ( int_buffer , junk_int_buffer , buffer_size , MPI_INTEGER , MPI_MAX , 0 , MPI_COMM_WORLD , ier ) iib = 1 ; fib = kmax - 1 DO j = 1 , comm_size WRITE ( unstdy_unit , * ) kmax , junk_int_buffer ( iib : iib + fib ) iib = iib + kmax END DO END DO !-- clean up -- DEALLOCATE ( int_buffer , junk_int_buffer ) !------------------------------------------------------------------------- !-- write primal volumes types -- !-- write header -- write ( unstdy_unit , '(A,I10)' ) 'CELL_TYPES' , glb_num_elm ( kmax ) !-- write data -- SELECT CASE ( dim_cmplx ) CASE ( 1 ) DO i = 1 , glb_num_elm ( kmax ); write ( unstdy_unit , '(I1)' ) 3 ; END DO CASE ( 2 ) DO i = 1 , glb_num_elm ( kmax ); write ( unstdy_unit , '(I1)' ) 5 ; END DO CASE ( 3 ) DO i = 1 , glb_num_elm ( kmax ); write ( unstdy_unit , '(I2)' ) 10 ; END DO CASE DEFAULT END SELECT !------------------------------------------------------------------------- !-- write primal volumes scalar data -- !-- write header -- write ( unstdy_unit , '(A,I10)' ) 'CELL_DATA' , glb_num_elm ( kmax ) write ( unstdy_unit , '(A,A,A)' ) 'SCALARS ' , sclr_name , ' double 1' write ( unstdy_unit , '(A)' ) 'LOOKUP_TABLE default' !-- comm_size = max_comm_size buffer_size = comm_size ALLOCATE ( real_buffer ( buffer_size ), junk_real_buffer ( buffer_size )) strt_indx = 1 DO i = 0 , num_comm_iters IF ( i == num_comm_iters ) THEN comm_size = resid_comm_size buffer_size = comm_size END IF real_buffer =- large DO j = strt_indx , num_elm ( kmax ) gindx = lcl_complex ( kmax )% glb_indx ( j ) - i * max_comm_size IF ( gindx > comm_size ) THEN strt_indx = j EXIT END IF real_buffer ( gindx ) = lcl_complex ( kmax )% dual_sol ( j , 1 ) IF ( j == num_elm ( kmax )) strt_indx = num_elm ( kmax ) + 1 END DO CALL MPI_REDUCE ( real_buffer , junk_real_buffer , buffer_size , MPI_DOUBLE_PRECISION , MPI_MAX , 0 , MPI_COMM_WORLD , ier ) DO j = 1 , comm_size WRITE ( unstdy_unit , * ) junk_real_buffer ( j ) END DO END DO !-- clean up -- DEALLOCATE ( real_buffer , junk_real_buffer ) !------------------------------------------------------------------------- !-- write cell center vector data -- !-- write header -- write ( unstdy_unit , '(A,A,A)' ) 'VECTORS ' , vctr_name , ' double' !-- comm_size = max_comm_size buffer_size = comm_size * dim_embbd ALLOCATE ( real_buffer ( buffer_size ), junk_real_buffer ( buffer_size )) strt_indx = 1 DO i = 0 , num_comm_iters IF ( i == num_comm_iters ) THEN comm_size = resid_comm_size buffer_size = comm_size * dim_embbd END IF real_buffer =- large DO j = strt_indx , num_elm ( kmax ) gindx = lcl_complex ( kmax )% glb_indx ( j ) - i * max_comm_size IF ( gindx > comm_size ) THEN strt_indx = j EXIT END IF iib = ( gindx - 1 ) * dim_embbd + 1 ; fib = iib + dim_embbd - 1 real_buffer ( iib : fib ) = lcl_complex ( kmax )% whtny_sol ( j , 1 : dim_embbd ) IF ( j == num_elm ( kmax )) strt_indx = num_elm ( kmax ) + 1 END DO CALL MPI_REDUCE ( real_buffer , junk_real_buffer , buffer_size , MPI_DOUBLE_PRECISION , MPI_MAX , 0 , MPI_COMM_WORLD , ier ) iib = 1 ; fib = dim_embbd - 1 DO j = 1 , comm_size WRITE ( unstdy_unit , * ) junk_real_buffer ( iib : iib + fib ) iib = iib + dim_embbd END DO END DO !-- clean up -- DEALLOCATE ( real_buffer , junk_real_buffer ) !------------------------------------------------------------------------- !-- close files -- CLOSE ( unstdy_unit , IOSTAT = ier ) IF ( ier /= 0 ) THEN WRITE ( log_unit , '(A,A,A,I5)' ) '***Error: failed to close ' , trim ( fname ),& ': errcode = ' , ier !err = .TRUE. END IF ELSE !-- adjacent processes -- !------------------------------------------------------------------------- !-- send header -- !------------------------------------------------------------------------- !-- send primal vertex locations -- !-- find starting index of local primary vertices -- DO i = 1 , num_elm ( 1 ) IF ( lcl_complex ( 1 )% glb_indx ( i ) == glb_offset + 1 ) THEN ptr = i - 1 ; EXIT END IF END DO !-- set up and pack buffer -- num_comm_iters = num_pelm_pp ( rank + 1 ) / max_comm_size resid_comm_size = mod ( num_pelm_pp ( rank + 1 ), max_comm_size ) comm_size = max_comm_size buffer_size = comm_size * dim_embbd ALLOCATE ( real_buffer ( buffer_size )) DO i = 0 , num_comm_iters IF ( i == num_comm_iters ) THEN comm_size = resid_comm_size buffer_size = comm_size * dim_embbd END IF iib = 1 ; fib = dim_embbd - 1 DO j = 1 , comm_size real_buffer ( iib : iib + fib ) = lcl_complex ( 1 )% centers ( ptr + j ,:) iib = iib + dim_embbd END DO !-- send primal vertex locations -- CALL MPI_SEND ( real_buffer , buffer_size , MPI_DOUBLE_PRECISION , 0 , i , MPI_COMM_WORLD , ier ) END DO DEALLOCATE ( real_buffer ) !------------------------------------------------------------------------- !-- send primal volume indices -- !-- num_comm_iters = glb_num_elm ( kmax ) / max_comm_size resid_comm_size = mod ( glb_num_elm ( kmax ), max_comm_size ) comm_size = max_comm_size buffer_size = comm_size * kmax ALLOCATE ( int_buffer ( buffer_size ), junk_int_buffer ( buffer_size )) strt_indx = 1 DO i = 0 , num_comm_iters IF ( i == num_comm_iters ) THEN comm_size = resid_comm_size buffer_size = comm_size * kmax END IF int_buffer = 0 DO j = strt_indx , num_elm ( kmax ) gindx = lcl_complex ( kmax )% glb_indx ( j ) - i * max_comm_size IF ( gindx > comm_size ) THEN strt_indx = j EXIT END IF iib = ( gindx - 1 ) * kmax + 1 ; fib = iib + kmax - 1 IF ( lcl_complex ( kmax )% orientation ( j ) == 1 ) THEN int_buffer ( iib : fib ) = lcl_complex ( kmax )% node_indx ( j , 1 : kmax ) - 1 ELSE int_buffer ( iib ) = lcl_complex ( kmax )% node_indx ( j , 2 ) - 1 int_buffer ( iib + 1 ) = lcl_complex ( kmax )% node_indx ( j , 1 ) - 1 int_buffer ( iib + 2 : fib ) = lcl_complex ( kmax )% node_indx ( j , 3 : kmax ) - 1 END IF IF ( j == num_elm ( kmax )) strt_indx = num_elm ( kmax ) + 1 END DO CALL MPI_REDUCE ( int_buffer , junk_int_buffer , buffer_size , MPI_INTEGER , MPI_MAX , 0 , MPI_COMM_WORLD , ier ) END DO !-- clean up -- DEALLOCATE ( int_buffer , junk_int_buffer ) !------------------------------------------------------------------------- !-- send primal volumes types -- !------------------------------------------------------------------------- !-- send primal volume scalar values -- !-- comm_size = max_comm_size buffer_size = comm_size ALLOCATE ( real_buffer ( buffer_size ), junk_real_buffer ( buffer_size )) strt_indx = 1 DO i = 0 , num_comm_iters IF ( i == num_comm_iters ) THEN comm_size = resid_comm_size buffer_size = comm_size END IF real_buffer =- large DO j = strt_indx , num_elm ( kmax ) gindx = lcl_complex ( kmax )% glb_indx ( j ) - i * max_comm_size IF ( gindx > comm_size ) THEN strt_indx = j EXIT END IF real_buffer ( gindx ) = lcl_complex ( kmax )% dual_sol ( j , 1 ) IF ( j == num_elm ( kmax )) strt_indx = num_elm ( kmax ) + 1 END DO CALL MPI_REDUCE ( real_buffer , junk_real_buffer , buffer_size , MPI_DOUBLE_PRECISION , MPI_MAX , 0 , MPI_COMM_WORLD , ier ) END DO !-- clean up -- DEALLOCATE ( real_buffer , junk_real_buffer ) !------------------------------------------------------------------------- !-- send cell center vector data -- !-- comm_size = max_comm_size buffer_size = comm_size * dim_embbd ALLOCATE ( real_buffer ( buffer_size ), junk_real_buffer ( buffer_size )) strt_indx = 1 DO i = 0 , num_comm_iters IF ( i == num_comm_iters ) THEN comm_size = resid_comm_size buffer_size = comm_size * dim_embbd END IF real_buffer =- large DO j = strt_indx , num_elm ( kmax ) gindx = lcl_complex ( kmax )% glb_indx ( j ) - i * max_comm_size IF ( gindx > comm_size ) THEN strt_indx = j EXIT END IF iib = ( gindx - 1 ) * dim_embbd + 1 ; fib = iib + dim_embbd - 1 real_buffer ( iib : fib ) = lcl_complex ( kmax )% whtny_sol ( j , 1 : dim_embbd ) IF ( j == num_elm ( kmax )) strt_indx = num_elm ( kmax ) + 1 END DO CALL MPI_REDUCE ( real_buffer , junk_real_buffer , buffer_size , MPI_DOUBLE_PRECISION , MPI_MAX , 0 , MPI_COMM_WORLD , ier ) END DO !-- clean up -- DEALLOCATE ( real_buffer , junk_real_buffer ) END IF !-- wait for previous communications to finish -- CALL MPI_BARRIER ( MPI_COMM_WORLD , ier ) CALL syncwrite_log_time () curnt_time = tmp_time RETURN END SUBROUTINE ! io_mod|write_unsteady_D0S2 !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s*  io_mod/read_crack_faces !* SYNOPSIS SUBROUTINE read_crack_faces () !* PURPOSE !*   read crack faces from file !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> read crack faces from file !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- local variables -- CHARACTER ( LEN = slen ) :: fname !> file name INTEGER :: junk !> junk IO variable INTEGER :: ie , in , i , j !> loop indices (procs,elems) INTEGER :: buffer_size !> size of MPI comm buffer INTEGER , ALLOCATABLE :: int_buffer (:,:) !> integer buffer for MPI comms INTEGER , ALLOCATABLE :: req (:) !> request variable for non-blocking comms INTEGER , ALLOCATABLE :: status (:) !> size of MPI comm buffer INTEGER , ALLOCATABLE :: prml_elm (:,:) !> primal element list INTEGER :: proc , proc_old !> processor rank (current, past) INTEGER :: ptr !> pointer INTEGER :: glb_num_cracks !> global number of cracks !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- Open mesh file and check for errors -- fname = trim ( mesh_prefix ) // \".crack\" ; CALL root_open_file_read ( fname , mesh_unit ) !-- read header and broadcast global number of entities -- IF ( rank == root ) READ ( mesh_unit , * ) glb_num_cracks CALL MPI_BCAST ( glb_num_cracks , 1 , MPI_INTEGER , 0 , MPI_COMM_WORLD , ier ) !-- set buffer size and allocate temp. storage for primal elements -- buffer_size = dim_cmplx + 1 ALLOCATE ( prml_elm ( 4 * min ( num_elm ( dim_cmplx ), glb_num_cracks ), buffer_size )) IF ( rank == root ) THEN !-- on root process -- !-- allocate integer communication buffer -- ALLOCATE ( int_buffer ( buffer_size , glb_num_cracks + 1 )) !-- loop through primal elements -- ptr = 0 ; DO ie = 1 , glb_num_cracks !-- read nodal indices of primal element and sort -- READ ( mesh_unit , * ) int_buffer (:, ie ) int_buffer ( 1 , ie ) = ie int_buffer ( 2 : buffer_size , ie ) = int_buffer ( 2 : buffer_size , ie ) + indx_offset ( 1 ) CALL int_insertion_sort ( int_buffer ( 2 : buffer_size , ie )) !-- loop through dual elements -- proc_old = - 1 DO in = 2 , buffer_size !-- get process of dual element -- proc = elm2proc ( int_buffer ( in , ie )) IF ( proc == 0 . and . proc /= proc_old ) THEN !-- if local: assign to local array -- ptr = ptr + 1 ; prml_elm ( ptr ,:) = int_buffer (:, ie ) ELSEIF ( proc /= proc_old ) THEN !-- otherwise: send data to neighbouring process -- CALL MPI_ISEND ( int_buffer (:, ie ), buffer_size , MPI_INTEGER , proc , 0 ,& MPI_COMM_WORLD , junk , ier ) CALL MPI_REQUEST_FREE ( junk , ier ) END IF proc_old = proc END DO END DO !-- send communication termination message -- int_buffer (:, ie ) = - 1 DO proc = 1 , num_procs - 1 CALL MPI_ISEND ( int_buffer (:, ie ), buffer_size , MPI_INTEGER , proc , 0 ,& MPI_COMM_WORLD , junk , ier ) CALL MPI_REQUEST_FREE ( junk , ier ) END DO ELSE !-- on neighbouring process -- !-- allocate integer communication buffer and status variable -- ALLOCATE ( int_buffer ( buffer_size , 1 ), status ( MPI_STATUS_SIZE )) !-- poll for incoming MPI communications -- ptr = 0 DO !-- receive nodal indices (blocking) -- CALL MPI_RECV ( int_buffer , buffer_size , MPI_INTEGER , 0 , 0 ,& MPI_COMM_WORLD , status , ier ) IF ( ALL ( int_buffer ==- 1 )) THEN !-- received termination message: stop polling for communication -- EXIT ELSE !-- assign to local array -- ptr = ptr + 1 ; prml_elm ( ptr ,:) = int_buffer (:, 1 ) END IF END DO END IF !-- assign primal element data to local structures -- lcl_complex ( dim_cmplx )% num_crack = ptr if ( ptr > 0 ) ALLOCATE ( lcl_complex ( dim_cmplx )% crack_indx ( ptr )) DO i = 1 , ptr j = 1 DO IF ( ALL ( prml_elm ( i , 2 : buffer_size ) == lcl_complex ( dim_cmplx )% node_indx ( j ,:) - indx_offset ( 1 ))) THEN lcl_complex ( dim_cmplx )% crack_indx ( i ) = j EXIT END IF j = j + 1 END DO END DO !-- close file and clean up -- IF ( rank == root ) THEN CLOSE ( mesh_unit , IOSTAT = ier ) IF ( ier /= 0 ) THEN WRITE ( log_unit , '(A,A,A,I5)' ) '***Error: failed to close ' , trim ( fname ),& ': errcode = ' , ier !err = .TRUE. END IF DEALLOCATE ( prml_elm , int_buffer ) ELSE DEALLOCATE ( prml_elm , int_buffer , status ) END IF !-- make sure all comms have completed -- CALL MPI_BARRIER ( MPI_COMM_WORLD , ier ) RETURN END SUBROUTINE ! io_mod|read_crack_faces !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s*  io_mod/read_bndry_cond !* SYNOPSIS SUBROUTINE read_bndry_cond () !* PURPOSE !*   Read boundary conditions from file !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Read boundary conditions from file !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- local variables -- CHARACTER ( LEN = slen ) :: fname !> file name INTEGER :: junk !> junk IO variable INTEGER , ALLOCATABLE :: node_indices (:) !> junk IO variable for node indicies INTEGER :: ie , in , i , j !> loop indices (procs,elems) INTEGER :: buffer_size !> size of MPI comm buffer INTEGER , ALLOCATABLE :: int_buffer (:,:) !> integer buffer for MPI comms INTEGER , ALLOCATABLE :: req (:) !> request variable for non-blocking comms INTEGER , ALLOCATABLE :: status (:) !> size of MPI comm buffer INTEGER , ALLOCATABLE :: prml_elm (:,:) !> primal element list INTEGER :: proc , proc_old !> processor rank (current, past) INTEGER :: ptr !> pointer !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- Open mesh file and check for errors -- fname = trim ( mesh_prefix ) // \".face\" ; CALL root_open_file_read ( fname , mesh_unit ) !-- read header and broadcast global number of entities -- IF ( rank == root ) READ ( mesh_unit , * ) !-- allocate boundary type -- ALLOCATE ( lcl_complex ( dim_cmplx )% bc_type ( num_elm ( dim_cmplx ))) lcl_complex ( dim_cmplx )% bc_type = 0 !-- set buffer size and allocate temp. storage for primal elements -- buffer_size = 2 ALLOCATE ( prml_elm ( num_elm ( dim_cmplx ), buffer_size )) IF ( rank == root ) THEN !-- on root process -- !-- allocate integer communication buffer -- ALLOCATE ( int_buffer ( buffer_size , glb_num_elm ( dim_cmplx ) + 1 ), & node_indices ( dim_cmplx )) !-- loop through primal elements -- ptr = 0 ; DO ie = 1 , glb_num_elm ( dim_cmplx ) !-- read nodal indices of primal element and sort -- READ ( mesh_unit , * ) junk , node_indices , int_buffer ( 2 , ie ) IF ( int_buffer ( 2 , ie ) == 0 ) CYCLE int_buffer ( 1 , ie ) = ie node_indices = node_indices + indx_offset ( 1 ) CALL int_insertion_sort ( node_indices ) !-- loop through dual elements -- proc_old = - 1 DO in = 1 , dim_cmplx !-- get process of dual element -- proc = elm2proc ( node_indices ( in )) IF ( proc == 0 . and . proc /= proc_old ) THEN !-- if local: assign to local array -- ptr = ptr + 1 ; prml_elm ( ptr ,:) = int_buffer (:, ie ) ELSEIF ( proc /= proc_old ) THEN !-- otherwise: send data to neighbouring process -- CALL MPI_ISEND ( int_buffer (:, ie ), buffer_size , MPI_INTEGER , proc , 0 ,& MPI_COMM_WORLD , junk , ier ) CALL MPI_REQUEST_FREE ( junk , ier ) END IF proc_old = proc END DO END DO !-- send communication termination message -- int_buffer (:, ie ) = - 1 DO proc = 1 , num_procs - 1 CALL MPI_ISEND ( int_buffer (:, ie ), buffer_size , MPI_INTEGER , proc , 0 ,& MPI_COMM_WORLD , junk , ier ) CALL MPI_REQUEST_FREE ( junk , ier ) END DO ELSE !-- on neighbouring process -- !-- allocate integer communication buffer and status variable -- ALLOCATE ( int_buffer ( buffer_size , 1 ), status ( MPI_STATUS_SIZE )) !-- poll for incoming MPI communications -- ptr = 0 DO !-- receive nodal indices (blocking) -- CALL MPI_RECV ( int_buffer (:, 1 ), buffer_size , MPI_INTEGER , 0 , 0 ,& MPI_COMM_WORLD , status , ier ) IF ( ALL ( int_buffer ==- 1 )) THEN !-- received termination message: stop polling for communication -- EXIT ELSE !-- assign to local array -- ptr = ptr + 1 ; prml_elm ( ptr ,:) = int_buffer (:, 1 ) END IF END DO END IF !-- assign primal element data to local structures -- lcl_complex ( dim_cmplx + 1 )% surf_indx (:, 4 ) = 0 ptr_loop : DO i = 1 , ptr DO j = 1 , lcl_complex ( dim_cmplx + 1 )% num_surf IF ( prml_elm ( i , 1 ) == lcl_complex ( dim_cmplx )% glb_indx ( lcl_complex ( dim_cmplx + 1 )% surf_indx ( j , 2 ))) THEN lcl_complex ( dim_cmplx + 1 )% surf_indx ( j , 4 ) = prml_elm ( i , 2 ) EXIT END IF END DO DO j = 1 , num_elm ( dim_cmplx ) IF ( prml_elm ( i , 1 ) == lcl_complex ( dim_cmplx )% glb_indx ( j )) THEN lcl_complex ( dim_cmplx )% bc_type ( j ) = prml_elm ( i , 2 ) CYCLE ptr_loop END IF END DO WRITE ( log_unit , '(A,I5)' ) 'Error : face not found = ' , prml_elm ( i , 1 ) CALL end_mpi () END DO ptr_loop !-- close file and clean up -- IF ( rank == root ) THEN CLOSE ( mesh_unit , IOSTAT = ier ) IF ( ier /= 0 ) THEN WRITE ( log_unit , '(A,A,A,I5)' ) '***Error: failed to close ' , trim ( fname ),& ': errcode = ' , ier !err = .TRUE. END IF DEALLOCATE ( prml_elm , int_buffer , node_indices ) ELSE DEALLOCATE ( prml_elm , int_buffer , status ) END IF !-- make sure all comms have completed -- CALL MPI_BARRIER ( MPI_COMM_WORLD , ier ) RETURN END SUBROUTINE ! io_mod|read_bndry_cond !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s*  io_mod/read_bndry_cond2 !* SYNOPSIS SUBROUTINE read_bndry_cond2 () !* PURPOSE !*   Read boundary conditions from file !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Read boundary conditions from file !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- local variables -- CHARACTER ( LEN = slen ) :: fname !> file name INTEGER :: junk !> junk IO variable INTEGER , ALLOCATABLE :: node_indices (:) !> junk IO variable for node indicies INTEGER :: ie , in , i , j !> loop indices (procs,elems) INTEGER :: buffer_size !> size of MPI comm buffer INTEGER , ALLOCATABLE :: int_buffer (:) !> integer buffer for MPI comms INTEGER , ALLOCATABLE :: req (:) !> request variable for non-blocking comms INTEGER , ALLOCATABLE :: status (:) !> size of MPI comm buffer INTEGER , ALLOCATABLE :: prml_elm (:,:) !> primal element list INTEGER :: proc , proc_old !> processor rank (current, past) INTEGER :: ptr !> pointer INTEGER :: k , l , orient , iib , fib , num_read_iters , & max_read_size , resid_read_size , read_size , stride , strt_indx !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- Open mesh file and check for errors -- fname = trim ( mesh_prefix ) // \".face\" ; CALL root_open_file_read ( fname , mesh_unit ) !-- read header and broadcast global number of entities -- IF ( rank == root ) READ ( mesh_unit , * ) !-- allocate boundary type -- ALLOCATE ( lcl_complex ( dim_cmplx )% bc_type ( num_elm ( dim_cmplx ))) lcl_complex ( dim_cmplx )% bc_type = 0 lcl_complex ( dim_cmplx + 1 )% surf_indx (:, 4 ) = 0 !-- get some basic information for reuse -- k = dim_cmplx max_read_size = 1000 ; read_size = max_read_size stride = 2 ; buffer_size = read_size * stride num_read_iters = glb_num_elm ( k ) / read_size resid_read_size = mod ( glb_num_elm ( k ), read_size ) ptr = 0 ; fib = 1 ALLOCATE ( int_buffer ( read_size * ( stride + 1 ))) IF ( rank == root ) THEN !-- on root process -- !-- allocate integer communication buffer -- ALLOCATE ( node_indices ( dim_cmplx )) !-- DO i = 0 , num_read_iters IF ( i == num_read_iters ) THEN read_size = resid_read_size buffer_size = read_size * stride END IF !-- iib = 1 DO j = 1 , read_size READ ( mesh_unit , * ) junk , node_indices , int_buffer ( iib + 1 ) !-- int_buffer ( iib ) = i * max_read_size + j iib = iib + stride END DO !-- CALL MPI_BCAST ( int_buffer , buffer_size , MPI_INTEGER , 0 , MPI_COMM_WORLD , ier ) !-- iib = 1 DO j = 1 , read_size IF ( int_buffer ( iib + 1 ) == 0 ) THEN iib = iib + stride ; CYCLE END IF DO l = 1 , lcl_complex ( dim_cmplx + 1 )% num_surf IF ( int_buffer ( iib ) == lcl_complex ( dim_cmplx )% glb_indx ( lcl_complex ( dim_cmplx + 1 )% surf_indx ( l , 2 ))) THEN lcl_complex ( dim_cmplx + 1 )% surf_indx ( l , 4 ) = int_buffer ( iib + 1 ) EXIT END IF END DO DO l = 1 , num_elm ( dim_cmplx ) IF ( int_buffer ( iib ) == lcl_complex ( dim_cmplx )% glb_indx ( l )) THEN lcl_complex ( dim_cmplx )% bc_type ( l ) = int_buffer ( iib + 1 ) EXIT END IF END DO iib = iib + stride END DO END DO !-- close file -- DEALLOCATE ( node_indices ) CLOSE ( mesh_unit , IOSTAT = ier ) IF ( ier /= 0 ) THEN WRITE ( log_unit , '(A,A,A,I5)' ) '***Error: failed to close ' , trim ( fname ),& ': errcode = ' , ier !err = .TRUE. END IF ELSE !-- on neighbouring processes -- !-- DO i = 0 , num_read_iters IF ( i == num_read_iters ) THEN read_size = resid_read_size buffer_size = read_size * stride END IF !-- CALL MPI_BCAST ( int_buffer , buffer_size , MPI_INTEGER , 0 , MPI_COMM_WORLD , ier ) !-- iib = 1 DO j = 1 , read_size IF ( int_buffer ( iib + 1 ) == 0 ) THEN iib = iib + stride ; CYCLE END IF DO l = 1 , lcl_complex ( dim_cmplx + 1 )% num_surf IF ( int_buffer ( iib ) == lcl_complex ( dim_cmplx )% glb_indx ( lcl_complex ( dim_cmplx + 1 )% surf_indx ( l , 2 ))) THEN lcl_complex ( dim_cmplx + 1 )% surf_indx ( l , 4 ) = int_buffer ( iib + 1 ) EXIT END IF END DO DO l = 1 , num_elm ( dim_cmplx ) IF ( int_buffer ( iib ) == lcl_complex ( dim_cmplx )% glb_indx ( l )) THEN lcl_complex ( dim_cmplx )% bc_type ( l ) = int_buffer ( iib + 1 ) EXIT END IF END DO iib = iib + stride END DO END DO END IF !-- clean up -- DEALLOCATE ( int_buffer ) !-- make sure all comms have completed -- CALL MPI_BARRIER ( MPI_COMM_WORLD , ier ) RETURN END SUBROUTINE ! io_mod|read_bndry_cond2 !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s*  io_mod/init_write_MATLAB !* SYNOPSIS SUBROUTINE init_write_MATLAB () !* PURPOSE !*   Setup PETSc for MATLAB output !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2020/09/23 !> Setup PETSc for MATLAB output !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- Set MATLAB as output format -- CALL PetscViewerPushFormat ( output_view , PETSC_VIEWER_ASCII_MATLAB , petsc_ier ) RETURN END SUBROUTINE ! io_mod|write_centers_MATLAB !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s*  io_mod/clean_PETSc_output !* SYNOPSIS SUBROUTINE clean_PETSc_output () !* PURPOSE !*   Clean up PETSc output format !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2020/09/23 !> Clean up PETSc output format !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- number of rows -- CALL PetscViewerPopFormat ( output_view , petsc_ier ) RETURN END SUBROUTINE ! io_mod|clean_PETSc_output !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s*  io_mod/write_centers_MATLAB2 !* SYNOPSIS SUBROUTINE write_centers_MATLAB2 () !* PURPOSE !*   write barycenters to file in MATLAB format !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> write barycenters to file in MATLAB format !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- local variables -- INTEGER :: i , id !> loop and temporary indicies INTEGER :: m !> number of rows in global solution variables CHARACTER ( LEN = slen ) :: fname !> file name Vec :: work !> PETSc work array !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- number of rows -- m = glb_num_elm ( dim_cmplx + 1 ) + glb_num_elm ( dim_cmplx ) !-- write circumcenters to MATLAB file -- CALL VecCreateMPI ( MPI_COMM_WORLD , PETSC_DECIDE , m , work , petsc_ier ) fname = trim ( sol_prefix ) // '_xyz.m' CALL PetscViewerASCIIOpen ( MPI_COMM_WORLD , fname , output_view , petsc_ier ) CALL PetscViewerPushFormat ( output_view , PETSC_VIEWER_ASCII_MATLAB , petsc_ier ) DO id = 1 , 3 DO i = 1 , num_elm ( dim_cmplx ) CALL VecSetValues ( work , 1 , lcl_complex ( dim_cmplx )% glb_indx ( i ) - 1 ,& lcl_complex ( dim_cmplx )% centers ( i , id ), INSERT_VALUES , petsc_ier ) END DO DO i = 1 , num_elm ( dim_cmplx + 1 ) CALL VecSetValues ( work , 1 , glb_num_elm ( dim_cmplx ) + lcl_complex ( dim_cmplx + 1 )% glb_indx ( i ) - 1 ,& lcl_complex ( dim_cmplx + 1 )% centers ( i , id ), INSERT_VALUES , petsc_ier ) END DO CALL VecAssemblyBegin ( work , petsc_ier ); CALL VecAssemblyEnd ( work , petsc_ier ) SELECT CASE ( id ) CASE ( 1 ) CALL PetscObjectSetName ( work , 'x' , petsc_ier ) CASE ( 2 ) CALL PetscObjectSetName ( work , 'y' , petsc_ier ) CASE ( 3 ) CALL PetscObjectSetName ( work , 'z' , petsc_ier ) END SELECT CALL VecView ( work , output_view , petsc_ier ) END DO CALL PetscViewerDestroy ( output_view , petsc_ier ) CALL VecDestroy ( work , petsc_ier ) RETURN END SUBROUTINE ! io_mod|write_centers_MATLAB2 !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s*  io_mod/write_prml_volumes_MATLAB2 !* SYNOPSIS SUBROUTINE write_prml_volumes_MATLAB2 () !* PURPOSE !*   write primary volumes to file in MATLAB format !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> write primary volumes to file in MATLAB format !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- local variables -- INTEGER :: i , id !> loop and temporary indicies INTEGER :: m !> number of rows in global solution variables CHARACTER ( LEN = slen ) :: fname !> file name Vec :: work !> PETSc work array !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- number of rows -- m = glb_num_elm ( dim_cmplx + 1 ) + glb_num_elm ( dim_cmplx ) !-- write volumes to MATLAB file -- CALL VecCreateMPI ( MPI_COMM_WORLD , PETSC_DECIDE , m , work , petsc_ier ) fname = trim ( sol_prefix ) // '_vol.m' CALL PetscViewerASCIIOpen ( MPI_COMM_WORLD , fname , output_view , petsc_ier ) CALL PetscViewerPushFormat ( output_view , PETSC_VIEWER_ASCII_MATLAB , petsc_ier ) DO i = 1 , num_elm ( dim_cmplx ) CALL VecSetValues ( work , 1 , lcl_complex ( dim_cmplx )% glb_indx ( i ) - 1 ,& lcl_complex ( dim_cmplx )% prml_volume ( i ), INSERT_VALUES , petsc_ier ) END DO DO i = 1 , num_elm ( dim_cmplx + 1 ) CALL VecSetValues ( work , 1 , glb_num_elm ( dim_cmplx ) + lcl_complex ( dim_cmplx + 1 )% glb_indx ( i ) - 1 ,& lcl_complex ( dim_cmplx + 1 )% prml_volume ( i ), INSERT_VALUES , petsc_ier ) END DO CALL VecAssemblyBegin ( work , petsc_ier ); CALL VecAssemblyEnd ( work , petsc_ier ) CALL PetscObjectSetName ( work , 'vol' , petsc_ier ) CALL VecView ( work , output_view , petsc_ier ) CALL VecDestroy ( work , petsc_ier ) CALL PetscViewerDestroy ( output_view , petsc_ier ) RETURN END SUBROUTINE ! io_mod|write_prml_volumes_MATLAB2 !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s*  io_mod/write_solution_MATLAB2 !* SYNOPSIS SUBROUTINE write_solution_MATLAB2 ( iter ) !* PURPOSE !*   write solution to file in MATLAB format !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> write solution to file in MATLAB format !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- INTEGER , INTENT ( IN ), OPTIONAL :: iter !> iteration for file name !-- local variables -- INTEGER :: prelen !> string pointer CHARACTER ( LEN = slen ) :: fname !> file name !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IF ( PRESENT ( iter )) THEN prelen = len_trim ( unstdy_prefix ) fname = trim ( unstdy_prefix ) // \"_00000000.m\" WRITE ( fname ( prelen + 1 : prelen + 9 ), '(I9)' ) 100000000 + iter fname ( prelen + 1 : prelen + 1 ) = \"_\" ELSE fname = trim ( sol_prefix ) // '.m' END IF CALL PetscViewerASCIIOpen ( MPI_COMM_WORLD , fname , output_view , petsc_ier ) CALL PetscViewerPushFormat ( output_view , PETSC_VIEWER_ASCII_MATLAB , petsc_ier ) CALL PetscObjectSetName ( sol , 'flux' , petsc_ier ) CALL VecView ( sol , output_view , petsc_ier ) CALL PetscViewerDestroy ( output_view , petsc_ier ) RETURN END SUBROUTINE ! io_mod|write_solution_MATLAB2 !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s*  io_mod/write_pressure_MATLAB2 !* SYNOPSIS SUBROUTINE write_pressure_MATLAB2 ( iter ) !* PURPOSE !*   write scaled pressure solution to file in MATLAB format !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> write scaled pressure solution to file in MATLAB format !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- INTEGER , INTENT ( IN ), OPTIONAL :: iter !> iteration for file name !-- local variables -- INTEGER :: prelen !> string pointer CHARACTER ( LEN = slen ) :: fname !> file name INTEGER :: i , j !> loop index REAL ( KIND = iwp ), ALLOCATABLE :: sol_edge (:) !> solution variable for MATLAB output REAL ( KIND = iwp ), ALLOCATABLE :: cnt (:) !> counter REAL ( KIND = iwp ) :: dxyz !> extrapolation variable REAL ( KIND = iwp ) :: nan !> nan variable INTEGER :: indx !> face index Vec :: work !> PETSc work array !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IF ( PRESENT ( iter )) THEN prelen = len_trim ( unstdy_prefix ) fname = trim ( unstdy_prefix ) // \"_00000000_press.m\" WRITE ( fname ( prelen + 1 : prelen + 9 ), '(I9)' ) 100000000 + iter fname ( prelen + 1 : prelen + 1 ) = \"_\" ELSE fname = trim ( sol_prefix ) // '.m' END IF !-- extrapolate surface pressures from volumes -- !-- interior faces -- ALLOCATE ( cnt ( num_elm ( dim_cmplx )), sol_edge ( num_elm ( dim_cmplx ))) cnt = 0.d0 ; sol_edge = 0.d0 DO i = 1 , num_elm ( dim_cmplx + 1 ) DO j = 1 , lcl_complex ( dim_cmplx + 1 )% num_bndry ( i ) indx = lcl_complex ( dim_cmplx + 1 )% bndry ( i )% indx ( j ) dxyz = lcl_complex ( dim_cmplx + 1 )% centers ( i , 3 ) - lcl_complex ( dim_cmplx )% centers ( indx , 3 ) sol_edge ( indx ) = sol_edge ( indx ) + & lcl_complex ( dim_cmplx )% prml_volume ( indx ) * ( & lcl_complex ( dim_cmplx + 1 )% dual_sol ( i , 1 ) + dxyz ) cnt ( indx ) = cnt ( indx ) + 1.d0 END DO END DO !-- surface faces -- DO j = 1 , lcl_complex ( dim_cmplx + 1 )% num_surf i = lcl_complex ( dim_cmplx + 1 )% surf_indx ( j , 3 ) indx = lcl_complex ( dim_cmplx + 1 )% surf_indx ( j , 2 ) dxyz = lcl_complex ( dim_cmplx + 1 )% centers ( i , 3 ) - lcl_complex ( dim_cmplx )% centers ( indx , 3 ) sol_edge ( indx ) = sol_edge ( indx ) + & lcl_complex ( dim_cmplx )% prml_volume ( indx ) * ( & lcl_complex ( dim_cmplx + 1 )% dual_sol ( i , 1 ) + dxyz ) cnt ( indx ) = cnt ( indx ) + 1.d0 END DO !-- set crack faces to NaN by zeroing the face count -- DO i = 1 , lcl_complex ( dim_cmplx )% num_crack indx = lcl_complex ( dim_cmplx )% crack_indx ( i ) sol_edge ( indx ) = IEEE_VALUE ( nan , IEEE_QUIET_NAN ) END DO !-- write extrapolated pressure solution to MATLAB file -- CALL VecDuplicate ( sol , work , petsc_ier ) DO i = 1 , num_elm ( dim_cmplx ) IF ( cnt ( i ) < 1.d0 ) CYCLE CALL VecSetValues ( work , 1 , lcl_complex ( dim_cmplx )% glb_indx ( i ) - 1 ,& sol_edge ( i ) / cnt ( i ), INSERT_VALUES , petsc_ier ) END DO DO i = 1 , num_elm ( dim_cmplx + 1 ) CALL VecSetValues ( work , 1 , glb_num_elm ( dim_cmplx ) + lcl_complex ( dim_cmplx + 1 )% glb_indx ( i ) - 1 ,& lcl_complex ( dim_cmplx + 1 )% dual_sol ( i , 1 ), INSERT_VALUES , petsc_ier ) END DO DEALLOCATE ( cnt , sol_edge ) CALL VecAssemblyBegin ( work , petsc_ier ); CALL VecAssemblyEnd ( work , petsc_ier ) CALL PetscViewerASCIIOpen ( MPI_COMM_WORLD , fname , output_view , petsc_ier ) CALL PetscViewerPushFormat ( output_view , PETSC_VIEWER_ASCII_MATLAB , petsc_ier ) CALL PetscObjectSetName ( work , 'press' , petsc_ier ) CALL VecView ( work , output_view , petsc_ier ) CALL VecDestroy ( work , petsc_ier ) CALL PetscViewerDestroy ( output_view , petsc_ier ) RETURN END SUBROUTINE ! io_mod|write_pressure_MATLAB2 !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s*  io_mod/write_centers_MATLAB !* SYNOPSIS SUBROUTINE write_centers_MATLAB () !* PURPOSE !*   write barycenters to file in MATLAB format !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> write barycenters to file in MATLAB format !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- local variables -- INTEGER :: i , id !> loop and temporary indicies CHARACTER ( LEN = slen ) :: fname !> file name Vec :: work !> PETSc work array Vec :: wrk1 , wrk2 !> sub arrays !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- duplicate solution vector -- CALL VecDuplicate ( sol , work , petsc_ier ) !-- write circumcenters to MATLAB file -- fname = trim ( sol_prefix ) // '_xyz.m' CALL PetscViewerASCIIOpen ( MPI_COMM_WORLD , fname , output_view , petsc_ier ) CALL PetscViewerPushFormat ( output_view , PETSC_VIEWER_ASCII_MATLAB , petsc_ier ) DO id = 1 , 3 CALL VecGetSubVector ( work , isg ( 1 ), wrk1 , petsc_ier ) CALL VecGetSubVector ( work , isg ( 2 ), wrk2 , petsc_ier ) DO i = 1 , num_elm ( dim_cmplx ) CALL VecSetValues ( wrk1 , 1 , lcl_complex ( dim_cmplx )% glb_indx ( i ) - 1 ,& lcl_complex ( dim_cmplx )% centers ( i , id ), INSERT_VALUES , petsc_ier ) END DO DO i = 1 , num_elm ( dim_cmplx + 1 ) CALL VecSetValues ( wrk2 , 1 , lcl_complex ( dim_cmplx + 1 )% glb_indx ( i ) - 1 ,& lcl_complex ( dim_cmplx + 1 )% centers ( i , id ), INSERT_VALUES , petsc_ier ) END DO CALL VecAssemblyBegin ( wrk1 , petsc_ier ); CALL VecAssemblyEnd ( wrk1 , petsc_ier ) CALL VecAssemblyBegin ( wrk2 , petsc_ier ); CALL VecAssemblyEnd ( wrk2 , petsc_ier ) CALL VecRestoreSubVector ( work , isg ( 1 ), wrk1 , petsc_ier ) CALL VecRestoreSubVector ( work , isg ( 2 ), wrk2 , petsc_ier ) SELECT CASE ( id ) CASE ( 1 ) CALL PetscObjectSetName ( work , 'x' , petsc_ier ) CASE ( 2 ) CALL PetscObjectSetName ( work , 'y' , petsc_ier ) CASE ( 3 ) CALL PetscObjectSetName ( work , 'z' , petsc_ier ) END SELECT CALL VecView ( work , output_view , petsc_ier ) END DO CALL PetscViewerDestroy ( output_view , petsc_ier ) CALL VecDestroy ( work , petsc_ier ) RETURN END SUBROUTINE ! io_mod|write_centers_MATLAB !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s*  io_mod/write_prml_volumes_MATLAB !* SYNOPSIS SUBROUTINE write_prml_volumes_MATLAB () !* PURPOSE !*   write primal volumes to file in MATLAB format !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> write primal volumes to file in MATLAB format !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- local variables -- INTEGER :: i , id !> loop and temporary indicies CHARACTER ( LEN = slen ) :: fname !> file name Vec :: work !> PETSc work array Vec :: wrk1 , wrk2 !> sub arrays !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- duplicate solution vector -- CALL VecDuplicate ( sol , work , petsc_ier ) CALL VecGetSubVector ( work , isg ( 1 ), wrk1 , petsc_ier ) CALL VecGetSubVector ( work , isg ( 2 ), wrk2 , petsc_ier ) !-- write volumes to MATLAB file -- fname = trim ( sol_prefix ) // '_vol.m' CALL PetscViewerASCIIOpen ( MPI_COMM_WORLD , fname , output_view , petsc_ier ) CALL PetscViewerPushFormat ( output_view , PETSC_VIEWER_ASCII_MATLAB , petsc_ier ) DO i = 1 , num_elm ( dim_cmplx ) CALL VecSetValues ( wrk1 , 1 , lcl_complex ( dim_cmplx )% glb_indx ( i ) - 1 ,& lcl_complex ( dim_cmplx )% prml_volume ( i ), INSERT_VALUES , petsc_ier ) END DO DO i = 1 , num_elm ( dim_cmplx + 1 ) CALL VecSetValues ( wrk2 , 1 , lcl_complex ( dim_cmplx + 1 )% glb_indx ( i ) - 1 ,& lcl_complex ( dim_cmplx + 1 )% prml_volume ( i ), INSERT_VALUES , petsc_ier ) END DO CALL VecAssemblyBegin ( wrk1 , petsc_ier ); CALL VecAssemblyEnd ( wrk1 , petsc_ier ) CALL VecAssemblyBegin ( wrk2 , petsc_ier ); CALL VecAssemblyEnd ( wrk2 , petsc_ier ) CALL VecRestoreSubVector ( work , isg ( 1 ), wrk1 , petsc_ier ) CALL VecRestoreSubVector ( work , isg ( 2 ), wrk2 , petsc_ier ) CALL PetscObjectSetName ( work , 'vol' , petsc_ier ) CALL VecView ( work , output_view , petsc_ier ) CALL VecDestroy ( work , petsc_ier ) CALL PetscViewerDestroy ( output_view , petsc_ier ) RETURN END SUBROUTINE ! io_mod|write_prml_volumes_MATLAB !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s*  io_mod/write_solution_MATLAB !* SYNOPSIS SUBROUTINE write_solution_MATLAB ( iter ) !* PURPOSE !*   write solution to file in MATLAB format !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> write solution to file in MATLAB format !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- INTEGER , INTENT ( IN ), OPTIONAL :: iter !> iteration for file name !-- local variables -- INTEGER :: prelen !> string pointer CHARACTER ( LEN = slen ) :: fname !> file name !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IF ( PRESENT ( iter )) THEN prelen = len_trim ( unstdy_prefix ) fname = trim ( unstdy_prefix ) // \"_00000000.m\" WRITE ( fname ( prelen + 1 : prelen + 9 ), '(I9)' ) 100000000 + iter fname ( prelen + 1 : prelen + 1 ) = \"_\" ELSE fname = trim ( sol_prefix ) // '.m' END IF CALL PetscViewerASCIIOpen ( MPI_COMM_WORLD , fname , output_view , petsc_ier ) CALL PetscViewerPushFormat ( output_view , PETSC_VIEWER_ASCII_MATLAB , petsc_ier ) CALL PetscObjectSetName ( sol , 'flux' , petsc_ier ) CALL VecView ( sol , output_view , petsc_ier ) CALL PetscViewerDestroy ( output_view , petsc_ier ) RETURN END SUBROUTINE ! io_mod|write_solution_MATLAB !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s*  io_mod/write_pressure_MATLAB !* SYNOPSIS SUBROUTINE write_pressure_MATLAB ( iter ) !* PURPOSE !*   write scaled pressure solution to file in MATLAB format !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> write scaled pressure solution to file in MATLAB format !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- INTEGER , INTENT ( IN ), OPTIONAL :: iter !> iteration for file name !-- local variables -- INTEGER :: prelen !> string pointer CHARACTER ( LEN = slen ) :: fname !> file name INTEGER :: i , j !> loop index REAL ( KIND = iwp ), ALLOCATABLE :: sol_edge (:) !> solution variable for MATLAB output REAL ( KIND = iwp ), ALLOCATABLE :: cnt (:) !> counter REAL ( KIND = iwp ) :: dxyz !> extrapolation variable REAL ( KIND = iwp ) :: nan !> nan variable INTEGER :: indx !> face index Vec :: work !> PETSc work array Vec :: wrk1 , wrk2 !> sub arrays !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IF ( PRESENT ( iter )) THEN prelen = len_trim ( unstdy_prefix ) fname = trim ( unstdy_prefix ) // \"_00000000_press.m\" WRITE ( fname ( prelen + 1 : prelen + 9 ), '(I9)' ) 100000000 + iter fname ( prelen + 1 : prelen + 1 ) = \"_\" ELSE fname = trim ( sol_prefix ) // '.m' END IF !-- extrapolate surface pressures from volumes -- !-- interior faces -- ALLOCATE ( cnt ( num_elm ( dim_cmplx )), sol_edge ( num_elm ( dim_cmplx ))) cnt = 0.d0 ; sol_edge = 0.d0 DO i = 1 , num_elm ( dim_cmplx + 1 ) DO j = 1 , lcl_complex ( dim_cmplx + 1 )% num_bndry ( i ) indx = lcl_complex ( dim_cmplx + 1 )% bndry ( i )% indx ( j ) dxyz = lcl_complex ( dim_cmplx + 1 )% centers ( i , 3 ) - lcl_complex ( dim_cmplx )% centers ( indx , 3 ) sol_edge ( indx ) = sol_edge ( indx ) + & lcl_complex ( dim_cmplx )% prml_volume ( indx ) * ( & lcl_complex ( dim_cmplx + 1 )% dual_sol ( i , 1 ) + dxyz ) cnt ( indx ) = cnt ( indx ) + 1.d0 END DO END DO !-- surface faces -- DO j = 1 , lcl_complex ( dim_cmplx + 1 )% num_surf i = lcl_complex ( dim_cmplx + 1 )% surf_indx ( j , 3 ) indx = lcl_complex ( dim_cmplx + 1 )% surf_indx ( j , 2 ) dxyz = lcl_complex ( dim_cmplx + 1 )% centers ( i , 3 ) - lcl_complex ( dim_cmplx )% centers ( indx , 3 ) sol_edge ( indx ) = sol_edge ( indx ) + & lcl_complex ( dim_cmplx )% prml_volume ( indx ) * ( & lcl_complex ( dim_cmplx + 1 )% dual_sol ( i , 1 ) + dxyz ) cnt ( indx ) = cnt ( indx ) + 1.d0 END DO !-- set crack faces to NaN by zeroing the face count -- DO i = 1 , lcl_complex ( dim_cmplx )% num_crack indx = lcl_complex ( dim_cmplx )% crack_indx ( i ) sol_edge ( indx ) = IEEE_VALUE ( nan , IEEE_QUIET_NAN ) END DO !-- write extrapolated pressure solution to MATLAB file -- CALL VecDuplicate ( sol , work , petsc_ier ) CALL VecGetSubVector ( work , isg ( 1 ), wrk1 , petsc_ier ) CALL VecGetSubVector ( work , isg ( 2 ), wrk2 , petsc_ier ) DO i = 1 , num_elm ( dim_cmplx ) IF ( cnt ( i ) < 1.d0 ) CYCLE CALL VecSetValues ( wrk1 , 1 , lcl_complex ( dim_cmplx )% glb_indx ( i ) - 1 ,& sol_edge ( i ) / cnt ( i ), INSERT_VALUES , petsc_ier ) END DO DO i = 1 , num_elm ( dim_cmplx + 1 ) CALL VecSetValues ( wrk2 , 1 , lcl_complex ( dim_cmplx + 1 )% glb_indx ( i ) - 1 ,& lcl_complex ( dim_cmplx + 1 )% dual_sol ( i , 1 ), INSERT_VALUES , petsc_ier ) END DO DEALLOCATE ( cnt , sol_edge ) CALL VecAssemblyBegin ( wrk1 , petsc_ier ); CALL VecAssemblyEnd ( wrk1 , petsc_ier ) CALL VecAssemblyBegin ( wrk2 , petsc_ier ); CALL VecAssemblyEnd ( wrk2 , petsc_ier ) CALL VecRestoreSubVector ( work , isg ( 1 ), wrk1 , petsc_ier ) CALL VecRestoreSubVector ( work , isg ( 2 ), wrk2 , petsc_ier ) CALL PetscViewerASCIIOpen ( MPI_COMM_WORLD , fname , output_view , petsc_ier ) CALL PetscViewerPushFormat ( output_view , PETSC_VIEWER_ASCII_MATLAB , petsc_ier ) CALL PetscObjectSetName ( work , 'press' , petsc_ier ) CALL VecView ( work , output_view , petsc_ier ) CALL VecDestroy ( work , petsc_ier ) CALL PetscViewerDestroy ( output_view , petsc_ier ) RETURN END SUBROUTINE ! io_mod|write_pressure_MATLAB !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ END MODULE ! io_mod !=============================================================================== !","tags":"","loc":"sourcefile/io_mod.f90.html","title":"io_mod.f90 – ParaGEMS"},{"text":"This file depends on sourcefile~~test_solver_mod.f90~~EfferentGraph sourcefile~test_solver_mod.f90 test_solver_mod.f90 sourcefile~solver_mod.f90 solver_mod.f90 sourcefile~test_solver_mod.f90:->sourcefile~solver_mod.f90: sourcefile~common_mod.f90 common_mod.f90 sourcefile~solver_mod.f90:->sourcefile~common_mod.f90: sourcefile~mpi_mod.f90 mpi_mod.f90 sourcefile~solver_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~mpi_mod.f90:->sourcefile~common_mod.f90: Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules test_solver_mod Source Code test_solver_mod.f90 Source Code ! !=============================================================================== ! /****m*/ /src/modules/common/common_test MODULE test_solver_mod ! ! PURPOSE:    Tests for common_mod module ! ! TESTS: ! ! UPDATES:  created (PDB) :: 2019/08/21 ! !=============================================================================== !----------------------------------------------------------------------------- ! use statements and implicit none !----------------------------------------------------------------------------- USE solver_mod IMPLICIT NONE !=============================================================================== CONTAINS !=============================================================================== ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! /****s*/ /src/common/common_test|test_vars SUBROUTINE test_vars ( cnt ) ! ! PURPOSE: ! ! UPDATES:  created (PDB) :: 2019/08/21 ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! TO DO: ! - global variables? ! - primary element data structure ! - clean up !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !--------------------------------------------------------------------- ! arguments !--------------------------------------------------------------------- INTEGER , INTENT ( INOUT ) :: cnt ! test counter !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !--------------------------------------------------------------------- ! 1. precision !--------------------------------------------------------------------- cnt = cnt + 1 IF ( 1.d0 - 9.999999999999999d-1 > 0.d0 ) THEN WRITE ( * , '(A,I5,A)' ) 'common_test : ' , cnt ,& ' : v/      PASSED : precision accurate to 16 decimal places' ELSE WRITE ( * , '(A,I5,A)' ) 'common_test : ' , cnt ,& ' : XXXXXX  FAILED : precision NOT accurate to 16 decimal places' END IF !--------------------------------------------------------------------- ! 2. global variables ! 3. global mpi ! 4. global io ! 5. global mesh ! 6. primary element ! 7. variable clean up !--------------------------------------------------------------------- END SUBROUTINE ! common_test|test_vars !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! END MODULE ! common_test !=============================================================================== !","tags":"","loc":"sourcefile/test_solver_mod.f90.html","title":"test_solver_mod.f90 – ParaGEMS"},{"text":"Module contains general routines for solvers (linear and nonlinear) This file depends on sourcefile~~solver_mod.f90~~EfferentGraph sourcefile~solver_mod.f90 solver_mod.f90 sourcefile~common_mod.f90 common_mod.f90 sourcefile~solver_mod.f90:->sourcefile~common_mod.f90: sourcefile~mpi_mod.f90 mpi_mod.f90 sourcefile~solver_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~mpi_mod.f90:->sourcefile~common_mod.f90: Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Files dependent on this one sourcefile~~solver_mod.f90~~AfferentGraph sourcefile~solver_mod.f90 solver_mod.f90 sourcefile~darcy.f90 darcy.f90 sourcefile~darcy.f90:->sourcefile~solver_mod.f90: sourcefile~darcy_mod.f90 darcy_mod.f90 sourcefile~darcy.f90:->sourcefile~darcy_mod.f90: sourcefile~partition_mod.f90 partition_mod.f90 sourcefile~darcy.f90:->sourcefile~partition_mod.f90: sourcefile~darcy_1f.f90 darcy_1f.f90 sourcefile~darcy_1f.f90:->sourcefile~solver_mod.f90: sourcefile~darcy_1f.f90:->sourcefile~darcy_mod.f90: sourcefile~darcy_1f.f90:->sourcefile~partition_mod.f90: sourcefile~io_mod.f90 io_mod.f90 sourcefile~io_mod.f90:->sourcefile~solver_mod.f90: sourcefile~darcy_crkp_2f_old.f90 darcy_crkp_2f_old.f90 sourcefile~darcy_crkp_2f_old.f90:->sourcefile~solver_mod.f90: sourcefile~darcy_crkp_2f_old.f90:->sourcefile~darcy_mod.f90: sourcefile~darcy_crkp_2f_old.f90:->sourcefile~partition_mod.f90: sourcefile~darcy_crkp_2f.f90 darcy_crkp_2f.f90 sourcefile~darcy_crkp_2f.f90:->sourcefile~solver_mod.f90: sourcefile~darcy_crkp_2f.f90:->sourcefile~darcy_mod.f90: sourcefile~darcy_crkp_2f.f90:->sourcefile~partition_mod.f90: sourcefile~darcy_mod.f90:->sourcefile~solver_mod.f90: sourcefile~darcy_mod.f90:->sourcefile~io_mod.f90: sourcefile~test_solver_mod.f90 test_solver_mod.f90 sourcefile~test_solver_mod.f90:->sourcefile~solver_mod.f90: sourcefile~darcy_crkp_1f.f90 darcy_crkp_1f.f90 sourcefile~darcy_crkp_1f.f90:->sourcefile~solver_mod.f90: sourcefile~darcy_crkp_1f.f90:->sourcefile~darcy_mod.f90: sourcefile~darcy_crkp_1f.f90:->sourcefile~partition_mod.f90: sourcefile~darcy_2f.f90 darcy_2f.f90 sourcefile~darcy_2f.f90:->sourcefile~solver_mod.f90: sourcefile~darcy_2f.f90:->sourcefile~darcy_mod.f90: sourcefile~darcy_2f.f90:->sourcefile~partition_mod.f90: sourcefile~test_darcy_mod.f90 test_darcy_mod.f90 sourcefile~test_darcy_mod.f90:->sourcefile~darcy_mod.f90: sourcefile~partition_mod.f90:->sourcefile~io_mod.f90: sourcefile~test_io_mod.f90 test_io_mod.f90 sourcefile~test_io_mod.f90:->sourcefile~io_mod.f90: sourcefile~test_partition_mod.f90 test_partition_mod.f90 sourcefile~test_partition_mod.f90:->sourcefile~partition_mod.f90: Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules solver_mod Source Code solver_mod.f90 Source Code ! !=============================================================================== !-- Solver Module !> Module contains general routines for solvers (linear and nonlinear) !=============================================================================== !/****/h* modules|solvers/solver_mod !* SYNOPSIS MODULE solver_mod !* PURPOSE !*   Module contains general routines for solvers (linear and nonlinear) !* INCLUDES !*   Name                    Purpose !*   common_mod              variable definitions !*   mpi_mod                 ??? !*   ieee_arithmeti !*   iso_fortran_env !*   petsc.h !*   solver_vars.inc !* CONTAINS !*   Subroutine              Purpose !* !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !*   2019/11/21: KSP (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Module contains general routines for solvers (linear and nonlinear) !=============================================================================== !-- use statements and implicit none -- USE common_mod USE mpi_mod USE , INTRINSIC :: ieee_arithmetic , only : IEEE_Value , IEEE_QUIET_NAN USE , INTRINSIC :: iso_fortran_env , only : real32 IMPLICIT NONE !-- include solver variables -- #include <petsc/finclude/petsc.h> #include \"solver_vars.inc\" !=============================================================================== CONTAINS !=============================================================================== ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* solver_mod/set_defaults_solver !* SYNOPSIS SUBROUTINE set_defaults_solver ( solver ) !* PURPOSE !*   Set default values for solver !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !*   2019/11/21: KSP (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Set default values for solver !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- local variables -- CHARACTER ( LEN =* ) :: solver !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SELECT CASE ( solver ) CASE ( 'KSP' ) pc_type = PCASM CASE DEFAULT WRITE ( log_unit , '(A,A)' ) '***Error: invalid solver: name: ' , solver CALL end_mpi () END SELECT RETURN END SUBROUTINE ! solver_mod|set_defaults_solver !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* solver_mod/check_param_solver !* SYNOPSIS SUBROUTINE check_param_solver () !* PURPOSE !*   Check values set for solver !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Check values set for solver !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- local variables -- LOGICAL :: error = . FALSE . !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- Nothing checked (yet) -- IF ( error ) CALL end_mpi () RETURN END SUBROUTINE ! solver_mod|check_param_solver !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* solver_mod/start_petsc !* SYNOPSIS SUBROUTINE start_petsc () !* PURPOSE !*   Initialise PETSc library !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Initialise PETSc library !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ CALL syncwrite_log ( '> start_petsc()' ) !-- initialise PETSc and check for errors -- CALL PetscInitialize ( PETSC_NULL_CHARACTER , petsc_ier ) IF ( petsc_ier /= 0 ) THEN WRITE ( log_unit , '(A,A,I5)' ) '***Error: failed to start PETSc: errcode = ' , petsc_ier CALL end_mpi () END IF RETURN END SUBROUTINE ! solver_mod|start_petsc !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* solver_mod/end_petsc !* SYNOPSIS SUBROUTINE end_petsc () !* PURPOSE !*   Finalise PETSc library !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Finalise PETSc library !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- finalise PETSc -- CALL PetscFinalize ( petsc_ier ) RETURN END SUBROUTINE ! solver_mod|end_petsc !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* solver_mod/solve_KSP !* SYNOPSIS SUBROUTINE solve_KSP () !* PURPOSE !*   Solve linear system using PETSc KSP !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Solve linear system using PETSc KSP !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- local variables -- CHARACTER ( LEN = slen ) :: msg !> message sting for log file INTEGER :: iters !> number of iterations REAL ( KIND = iwp ) :: rnorm !> final residual norm !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ CALL syncwrite_log ( '> solve_KSP()' ) curnt_time = MPI_WTIME () !-- create KSP solver -- CALL KSPCreate ( MPI_COMM_WORLD , ksp_id , petsc_ier ) CALL KSPSetType ( ksp_id , ksp_type , petsc_ier ) CALL KSPSetFromOptions ( ksp_id , petsc_ier ) CALL KSPSetTolerances ( ksp_id , rel_tol , abs_tol , div_tol , max_iter , petsc_ier ) IF ( nz_init ) CALL KSPSetInitialGuessNonzero ( ksp_id , PETSC_TRUE , petsc_ier ) !-- setup solver monitor -- IF ( solver_monitor ) THEN CALL PetscViewerAndFormatCreate ( PETSC_VIEWER_STDOUT_WORLD , PETSC_VIEWER_DEFAULT , output_vfrmt , petsc_ier ) CALL KSPMonitorSet ( ksp_id , KSPMonitorDefault , output_vfrmt , PetscViewerAndFormatDestroy , petsc_ier ) END IF !-- set LHS -- CALL KSPSetOperators ( ksp_id , A , A , petsc_ier ) !-- set preconditioner -- CALL KSPGetPC ( ksp_id , pc_id , petsc_ier ) CALL PCFactorSetMatOrderingType ( pc_id , pc_reorder , petsc_ier ) CALL PCSetType ( pc_id , pc_type , petsc_ier ) !-- solve linear system and output details -- CALL KSPSetUp ( ksp_id , petsc_ier ) CALL KSPSolve ( ksp_id , b , sol , petsc_ier ) CALL KSPView ( ksp_id , PETSC_VIEWER_STDOUT_WORLD , petsc_ier ) !-- write simulation details to log file -- CALL KSPGetIterationNumber ( ksp_id , iters , petsc_ier ) WRITE ( msg , * ) '  - solution iterations: ' , iters ; CALL syncwrite_log ( msg ) CALL KSPGetResidualNorm ( ksp_id , rnorm , petsc_ier ) WRITE ( msg , * ) '  - final residual norm: ' , rnorm ; CALL syncwrite_log ( msg ) RETURN END SUBROUTINE ! solver_mod|end_petsc !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* solver_mod/solve_KSP2 !* SYNOPSIS SUBROUTINE solve_KSP2 () !* PURPOSE !*   Solve linear system using PETSc KSP !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Solve linear system using PETSc KSP !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- local variables -- CHARACTER ( LEN = slen ) :: msg !> message sting for log file INTEGER :: iters , m , n !> number of iterations REAL ( KIND = iwp ) :: rnorm !> final residual norm CHARACTER ( LEN = slen ) :: fname !> file name KSPConvergedReason :: reason !> KSP :: subksp_id ( 2 ), sub2ksp_id ( 2 ) PC :: subpc_id ( 2 ), sub2pc_id ( 2 ) !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ CALL syncwrite_log ( '> solve_KSP()' ) curnt_time = MPI_WTIME () !-- create KSP solver -- CALL KSPCreate ( MPI_COMM_WORLD , ksp_id , petsc_ier ) CALL KSPSetType ( ksp_id , ksp_type , petsc_ier ) CALL KSPSetFromOptions ( ksp_id , petsc_ier ) CALL KSPSetTolerances ( ksp_id , rel_tol , abs_tol , div_tol , max_iter , petsc_ier ) IF ( nz_init ) CALL KSPSetInitialGuessNonzero ( ksp_id , PETSC_TRUE , petsc_ier ) !-- setup solver monitor -- IF ( solver_monitor ) THEN CALL PetscViewerAndFormatCreate ( PETSC_VIEWER_STDOUT_WORLD , PETSC_VIEWER_DEFAULT , output_vfrmt , petsc_ier ) CALL KSPMonitorSet ( ksp_id , KSPMonitorDefault , output_vfrmt , PetscViewerAndFormatDestroy , petsc_ier ) END IF !-- set LHS -- CALL KSPSetOperators ( ksp_id , A , A , petsc_ier ) !-- set preconditioner -- CALL KSPGetPC ( ksp_id , pc_id , petsc_ier ) CALL PCSetType ( pc_id , PCFIELDSPLIT , petsc_ier ) CALL PCFieldSplitSetIS ( pc_id , \"fcs\" , isg ( 1 ), petsc_ier ) CALL PCFieldSplitSetIS ( pc_id , \"vls\" , isg ( 2 ), petsc_ier ) CALL PCFieldSplitSetType ( pc_id , PC_COMPOSITE_SCHUR , petsc_ier ) ! - PC_COMPOSITE_ADDITIVE - Jacobi ! - PC_COMPOSITE_MULTIPLICATIVE (default) - Gauss-Seidel ! - PC_COMPOSITE_SYMMETRIC_MULTIPLICATIVE - Symmetric Gauss-Seidel ! - PC_COMPOSITE_SPECIAL - ??? ! - PC_COMPOSITE_SCHUR - schur CALL PCFieldSplitSetSchurPre ( pc_id , PC_FIELDSPLIT_SCHUR_PRE_SELFP , PETSC_NULL_Mat , petsc_ier ) ! - custom - user ! - fuse a11 - a11 (a11=0) ! - full schur compliment - full ! - symbolic schur compliment - self ! - approximate schur compliment - selfp !!CALL MatSchurComplementSetAinvType(Mat S,MatSchurComplementAinvType ainvtype,petsc_ier) ! - for selfp ! -  MAT_SCHUR_COMPLEMENT_AINV_DIAG ! -  MAT_SCHUR_COMPLEMENT_AINV_LUMP ! -  MAT_SCHUR_COMPLEMENT_AINV_BLOCK_DIAG CALL PCFieldSplitSetSchurFactType ( pc_id , PC_FIELDSPLIT_SCHUR_FACT_FULL , petsc_ier ) ! - diag,lower,upper,full !!CALL PCFieldSplitSetSchurScale(pc_id,1.d0,petsc_ier) ! - default is -1.0 (for diag) ! CALL PCSetUseAmat(pc_id,PETSC_TRUE,petsc_ier) ! CALL PCFieldSplitSetDiagUseAmat(pc_id,PETSC_TRUE,petsc_ier) ! CALL PCFieldSplitSetOffDiagUseAmat(pc_id,PETSC_TRUE,petsc_ier) CALL PCSetUp ( pc_id , petsc_ier ) CALL PCFieldSplitGetSubKSP ( pc_id , n , subksp_id , petsc_ier ) CALL KSPSetType ( subksp_id ( 2 ), KSPGMRES , petsc_ier ); CALL KSPGetPC ( subksp_id ( 2 ), subpc_id ( 2 ), petsc_ier ) CALL PCSetType ( subpc_id ( 2 ), PCHYPRE , petsc_ier ) !CALL PCHYPRESetType(subpc_id(2),\"euclid\",petsc_ier) CALL PCHYPRESetType ( subpc_id ( 2 ), \"parasails\" , petsc_ier ) !CALL KSPSetTolerances(subksp_id(2),1.d-4,1.d-4,PETSC_DEFAULT_REAL,1000,petsc_ier) !CALL KSPMonitorSet(subksp_id(2),KSPMonitorDefault,output_vfrmt,PetscViewerAndFormatDestroy,petsc_ier) !-- solve linear system and output details -- CALL KSPSetUp ( ksp_id , petsc_ier ) CALL KSPSolve ( ksp_id , b , sol , petsc_ier ) CALL KSPView ( ksp_id , PETSC_VIEWER_STDOUT_WORLD , petsc_ier ) !-- write simulation details to log file -- CALL KSPGetIterationNumber ( ksp_id , iters , petsc_ier ) WRITE ( msg , * ) '  - solution iterations: ' , iters ; CALL syncwrite_log ( msg ) CALL KSPGetTotalIterations ( subksp_id ( 2 ), iters , petsc_ier ) WRITE ( msg , * ) '  - solution sub iters.: ' , iters ; CALL syncwrite_log ( msg ) CALL KSPGetResidualNorm ( ksp_id , rnorm , petsc_ier ) WRITE ( msg , * ) '  - final residual norm: ' , rnorm ; CALL syncwrite_log ( msg ) CALL KSPGetConvergedReason ( ksp_id , reason , petsc_ier ) WRITE ( msg , * ) '  - convergence reason : ' , reason ; CALL syncwrite_log ( msg ) CALL KSPDestroy ( ksp_id , petsc_ier ) RETURN END SUBROUTINE ! solver_mod|solve_KSP2 !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* solver_mod/extract_sol_KSP !* SYNOPSIS SUBROUTINE extract_sol_KSP () !* PURPOSE !*   extract local solution values from distributed PETSc vector !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> extract local solution values from distributed PETSc vector !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- local variables -- INTEGER :: i !> loop index Vec :: lcl_sol !> local solution vector VecScatter :: scatter !> scatter variable IS :: from , to !> global and local vecor indices !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- get local solution values - primal faces -- CALL VecCreateSeq ( PETSC_COMM_SELF , num_elm ( dim_cmplx ), lcl_sol , petsc_ier ) CALL ISCreateGeneral ( MPI_COMM_WORLD , num_elm ( dim_cmplx ),& lcl_complex ( dim_cmplx )% glb_indx - 1 , PETSC_COPY_VALUES , from , petsc_ier ) CALL ISCreateGeneral ( PETSC_COMM_SELF , num_elm ( dim_cmplx ),& ( / ( i , i = 0 , num_elm ( dim_cmplx ) - 1 ) / ), PETSC_COPY_VALUES , to , petsc_ier ) CALL VecScatterCreate ( sol , from , lcl_sol , to , scatter , petsc_ier ) CALL VecScatterBegin ( scatter , sol , lcl_sol , INSERT_VALUES , SCATTER_FORWARD , petsc_ier ) CALL VecScatterEnd ( scatter , sol , lcl_sol , INSERT_VALUES , SCATTER_FORWARD , petsc_ier ) CALL VecGetValues ( lcl_sol , num_elm ( dim_cmplx ),( / ( i , i = 0 , num_elm ( dim_cmplx ) - 1 ) / ),& lcl_complex ( dim_cmplx )% prml_sol (:, 1 ), petsc_ier ) CALL ISDestroy ( from , petsc_ier ); CALL ISDestroy ( to , petsc_ier ) CALL VecScatterDestroy ( scatter , petsc_ier ) CALL VecDestroy ( lcl_sol , petsc_ier ) !-- get local solution values - dual vertices -- CALL VecCreateSeq ( PETSC_COMM_SELF , num_elm ( dim_cmplx + 1 ), lcl_sol , petsc_ier ) CALL ISCreateGeneral ( MPI_COMM_WORLD , num_elm ( dim_cmplx + 1 ), glb_num_elm ( dim_cmplx ) + & lcl_complex ( dim_cmplx + 1 )% glb_indx - 1 , PETSC_COPY_VALUES , from , petsc_ier ) CALL ISCreateGeneral ( PETSC_COMM_SELF , num_elm ( dim_cmplx + 1 ),& ( / ( i , i = 0 , num_elm ( dim_cmplx + 1 ) - 1 ) / ), PETSC_COPY_VALUES , to , petsc_ier ) CALL VecScatterCreate ( sol , from , lcl_sol , to , scatter , petsc_ier ) CALL VecScatterBegin ( scatter , sol , lcl_sol , INSERT_VALUES , SCATTER_FORWARD , petsc_ier ) CALL VecScatterEnd ( scatter , sol , lcl_sol , INSERT_VALUES , SCATTER_FORWARD , petsc_ier ) CALL VecGetValues ( lcl_sol , num_elm ( dim_cmplx + 1 ),( / ( i , i = 0 , num_elm ( dim_cmplx + 1 ) - 1 ) / ),& lcl_complex ( dim_cmplx + 1 )% dual_sol (:, 1 ), petsc_ier ) CALL ISDestroy ( from , petsc_ier ); CALL ISDestroy ( to , petsc_ier ) CALL VecScatterDestroy ( scatter , petsc_ier ) CALL VecDestroy ( lcl_sol , petsc_ier ) !-- log time to solve system -- CALL syncwrite_log_time () RETURN END SUBROUTINE ! solver_mod|extract_sol_KSP !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* solver_mod/extract_sol_KSP2 !* SYNOPSIS SUBROUTINE extract_sol_KSP2 () !* PURPOSE !*   extract local solution values from distributed PETSc vector !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> extract local solution values from distributed PETSc vector !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- local variables -- INTEGER :: i !> loop index Vec :: lcl_sol !> local solution vector VecScatter :: scatter !> scatter variable IS :: from , to !> global and local vecor indices Vec :: sol1 , sol2 !> CHARACTER ( LEN = slen ) :: fname !> file name !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ CALL VecGetSubVector ( sol , isg ( 1 ), sol1 , petsc_ier ); CALL VecGetSubVector ( sol , isg ( 2 ), sol2 , petsc_ier ) !-- get local solution values - primal faces -- CALL VecCreateSeq ( PETSC_COMM_SELF , num_elm ( dim_cmplx ), lcl_sol , petsc_ier ) CALL ISCreateGeneral ( MPI_COMM_WORLD , num_elm ( dim_cmplx ),& lcl_complex ( dim_cmplx )% glb_indx - 1 , PETSC_COPY_VALUES , from , petsc_ier ) CALL ISCreateGeneral ( PETSC_COMM_SELF , num_elm ( dim_cmplx ),& ( / ( i , i = 0 , num_elm ( dim_cmplx ) - 1 ) / ), PETSC_COPY_VALUES , to , petsc_ier ) CALL VecScatterCreate ( sol1 , from , lcl_sol , to , scatter , petsc_ier ) CALL VecScatterBegin ( scatter , sol1 , lcl_sol , INSERT_VALUES , SCATTER_FORWARD , petsc_ier ) CALL VecScatterEnd ( scatter , sol1 , lcl_sol , INSERT_VALUES , SCATTER_FORWARD , petsc_ier ) CALL VecGetValues ( lcl_sol , num_elm ( dim_cmplx ),( / ( i , i = 0 , num_elm ( dim_cmplx ) - 1 ) / ),& lcl_complex ( dim_cmplx )% prml_sol (:, 1 ), petsc_ier ) CALL ISDestroy ( from , petsc_ier ); CALL ISDestroy ( to , petsc_ier ) CALL VecScatterDestroy ( scatter , petsc_ier ) CALL VecDestroy ( lcl_sol , petsc_ier ) !-- get local solution values - dual vertices -- CALL VecCreateSeq ( PETSC_COMM_SELF , num_elm ( dim_cmplx + 1 ), lcl_sol , petsc_ier ) CALL ISCreateGeneral ( MPI_COMM_WORLD , num_elm ( dim_cmplx + 1 ),& lcl_complex ( dim_cmplx + 1 )% glb_indx - 1 , PETSC_COPY_VALUES , from , petsc_ier ) CALL ISCreateGeneral ( PETSC_COMM_SELF , num_elm ( dim_cmplx + 1 ),& ( / ( i , i = 0 , num_elm ( dim_cmplx + 1 ) - 1 ) / ), PETSC_COPY_VALUES , to , petsc_ier ) CALL VecScatterCreate ( sol2 , from , lcl_sol , to , scatter , petsc_ier ) CALL VecScatterBegin ( scatter , sol2 , lcl_sol , INSERT_VALUES , SCATTER_FORWARD , petsc_ier ) CALL VecScatterEnd ( scatter , sol2 , lcl_sol , INSERT_VALUES , SCATTER_FORWARD , petsc_ier ) CALL VecGetValues ( lcl_sol , num_elm ( dim_cmplx + 1 ),( / ( i , i = 0 , num_elm ( dim_cmplx + 1 ) - 1 ) / ),& lcl_complex ( dim_cmplx + 1 )% dual_sol (:, 1 ), petsc_ier ) CALL ISDestroy ( from , petsc_ier ); CALL ISDestroy ( to , petsc_ier ) CALL VecScatterDestroy ( scatter , petsc_ier ) CALL VecDestroy ( lcl_sol , petsc_ier ) !-- log time to solve system -- CALL syncwrite_log_time () RETURN END SUBROUTINE ! solver_mod|extract_sol_KSP2 !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! END MODULE ! solver_mod !=============================================================================== !","tags":"","loc":"sourcefile/solver_mod.f90.html","title":"solver_mod.f90 – ParaGEMS"},{"text":"This file depends on sourcefile~~test_math_mod.f90~~EfferentGraph sourcefile~test_math_mod.f90 test_math_mod.f90 sourcefile~math_mod.f90 math_mod.f90 sourcefile~test_math_mod.f90:->sourcefile~math_mod.f90: sourcefile~common_mod.f90 common_mod.f90 sourcefile~math_mod.f90:->sourcefile~common_mod.f90: sourcefile~mpi_mod.f90 mpi_mod.f90 sourcefile~math_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~mpi_mod.f90:->sourcefile~common_mod.f90: Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules test_math_mod Source Code test_math_mod.f90 Source Code ! !=============================================================================== ! /****m*/ /src/modules/common/common_test MODULE test_math_mod ! ! PURPOSE:    Tests for common_mod module ! ! TESTS: ! ! UPDATES:  created (PDB) :: 2019/08/21 ! !=============================================================================== !----------------------------------------------------------------------------- ! use statements and implicit none !----------------------------------------------------------------------------- USE math_mod IMPLICIT NONE !=============================================================================== CONTAINS !=============================================================================== ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! /****s*/ /src/common/common_test|test_vars SUBROUTINE test_vars ( cnt ) ! ! PURPOSE: ! ! UPDATES:  created (PDB) :: 2019/08/21 ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! TO DO: ! - global variables? ! - primary element data structure ! - clean up !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !--------------------------------------------------------------------- ! arguments !--------------------------------------------------------------------- INTEGER , INTENT ( INOUT ) :: cnt ! test counter !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !--------------------------------------------------------------------- ! 1. precision !--------------------------------------------------------------------- cnt = cnt + 1 IF ( 1.d0 - 9.999999999999999d-1 > 0.d0 ) THEN WRITE ( * , '(A,I5,A)' ) 'common_test : ' , cnt ,& ' : v/      PASSED : precision accurate to 16 decimal places' ELSE WRITE ( * , '(A,I5,A)' ) 'common_test : ' , cnt ,& ' : XXXXXX  FAILED : precision NOT accurate to 16 decimal places' END IF !--------------------------------------------------------------------- ! 2. global variables ! 3. global mpi ! 4. global io ! 5. global mesh ! 6. primary element ! 7. variable clean up !--------------------------------------------------------------------- END SUBROUTINE ! common_test|test_vars !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! END MODULE ! common_test !=============================================================================== !","tags":"","loc":"sourcefile/test_math_mod.f90.html","title":"test_math_mod.f90 – ParaGEMS"},{"text":"Module for common math functions and routines This file depends on sourcefile~~math_mod.f90~~EfferentGraph sourcefile~math_mod.f90 math_mod.f90 sourcefile~common_mod.f90 common_mod.f90 sourcefile~math_mod.f90:->sourcefile~common_mod.f90: sourcefile~mpi_mod.f90 mpi_mod.f90 sourcefile~math_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~mpi_mod.f90:->sourcefile~common_mod.f90: Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Files dependent on this one sourcefile~~math_mod.f90~~AfferentGraph sourcefile~math_mod.f90 math_mod.f90 sourcefile~test_math_mod.f90 test_math_mod.f90 sourcefile~test_math_mod.f90:->sourcefile~math_mod.f90: sourcefile~dec_mod.f90 dec_mod.f90 sourcefile~dec_mod.f90:->sourcefile~math_mod.f90: sourcefile~partition_mod.f90 partition_mod.f90 sourcefile~partition_mod.f90:->sourcefile~math_mod.f90: sourcefile~partition_mod.f90:->sourcefile~dec_mod.f90: sourcefile~io_mod.f90 io_mod.f90 sourcefile~partition_mod.f90:->sourcefile~io_mod.f90: sourcefile~darcy.f90 darcy.f90 sourcefile~darcy.f90:->sourcefile~partition_mod.f90: sourcefile~darcy_mod.f90 darcy_mod.f90 sourcefile~darcy.f90:->sourcefile~darcy_mod.f90: sourcefile~darcy_1f.f90 darcy_1f.f90 sourcefile~darcy_1f.f90:->sourcefile~partition_mod.f90: sourcefile~darcy_1f.f90:->sourcefile~darcy_mod.f90: sourcefile~io_mod.f90:->sourcefile~dec_mod.f90: sourcefile~darcy_crkp_2f_old.f90 darcy_crkp_2f_old.f90 sourcefile~darcy_crkp_2f_old.f90:->sourcefile~partition_mod.f90: sourcefile~darcy_crkp_2f_old.f90:->sourcefile~darcy_mod.f90: sourcefile~darcy_crkp_2f.f90 darcy_crkp_2f.f90 sourcefile~darcy_crkp_2f.f90:->sourcefile~partition_mod.f90: sourcefile~darcy_crkp_2f.f90:->sourcefile~darcy_mod.f90: sourcefile~test_partition_mod.f90 test_partition_mod.f90 sourcefile~test_partition_mod.f90:->sourcefile~partition_mod.f90: sourcefile~darcy_2f.f90 darcy_2f.f90 sourcefile~darcy_2f.f90:->sourcefile~partition_mod.f90: sourcefile~darcy_2f.f90:->sourcefile~darcy_mod.f90: sourcefile~darcy_crkp_1f.f90 darcy_crkp_1f.f90 sourcefile~darcy_crkp_1f.f90:->sourcefile~partition_mod.f90: sourcefile~darcy_crkp_1f.f90:->sourcefile~darcy_mod.f90: sourcefile~test_dec_mod.f90 test_dec_mod.f90 sourcefile~test_dec_mod.f90:->sourcefile~dec_mod.f90: sourcefile~darcy_mod.f90:->sourcefile~io_mod.f90: sourcefile~test_io_mod.f90 test_io_mod.f90 sourcefile~test_io_mod.f90:->sourcefile~io_mod.f90: sourcefile~test_darcy_mod.f90 test_darcy_mod.f90 sourcefile~test_darcy_mod.f90:->sourcefile~darcy_mod.f90: Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules math_mod Source Code math_mod.f90 Source Code ! !=============================================================================== !-- Math Module !> Module for common math functions and routines !=============================================================================== !/****/h* modules|math/math_mod !* SYNOPSIS MODULE math_mod !* PURPOSE !*   Module for common math functions and routines !* INCLUDES !*   Name                    Purpose !*   common_mod              variable definitions !*   mpi_mod                 ??? !* CONTAINS !*   Subroutine              Purpose !*   int_merge_sort_rows()   merge sort rows of integer array in ascending order !*   int_merge_rows()        merge rows of integer array in ascending order !*   int_insertion_sort()    insertion sort for integer array !*   any_element_in_list()   test if any element of one array is in another !*   num_element_in_list()   determine how many elements of one array are in another !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Module for common math functions and routines !=============================================================================== !-- use statements and implicit none -- USE common_mod USE mpi_mod IMPLICIT NONE !=============================================================================== CONTAINS !=============================================================================== ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s*  math_mod/int_merge_sort_rows !* SYNOPSIS RECURSIVE SUBROUTINE int_merge_sort_rows ( A , length , irow , frow , work ) !* PURPOSE !*   Merge sort rows of integer array in ascending order !* ASSUMPTION !*   Rows are already sorted in ascending order !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !* NOTES !*   ADAPTED FROM: https://rosettacode.org/wiki/Sorting_algorithms/Merge_sort#Fortran !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Merge sort rows of integer array in ascending order !> Assumption: rows are already sorted in ascending order !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- INTEGER , INTENT ( INOUT ) :: A (:,:) !> INTEGER :: length !> INTEGER , INTENT ( IN ) :: irow , frow !> INTEGER , INTENT ( INOUT ) :: work (:,:) !> !-- local variables -- INTEGER :: half !> INTEGER :: ir , i !> !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- sort directly if there are two elements -- !-- NOTE: if there is only one row, then it is already sorted! -- IF ( length == 2 ) THEN !-- if out of order, then swap DO ir = irow , frow IF ( A ( 1 , ir ) > A ( 2 , ir )) THEN work ( 1 ,:) = A ( 1 ,:) A ( 1 ,:) = A ( 2 ,:) A ( 2 ,:) = work ( 1 ,:) EXIT ELSEIF ( A ( 1 , ir ) < A ( 2 , ir )) THEN EXIT END IF END DO !-- for more than two rows, split the array and call merge sort on each half -- ELSEIF ( length > 2 ) THEN !-- compute array midpoint -- half = ( length + 1 ) / 2 !-- merge sort half arrays -- CALL int_merge_sort_rows ( A (: half ,:), half , irow , frow , work ) CALL int_merge_sort_rows ( A ( half + 1 :,:), length - half , irow , frow , work ) !-- merge sorted half arrays -- DO ir = irow , frow IF ( A ( half , ir ) > A ( half + 1 , ir )) THEN work ( 1 : half ,:) = A ( 1 : half ,:) CALL int_merge_rows ( work ( 1 : half ,:), A ( half + 1 : length ,:), A , irow , frow ) EXIT ELSEIF ( A ( half , ir ) < A ( half + 1 , ir )) THEN EXIT END IF END DO END IF END SUBROUTINE !  math_mod/int_merge_sort_rows !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s*  math_mod/int_merge_rows !* SYNOPSIS SUBROUTINE int_merge_rows ( A , B , C , irow , frow ) !* PURPOSE !*   Merge rows of an array in ascending order !* ASSUMPTION !*   Rows are already sorted in ascending order !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !* NOTES !*   ADAPTED FROM: https://rosettacode.org/wiki/Sorting_algorithms/Merge_sort#Fortran !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Merge rows of an array in ascending order !> Assumption: rows are already sorted in ascending order !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- INTEGER , INTENT ( IN ) :: A (:,:), B (:,:) !> INTEGER , INTENT ( INOUT ) :: C (:,:) !> INTEGER , INTENT ( IN ) :: irow , frow !> !-- local variables -- LOGICAL :: is_le !> INTEGER :: i , j , k !> INTEGER :: ir !> !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- dimension check -- IF ( SIZE ( A , DIM = 1 ) + SIZE ( B , DIM = 1 ) > SIZE ( C , DIM = 1 )) THEN WRITE ( * , '(A)' ) 'Error in merge: Arrays sizes are not compatible' CALL end_mpi () END IF !-- loop for all elements of merged array -- i = 1 ; j = 1 DO k = 1 , SIZE ( C , DIM = 1 ) IF ( i <= SIZE ( A , DIM = 1 ) . and . j <= SIZE ( B , DIM = 1 )) THEN is_le = . TRUE . DO ir = irow , frow IF ( A ( i , ir ) < B ( j , ir )) THEN EXIT ELSEIF ( A ( i , ir ) > B ( j , ir )) THEN is_le = . FALSE . EXIT END IF END DO IF ( is_le ) THEN C ( k ,:) = A ( i ,:) i = i + 1 ELSE C ( k ,:) = B ( j ,:) j = j + 1 END IF ELSEIF ( i <= SIZE ( A , DIM = 1 )) THEN C ( k ,:) = A ( i ,:) i = i + 1 ELSEIF ( j <= SIZE ( B , DIM = 1 )) THEN C ( k ,:) = B ( j ,:) j = j + 1 END IF END DO END SUBROUTINE !  math_mod/int_merge_rows !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s*  math_mod/int_insertion_sort !* SYNOPSIS SUBROUTINE int_insertion_sort ( A ) !* PURPOSE !*   Insertion sort of integer array !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/29: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/29 !> Insertion sort of integer array !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- INTEGER , INTENT ( INOUT ) :: A (:) !> !-- local variables -- INTEGER :: i , j !> INTEGER :: work !> !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ DO i = 2 , SIZE ( A ) work = A ( i ) j = i - 1 DO WHILE ( j >= 1 ) IF ( A ( j ) <= work ) EXIT A ( j + 1 ) = A ( j ) j = j - 1 END DO A ( j + 1 ) = work END DO RETURN END SUBROUTINE !  math_mod/int_insertion_sort !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/f*  math_mod/any_element_in_list !* SYNOPSIS FUNCTION any_element_in_list ( elm , node_list ) !* PURPOSE !*   Test if any element of one array is in another !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/29: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/29 !> Test if any element of one array is in another !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- INTEGER , INTENT ( IN ) :: elm (:) !> INTEGER , INTENT ( IN ) :: node_list (:) !> LOGICAL :: any_element_in_list !> !-- local variables -- INTEGER :: in , jn , end_array !> !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- assume it is true -- any_element_in_list = . TRUE . !-- search second array for match -- end_array = SIZE ( node_list , DIM = 1 ) DO in = 1 , SIZE ( elm , DIM = 1 ) DO jn = 1 , CEILING ( end_array / 100 0.d0 ) - 1 IF ( ANY ( elm ( in ) == node_list ( jn * 1000 - 999 : jn * 1000 ))) RETURN END DO IF ( ANY ( elm ( in ) == node_list ( jn * 1000 - 999 : end_array ))) RETURN END DO !-- it is false -- any_element_in_list = . FALSE . RETURN END FUNCTION !  math_mod/any_element_in_list !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/f*  math_mod/num_element_in_list !* SYNOPSIS FUNCTION num_element_in_list ( elm , node_list ) !* PURPOSE !*   Count number of elements of one array is in another !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/29: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/29 !> Test if any element of one array is in another !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- INTEGER , INTENT ( IN ) :: elm (:) !> INTEGER , INTENT ( IN ) :: node_list (:) !> INTEGER :: num_element_in_list !> !-- local variables -- INTEGER :: in !> !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- search second array for matches -- num_element_in_list = 0 DO in = 1 , SIZE ( elm , DIM = 1 ) IF ( ANY ( elm ( in ) == node_list )) & num_element_in_list = num_element_in_list + 1 END DO RETURN END FUNCTION !  math_mod/num_element_in_list !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/f*  math_mod/index_in_list !* SYNOPSIS FUNCTION index_in_list ( elm , list , init_guess ) !* PURPOSE !*   Index of element in array !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/29: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/29 !> Test if any element of one array is in another !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- INTEGER , INTENT ( IN ) :: elm , list (:) !> INTEGER , INTENT ( IN ) :: init_guess !> INTEGER :: index_in_list !> !-- local variables -- INTEGER :: in !> !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- search second array for matches -- DO in = init_guess , SIZE ( list , DIM = 1 ) IF ( elm == list ( in )) THEN index_in_list = in RETURN END IF END DO DO in = 1 , init_guess - 1 IF ( elm == list ( in )) THEN index_in_list = in RETURN END IF END DO index_in_list =- 1 RETURN END FUNCTION !  math_mod/index_in_list !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/f*  math_mod/determinant !* SYNOPSIS RECURSIVE FUNCTION determinant ( A , n ) RESULT ( accumulation ) !* PURPOSE !*   Compute determinant !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/29: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !* NOTES !*   Adapted from: !*   http://fortranwiki.org/fortran/show/Matrix+inversion !*   https://rosettacode.org/wiki/Determinant_and_permanent#Fortran !******/ !> author: Pieter Boom !> date: 2019/08/29 !> Compute determinant !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- REAL ( KIND = iwp ), INTENT ( IN ) :: A ( n , n ) !> INTEGER , INTENT ( IN ) :: n !> !-- local variables -- INTEGER :: i , sgn !> REAL ( KIND = iwp ) :: B ( n - 1 , n - 1 ) !> REAL ( KIND = iwp ) :: accumulation !> !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- search second array for matches -- SELECT CASE ( n ) CASE ( 1 ) accumulation = A ( 1 , 1 ) CASE ( 2 ) accumulation = ( A ( 1 , 1 ) * A ( 2 , 2 ) - A ( 1 , 2 ) * A ( 2 , 1 )) CASE ( 3 ) accumulation = ( A ( 1 , 1 ) * ( A ( 2 , 2 ) * A ( 3 , 3 ) - A ( 2 , 3 ) * A ( 3 , 2 )) & - A ( 1 , 2 ) * ( A ( 2 , 1 ) * A ( 3 , 3 ) - A ( 2 , 3 ) * A ( 3 , 1 )) & + A ( 1 , 3 ) * ( A ( 2 , 1 ) * A ( 3 , 2 ) - A ( 2 , 2 ) * A ( 3 , 1 ))) CASE DEFAULT ! IF (n==1) THEN !   accumulation = a(1,1) ! ELSE accumulation = 0 ; sgn = 1 DO i = 1 , n B (:, :( i - 1 )) = A ( 2 :, : i - 1 ); B (:, i :) = A ( 2 :, i + 1 :) accumulation = accumulation + sgn * A ( 1 , i ) * determinant ( B , n - 1 ) sgn = - sgn END DO END SELECT RETURN END FUNCTION !  math_mod/determinant !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/f*  math_mod/matinv2 !* SYNOPSIS FUNCTION matinv2 ( A ) RESULT ( B ) !* PURPOSE !*   Performs a direct calculation of the inverse of a 2x2 matrix. !* INPUTS !*   Name                    Description !*   elem !*   orientation !* OUTPUTS !*   Name                    Description !*   elem !*   orientation !* SIDE EFFECTS !*   - the nodal indices of elm are sorted numerically in ascending order !*   - orientation contains the ±1 orientation of the element !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/29: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !* NOTES !*   Adapted from: http://fortranwiki.org/fortran/show/Matrix+inversion !******/ !> author: Pieter Boom !> date: 2019/08/29 !> Performs a direct calculation of the inverse of a 2x2 matrix. !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- REAL ( KIND = iwp ), INTENT ( IN ) :: A ( 2 , 2 ) !> Matrix !-- local variables -- REAL ( KIND = iwp ) :: B ( 2 , 2 ) !> Inverse matrix REAL ( KIND = iwp ) :: detinv !> !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- Calculate the inverse determinant of the matrix -- detinv = 1.d0 / ( A ( 1 , 1 ) * A ( 2 , 2 ) - A ( 1 , 2 ) * A ( 2 , 1 )) !-- Calculate the inverse of the matrix -- B ( 1 , 1 ) = + detinv * A ( 2 , 2 ) B ( 2 , 1 ) = - detinv * A ( 2 , 1 ) B ( 1 , 2 ) = - detinv * A ( 1 , 2 ) B ( 2 , 2 ) = + detinv * A ( 1 , 1 ) RETURN END FUNCTION !  math_mod/matinv2 !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/f*  math_mod/matinv3 !* SYNOPSIS FUNCTION matinv3 ( A ) RESULT ( B ) !* PURPOSE !*   Performs a direct calculation of the inverse of a 3×3 matrix. !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/29: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !* NOTES !*   Adapted from: http://fortranwiki.org/fortran/show/Matrix+inversion !******/ !> author: Pieter Boom !> date: 2019/08/29 !> Performs a direct calculation of the inverse of a 3×3 matrix. !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- REAL ( KIND = iwp ), INTENT ( IN ) :: A ( 3 , 3 ) !> Matrix !-- local variables -- REAL ( KIND = iwp ) :: B ( 3 , 3 ) !> Inverse matrix REAL ( KIND = iwp ) :: detinv !> !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- Calculate the inverse determinant of the matrix -- detinv = 1.d0 / ( A ( 1 , 1 ) * ( A ( 2 , 2 ) * A ( 3 , 3 ) - A ( 2 , 3 ) * A ( 3 , 2 ))& - A ( 1 , 2 ) * ( A ( 2 , 1 ) * A ( 3 , 3 ) - A ( 2 , 3 ) * A ( 3 , 1 ))& + A ( 1 , 3 ) * ( A ( 2 , 1 ) * A ( 3 , 2 ) - A ( 2 , 2 ) * A ( 3 , 1 ))) !-- Calculate the inverse of the matrix -- B ( 1 , 1 ) = + detinv * ( A ( 2 , 2 ) * A ( 3 , 3 ) - A ( 2 , 3 ) * A ( 3 , 2 )) B ( 2 , 1 ) = - detinv * ( A ( 2 , 1 ) * A ( 3 , 3 ) - A ( 2 , 3 ) * A ( 3 , 1 )) B ( 3 , 1 ) = + detinv * ( A ( 2 , 1 ) * A ( 3 , 2 ) - A ( 2 , 2 ) * A ( 3 , 1 )) B ( 1 , 2 ) = - detinv * ( A ( 1 , 2 ) * A ( 3 , 3 ) - A ( 1 , 3 ) * A ( 3 , 2 )) B ( 2 , 2 ) = + detinv * ( A ( 1 , 1 ) * A ( 3 , 3 ) - A ( 1 , 3 ) * A ( 3 , 1 )) B ( 3 , 2 ) = - detinv * ( A ( 1 , 1 ) * A ( 3 , 2 ) - A ( 1 , 2 ) * A ( 3 , 1 )) B ( 1 , 3 ) = + detinv * ( A ( 1 , 2 ) * A ( 2 , 3 ) - A ( 1 , 3 ) * A ( 2 , 2 )) B ( 2 , 3 ) = - detinv * ( A ( 1 , 1 ) * A ( 2 , 3 ) - A ( 1 , 3 ) * A ( 2 , 1 )) B ( 3 , 3 ) = + detinv * ( A ( 1 , 1 ) * A ( 2 , 2 ) - A ( 1 , 2 ) * A ( 2 , 1 )) RETURN END FUNCTION !  math_mod/matinv3 !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/f*  math_mod/cross_product !* SYNOPSIS FUNCTION cross_product ( a , b ) !* PURPOSE !*   Performs a direct calculation of vector cross product !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/29: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !* NOTES !*   Adapted from: https://rosettacode.org/wiki/Vector_products#Fortran !******/ !> author: Pieter Boom !> date: 2019/08/29 !> Performs a direct calculation of vector cross product !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- REAL ( KIND = iwp ), INTENT ( IN ) :: a ( 3 ), b ( 3 ) !> input vectors !-- local variables -- REAL ( KIND = iwp ) :: cross_product ( 3 ) !> resulting vector !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- compute the cross product -- cross_product ( 1 ) = a ( 2 ) * b ( 3 ) - a ( 3 ) * b ( 2 ) cross_product ( 2 ) = a ( 3 ) * b ( 1 ) - a ( 1 ) * b ( 3 ) cross_product ( 3 ) = a ( 1 ) * b ( 2 ) - b ( 1 ) * a ( 2 ) RETURN END FUNCTION !  math_mod/cross_product !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! END MODULE ! math_mod !=============================================================================== !","tags":"","loc":"sourcefile/math_mod.f90.html","title":"math_mod.f90 – ParaGEMS"},{"text":"Module contains routines specifically related to Darcy flow equations This file depends on sourcefile~~darcy_mod.f90~~EfferentGraph sourcefile~darcy_mod.f90 darcy_mod.f90 sourcefile~common_mod.f90 common_mod.f90 sourcefile~darcy_mod.f90:->sourcefile~common_mod.f90: sourcefile~mpi_mod.f90 mpi_mod.f90 sourcefile~darcy_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~io_mod.f90 io_mod.f90 sourcefile~darcy_mod.f90:->sourcefile~io_mod.f90: sourcefile~solver_mod.f90 solver_mod.f90 sourcefile~darcy_mod.f90:->sourcefile~solver_mod.f90: sourcefile~mpi_mod.f90:->sourcefile~common_mod.f90: sourcefile~io_mod.f90:->sourcefile~common_mod.f90: sourcefile~io_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~io_mod.f90:->sourcefile~solver_mod.f90: sourcefile~dec_mod.f90 dec_mod.f90 sourcefile~io_mod.f90:->sourcefile~dec_mod.f90: sourcefile~solver_mod.f90:->sourcefile~common_mod.f90: sourcefile~solver_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~dec_mod.f90:->sourcefile~common_mod.f90: sourcefile~dec_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~math_mod.f90 math_mod.f90 sourcefile~dec_mod.f90:->sourcefile~math_mod.f90: sourcefile~math_mod.f90:->sourcefile~common_mod.f90: sourcefile~math_mod.f90:->sourcefile~mpi_mod.f90: Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Files dependent on this one sourcefile~~darcy_mod.f90~~AfferentGraph sourcefile~darcy_mod.f90 darcy_mod.f90 sourcefile~darcy.f90 darcy.f90 sourcefile~darcy.f90:->sourcefile~darcy_mod.f90: sourcefile~darcy_1f.f90 darcy_1f.f90 sourcefile~darcy_1f.f90:->sourcefile~darcy_mod.f90: sourcefile~darcy_crkp_2f_old.f90 darcy_crkp_2f_old.f90 sourcefile~darcy_crkp_2f_old.f90:->sourcefile~darcy_mod.f90: sourcefile~darcy_crkp_2f.f90 darcy_crkp_2f.f90 sourcefile~darcy_crkp_2f.f90:->sourcefile~darcy_mod.f90: sourcefile~darcy_crkp_1f.f90 darcy_crkp_1f.f90 sourcefile~darcy_crkp_1f.f90:->sourcefile~darcy_mod.f90: sourcefile~darcy_2f.f90 darcy_2f.f90 sourcefile~darcy_2f.f90:->sourcefile~darcy_mod.f90: sourcefile~test_darcy_mod.f90 test_darcy_mod.f90 sourcefile~test_darcy_mod.f90:->sourcefile~darcy_mod.f90: Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules darcy_mod Source Code darcy_mod.f90 Source Code ! !=============================================================================== !-- Darcy Module !> Module contains routines specifically related to Darcy flow equations !=============================================================================== !/****/h* modules|phy_darcy_flow/darcy_mod MODULE darcy_mod !* PURPOSE !*   Module contains routines specifically related to Darcy flow equations !* INCLUDES !*   Name                    Purpose !*   common_mod              variable definitions !*   mpi_mod                 general mpi routines (start, end, syncwrite,etc) !*   io_mod                  IO functions and routines !*   solver_mod              Solver routines (PETSc) !*   petsc.h                 PETSc variables and routines !*   darcy_vars.inc          Darcy specific variables !* CONTAINS !*   Subroutine              Purpose !*   read_input_darcy        read inputs, and check parameters (phys/io/solver) !*   check_param_darcy       check parameters for Darcy flow simulations !*   initialise_darcy !*   finalise_darcy !*   get_RHS_darcy !*   get_LHS_darcy !*   identify_crack !*   identify_crack2 !*   identify_crack3 !*   exchange_bndry_cond !*   initialise_darcy2 !*   finalise_darcy2 !*   get_RHS_darcy2 !*   get_LHS_darcy2 !*   identify_crack4 !*   identify_crack5 !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2020/09/16: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2020/09/16 !> Module contains routines specifically related to Darcy flow equations !=============================================================================== ! TO DO: ! - set SOLVER defaults !=============================================================================== !-- use statements and implicit none -- USE common_mod USE mpi_mod USE io_mod USE solver_mod IMPLICIT NONE !-- include Darcy flow specific variables -- #include <petsc/finclude/petsc.h> #include \"darcy_vars.inc\" !=============================================================================== CONTAINS !=============================================================================== ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* darcy_mod/read_input_darcy !* SYNOPSIS SUBROUTINE read_input_darcy () !* PURPOSE !*   Read input file for user defined parameters, and perform checks to ensure !*   reasonable values are set !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2020/09/16: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2020/09/16 !> Read input filefor user defined parameters, and perform checks to ensure !> reasonable values are set !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! ??? read from root and distribute ??? !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- local variables -- CHARACTER ( LEN = slen ) :: fname !> file name CHARACTER ( LEN = slen ) :: buffer !> buffer for command line arguments CHARACTER ( LEN = SLEN ) :: msg !> error msg string !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ CALL syncwrite_log ( '> read_input_darcy()' ) !-- check for user supplied input file name, otherwise use default -- CALL GETARG ( 1 , buffer ) IF ( LEN_TRIM ( buffer ) == 0 ) THEN fname = input_file ELSE READ ( buffer , * ) fname END IF !-- open input file -- CALL rootwrite_log ( '   - input file name: ' // fname ) OPEN ( input_unit , FILE = fname , STATUS = 'OLD' , ACTION = 'READ' , IOSTAT = ier ) IF ( ier /= 0 ) THEN WRITE ( msg , '(A,A,A,I5)' ) '*** Error: failed to open ' , trim ( fname ),& ': errcode = ' , ier CALL rootwrite_log ( msg ); CALL end_mpi () END IF !-- read input file -- read ( input_unit , nml = darcy_param ) read ( input_unit , nml = io_param ) read ( input_unit , nml = solver_param ) !-- close input file -- CLOSE ( input_unit ) !-- check parameters -- CALL check_param_darcy () !CALL check_param_io()     !-- nothing checked (yet) !CALL check_param_solver() !-- nothing checked (yet) RETURN END SUBROUTINE ! darcy_mod/read_input_darcy !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* darcy_mod/check_param_darcy !* SYNOPSIS SUBROUTINE check_param_darcy () !* PURPOSE !*   Check values set for solving Darcy flow simulation !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2020/09/16: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2020/09/16 !> Check values set for solving Darcy flow simulation !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- local variables -- LOGICAL :: err = . FALSE . !> error flag CHARACTER ( LEN = SLEN ) :: msg !> error msg string !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- physical parameters -- IF ( mu < 0.d0 ) THEN WRITE ( msg , '(A,E10.3)' ) '***Error: Invalid viscosity mu (must be greater ' , & 'than 0): value given ' , mu CALL rootwrite_log ( msg ); err = . TRUE . END IF IF ( k < 0.d0 ) THEN WRITE ( log_unit , '(A,E10.3)' ) '***Error: Invalid permeability k (must be ' , & 'greater than 0): value given ' , k CALL rootwrite_log ( msg ); err = . TRUE . END IF IF ( re < 0.d0 ) THEN WRITE ( log_unit , '(A,E10.3)' ) '***Error: Invalid Reynolds number re (must ' , & 'be greater than 0): value given ' , re CALL rootwrite_log ( msg ); err = . TRUE . END IF IF ( err ) CALL end_mpi () RETURN END SUBROUTINE ! darcy_mod/check_param_darcy !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* darcy_mod/initialise_darcy !* SYNOPSIS SUBROUTINE initialise_darcy () !* PURPOSE !*   initialise matrices and vectors for Darcy simulations !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> initialise matrices and vectors for Darcy simulations !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- local variables -- INTEGER :: m !> number of rows/columns in global solution variables INTEGER :: i !> loop index INTEGER :: iib , fib !> loop index INTEGER :: offset !> loop index REAL ( KIND = iwp ), ALLOCATABLE :: o_nnz (:) !> CHARACTER ( LEN = slen ) :: fname !> file name Vec :: work !> PETSc work array INTEGER :: id !> index !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ CALL syncwrite_log ( '> initialise_darcy()' ) curnt_time = MPI_WTIME () !-- allocate local solution variables -- ALLOCATE (& lcl_complex ( dim_cmplx )% prml_sol ( num_elm ( dim_cmplx ), 1 ),& lcl_complex ( dim_cmplx + 1 )% whtny_sol ( num_elm ( dim_cmplx + 1 ), dim_embbd ),& lcl_complex ( dim_cmplx + 1 )% dual_sol ( num_elm ( dim_cmplx + 1 ), 1 )) !-- allocate global solution variables -- !-- number of rows/columns -- m = glb_num_elm ( dim_cmplx + 1 ) + glb_num_elm ( dim_cmplx ) !-- vectors (RHS:b, solution:sol) -- CALL VecCreateMPI ( MPI_COMM_WORLD , PETSC_DECIDE , m , b , petsc_ier ) CALL VecSetFromOptions ( b , petsc_ier ) CALL VecDuplicate ( b , sol , petsc_ier ) CALL VecDuplicate ( b , work , petsc_ier ) !-- get local range of vectors/matrix -- CALL VecGetOwnershipRange ( work , iib , fib , petsc_ier ); ALLOCATE ( o_nnz ( fib - iib )) !-- system matrix (LHS:A) -- CALL MatCreate ( MPI_COMM_WORLD , A , petsc_ier ) CALL MatSetSizes ( A , PETSC_DECIDE , PETSC_DECIDE , m , m , petsc_ier ) CALL MatSetFromOptions ( A , petsc_ier ) !-- preallocate system matrix -- !-- calculate number of non-zeros per row for the equations for Darcy's law -- DO i = 1 , num_elm ( dim_cmplx ) IF ( lcl_complex ( dim_cmplx )% num_cobndry ( i ) > 0 ) & CALL VecSetValues ( work , 1 , lcl_complex ( dim_cmplx )% glb_indx ( i ) - 1 ,& 1.d0 * lcl_complex ( dim_cmplx )% num_cobndry ( i ), INSERT_VALUES , petsc_ier ) END DO !-- calculate number of non-zeros per row for the continuity equations -- offset = glb_num_elm ( dim_cmplx ) DO i = 1 , num_elm ( dim_cmplx + 1 ) CALL VecSetValues ( work , 1 , offset + lcl_complex ( dim_cmplx + 1 )% glb_indx ( i ) - 1 ,& 4.d0 , INSERT_VALUES , petsc_ier ) END DO !-- assemble vector -- CALL VecAssemblyBegin ( work , petsc_ier ); CALL VecAssemblyEnd ( work , petsc_ier ) CALL VecGetValues ( work , fib - iib ,( / ( i , i = iib , fib - 1 ) / ), o_nnz , petsc_ier ) !-- preallocate system matrix -- IF ( num_procs == 1 ) THEN CALL MatSeqAIJSetPreallocation ( A , 1 , int ( o_nnz + 1.d0 ), petsc_ier ) ELSEIF ( iib < glb_num_elm ( dim_cmplx ) . AND . fib >= glb_num_elm ( dim_cmplx )) THEN CALL MatMPIAIJSetPreallocation ( A , 1 , int ( min ( o_nnz + 1.d0 , real ( fib - iib ))), 1 , int ( o_nnz + 1.d0 ), petsc_ier ) ELSE CALL MatMPIAIJSetPreallocation ( A , 1 , PETSC_NULL_INTEGER , 1 , int ( o_nnz ), petsc_ier ) END IF !-- clean up -- CALL VecDestroy ( work , petsc_ier ) DEALLOCATE ( o_nnz ) !-- log time to initialise the darcy flow problem -- CALL syncwrite_log_time () RETURN END SUBROUTINE ! darcy_mod/initialise_darcy !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* darcy_mod/finalise_darcy !* SYNOPSIS SUBROUTINE finalise_darcy () !* PURPOSE !*   clean up matrices and vectors for Darcy simulations !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> clean up matrices and vectors for Darcy simulations !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !--deallocate vectors, matrices, and solver -- !-- vectors -- CALL VecDestroy ( b , petsc_ier ) CALL VecDestroy ( sol , petsc_ier ) !-- matrices -- CALL MatDestroy ( A , petsc_ier ) !-- linear solver -- CALL KSPDestroy ( ksp_id , petsc_ier ) RETURN END SUBROUTINE ! darcy_mod/finalise_darcy !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* darcy_mod/get_RHS_darcy !* SYNOPSIS SUBROUTINE get_RHS_darcy () !* PURPOSE !*   setup right hand side vector and boundary conditions for darcy simulations !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> setup right hand side vector and boundary conditions for darcy simulations !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- local variables -- INTEGER :: i , indx !> loop and temporary indicies INTEGER :: offset !> vector offset for volumes (after faces) INTEGER :: type_bc !> type of boundary condition LOGICAL :: use_pref , l_use_pref !> use reference pressure (if no pressure bc used) INTEGER :: num_flx !> number of flux boundary faces INTEGER , ALLOCATABLE :: flx_indx (:) !> flx boundary condition indices CHARACTER ( LEN = slen ) :: fname !> file name !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ CALL syncwrite_log ( '> get_RHS_darcy()' ); curnt_time = MPI_WTIME () CALL syncwrite_log ( '> get_RHS_darcy(): read_bndry_cond()' ); CALL read_bndry_cond2 () !CALL exchange_bndry_cond(dim_cmplx) CALL syncwrite_log_time (); curnt_time = MPI_WTIME () !--  set known values for boundary pressures and fluxes, as well as initial condition -- num_flx = 0 ALLOCATE ( flx_indx ( num_elm ( dim_cmplx ))) DO i = 1 , num_elm ( dim_cmplx ) type_bc = lcl_complex ( dim_cmplx )% bc_type ( i ) IF ( type_bc == - 1 ) THEN !-- impermeable boundary -- CALL VecSetValues ( sol , 1 , lcl_complex ( dim_cmplx )% glb_indx ( i ) - 1 , 0.d0 ,& INSERT_VALUES , petsc_ier ) num_flx = num_flx + 1 flx_indx ( num_flx ) = lcl_complex ( dim_cmplx )% glb_indx ( i ) - 1 ELSEIF ( type_bc == - 2 ) THEN !-- flux boundary -- CALL VecSetValues ( sol , 1 , lcl_complex ( dim_cmplx )% glb_indx ( i ) - 1 , - sum ( vel * & lcl_complex ( dim_cmplx )% dual_dir ( i , 1 : 3 )) * lcl_complex ( dim_cmplx )% prml_volume ( i ),& INSERT_VALUES , petsc_ier ) num_flx = num_flx + 1 flx_indx ( num_flx ) = lcl_complex ( dim_cmplx )% glb_indx ( i ) - 1 ELSEIF ( type_bc >= 0 . AND . nz_init ) THEN CALL VecSetValues ( sol , 1 , lcl_complex ( dim_cmplx )% glb_indx ( i ) - 1 , - sum ( vel * & lcl_complex ( dim_cmplx )% dual_dir ( i , 1 : 3 )) * lcl_complex ( dim_cmplx )% prml_volume ( i ),& INSERT_VALUES , petsc_ier ) ! ELSE !   WRITE(log_unit,'(A,I5)')'Error : undefined boundary type = ',type_bc !   CALL end_mpi() END IF END DO IF ( nz_init ) THEN DO i = 1 , num_elm ( dim_cmplx + 1 ) CALL VecSetValues ( sol , 1 , glb_num_elm ( dim_cmplx ) + lcl_complex ( dim_cmplx + 1 )% glb_indx ( i ) - 1 ,& - sum ( vel * lcl_complex ( dim_cmplx + 1 )% centers ( i , 1 : 3 )) + p_ref , INSERT_VALUES , petsc_ier ) END DO END IF !-- assemble vector of initial condition -- CALL VecAssemblyBegin ( sol , petsc_ier ); CALL VecAssemblyEnd ( sol , petsc_ier ) !-- set known values for boundary pressures and fluxes -- CALL syncwrite_log ( '> get_RHS_darcy(): assign bndry_cond()' ); l_use_pref = . TRUE . DO i = 1 , lcl_complex ( dim_cmplx + 1 )% num_surf indx = lcl_complex ( dim_cmplx + 1 )% surf_indx ( i , 2 ) type_bc = lcl_complex ( dim_cmplx + 1 )% surf_indx ( i , 4 ) IF ( type_bc > 0 ) THEN !-- pressure boundary condition -- CALL VecSetValues ( b , 1 , lcl_complex ( dim_cmplx )% glb_indx ( indx ) - 1 ,& - lcl_complex ( dim_cmplx + 1 )% surf_indx ( i , 1 ) * bc_press ( type_bc ),& INSERT_VALUES , petsc_ier ) l_use_pref = . FALSE . END IF END DO CALL syncwrite_log_time (); curnt_time = MPI_WTIME () !-- set reference pressure if no boundary pressure specified -- CALL syncwrite_log ( '> get_RHS_darcy(): pressure bndry_cond() and assembly' ); CALL MPI_REDUCE ( l_use_pref , use_pref , 1 , MPI_LOGICAL , MPI_LAND , 0 , MPI_COMM_WORLD , petsc_ier ) IF ( rank == root . and . . not . nz_init . and . use_pref ) & CALL VecSetValues ( sol , 1 , glb_num_elm ( dim_cmplx ), p_ref , INSERT_VALUES , petsc_ier ) !-- assemble vector of known values -- CALL VecAssemblyBegin ( sol , petsc_ier ); CALL VecAssemblyEnd ( sol , petsc_ier ) CALL VecAssemblyBegin ( b , petsc_ier ); CALL VecAssemblyEnd ( b , petsc_ier ) CALL syncwrite_log_time (); curnt_time = MPI_WTIME () CALL syncwrite_log ( '> get_RHS_darcy(): update LHS' ); !-- set RHS (b vector) from known values and remove relevant equations from LHS (A matrix) -- IF ( rank == root ) THEN IF ( use_pref ) THEN CALL MatZeroRowsColumns ( A , num_flx + 1 ,( / flx_indx ( 1 : num_flx ),& glb_num_elm ( dim_cmplx ) / ), 1.d0 , sol , b , petsc_ier ) ELSE CALL MatZeroRowsColumns ( A , num_flx ,( / flx_indx ( 1 : num_flx ) / ), 1.d0 , sol , b , petsc_ier ) END IF ELSE CALL MatZeroRowsColumns ( A , num_flx ,( / flx_indx ( 1 : num_flx ) / ), 1.d0 , sol , b , petsc_ier ) END IF CALL syncwrite_log_time (); curnt_time = MPI_WTIME () !-- clean up -- DEALLOCATE ( flx_indx ) !-- log time to construct RHS -- CALL syncwrite_log_time () RETURN END SUBROUTINE ! darcy_mod/get_RHS_darcy !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* darcy_mod/get_LHS_darcy !* SYNOPSIS SUBROUTINE get_LHS_darcy () !* PURPOSE !*   setup left hand side matrix for darcy simulations !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> setup left hand side matrix for darcy simulations !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- local variables -- INTEGER :: i , j , offset !> !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ CALL syncwrite_log ( '> get_LHS_darcy()' ) curnt_time = MPI_WTIME () !-- build LHS (A matrix) -- !-- equations for Darcy's law -- offset = glb_num_elm ( dim_cmplx ) DO i = 1 , num_elm ( dim_cmplx ) IF ( lcl_complex ( dim_cmplx )% num_cobndry ( i ) > 0 ) & CALL MatSetValues ( A , 1 , lcl_complex ( dim_cmplx )% glb_indx ( i ) - 1 ,& lcl_complex ( dim_cmplx )% num_cobndry ( i ) + 1 ,( / lcl_complex ( dim_cmplx )% glb_indx ( i ),& offset + lcl_complex ( dim_cmplx + 1 )% glb_indx ( lcl_complex ( dim_cmplx )% cobndry ( i )% indx ) / ) - 1 ,& ( / - mu / k * lcl_complex ( dim_cmplx )% hdg_star ( i ), real ( lcl_complex ( dim_cmplx )% cobndry ( i )% sgn , iwp ) / ),& INSERT_VALUES , petsc_ier ) ! CALL MatSetValues(A,1,lcl_complex(dim_cmplx)%glb_indx(i)-1,& !   lcl_complex(dim_cmplx)%num_cobndry(i)+1,(/ lcl_complex(dim_cmplx)%glb_indx(i),& !   offset+lcl_complex(dim_cmplx+1)%glb_indx(lcl_complex(dim_cmplx)%cobndry(i)%indx) /)-1,& !   (/ -mu/k,real(lcl_complex(dim_cmplx)%cobndry(i)%sgn,iwp) /)*lcl_complex(dim_cmplx)%inv_hdg_star(i),& !   INSERT_VALUES,petsc_ier) END DO !-- continuity equations: dual edges corresponding to internal primal faces -- DO i = 1 , num_elm ( dim_cmplx + 1 ) CALL MatSetValues ( A , 1 , offset + lcl_complex ( dim_cmplx + 1 )% glb_indx ( i ) - 1 ,& lcl_complex ( dim_cmplx + 1 )% num_bndry ( i ) + 1 ,( / offset + lcl_complex ( dim_cmplx + 1 )% glb_indx ( i ),& lcl_complex ( dim_cmplx )% glb_indx ( lcl_complex ( dim_cmplx + 1 )% bndry ( i )% indx ) / ) - 1 ,& ( / 0.d0 , real ( lcl_complex ( dim_cmplx + 1 )% bndry ( i )% sgn , iwp ) / ), INSERT_VALUES , petsc_ier ) ! CALL MatSetValues(A,1,offset+lcl_complex(dim_cmplx+1)%glb_indx(i)-1,& !   lcl_complex(dim_cmplx+1)%num_bndry(i)+1,(/ offset+lcl_complex(dim_cmplx+1)%glb_indx(i),& !   lcl_complex(dim_cmplx)%glb_indx(lcl_complex(dim_cmplx+1)%bndry(i)%indx) /)-1,& !   (/ 0.d0, real(lcl_complex(dim_cmplx+1)%bndry(i)%sgn,iwp)*& !   lcl_complex(dim_cmplx)%inv_hdg_star(lcl_complex(dim_cmplx+1)%bndry(i)%indx) /),& !   INSERT_VALUES,petsc_ier) END DO !-- continuity equations: dual edges corresponding to surface primal faces -- DO i = 1 , lcl_complex ( dim_cmplx + 1 )% num_surf CALL MatSetValues ( A ,& 1 , offset + lcl_complex ( dim_cmplx + 1 )% glb_indx ( lcl_complex ( dim_cmplx + 1 )% surf_indx ( i , 3 )) - 1 ,& 1 , lcl_complex ( dim_cmplx )% glb_indx ( lcl_complex ( dim_cmplx + 1 )% surf_indx ( i , 2 )) - 1 ,& - real ( lcl_complex ( dim_cmplx + 1 )% surf_indx ( i , 1 ), iwp ), INSERT_VALUES , petsc_ier ) ! CALL MatSetValues(A,& !   1,offset+lcl_complex(dim_cmplx+1)%glb_indx(lcl_complex(dim_cmplx+1)%surf_indx(i,3))-1,& !   1,lcl_complex(dim_cmplx)%glb_indx(lcl_complex(dim_cmplx+1)%surf_indx(i,2))-1,& !   -real(lcl_complex(dim_cmplx+1)%surf_indx(i,1),iwp)*& !   lcl_complex(dim_cmplx)%inv_hdg_star(lcl_complex(dim_cmplx+1)%surf_indx(i,2)),& !   INSERT_VALUES,petsc_ier) END DO !-- assemble system matrix -- CALL MatAssemblyBegin ( A , MAT_FINAL_ASSEMBLY , petsc_ier ) CALL MatAssemblyEnd ( A , MAT_FINAL_ASSEMBLY , petsc_ier ) CALL MatSetOption ( A , MAT_SYMMETRIC , PETSC_TRUE , petsc_ier ) !-- log time to build LHS -- CALL syncwrite_log_time () RETURN END SUBROUTINE ! darcy_mod/get_LHS_darcy !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* darcy_mod/identify_crack !* SYNOPSIS SUBROUTINE identify_crack ( exit_cond ) !* PURPOSE !*   identify faces to crack based on threshold value !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> identify faces to crack based on threshold value !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- LOGICAL , INTENT ( OUT ) :: exit_cond !> !-- local variables -- INTEGER :: i !> loop and temporary indicies INTEGER :: type_bc !> type of boundary condition INTEGER :: num_flx !> number of flux boundary faces INTEGER , ALLOCATABLE :: flx_indx (:) !> flx boundary condition indices LOGICAL :: lcl_exit_cond = . FALSE . !> !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ CALL syncwrite_log ( '> update_LRHS_darcy_crack()' ); curnt_time = MPI_WTIME () !-- set new cracks -- num_flx = 0 ALLOCATE ( flx_indx ( num_elm ( dim_cmplx ))) DO i = 1 , num_elm ( dim_cmplx ) IF ( lcl_complex ( dim_cmplx )% bc_type ( i ) == 0 . AND . & ABS ( lcl_complex ( dim_cmplx )% prml_sol ( i , 1 ) / & lcl_complex ( dim_cmplx )% prml_volume ( i )) > crck_thrshld ) THEN !-- impermeable boundary -- CALL VecSetValues ( sol , 1 , lcl_complex ( dim_cmplx )% glb_indx ( i ) - 1 , 0.d0 ,& INSERT_VALUES , petsc_ier ) num_flx = num_flx + 1 flx_indx ( num_flx ) = lcl_complex ( dim_cmplx )% glb_indx ( i ) - 1 lcl_complex ( dim_cmplx )% bc_type ( i ) = - 1 END IF END DO !-- assemble solution vector  -- CALL VecAssemblyBegin ( sol , petsc_ier ); CALL VecAssemblyEnd ( sol , petsc_ier ) CALL syncwrite_log ( '> identify_max_flux(): update LHS' ); !-- set RHS (b vector) from known values and remove relevant equations from LHS (A matrix) -- CALL MatZeroRowsColumns ( A , num_flx ,( / flx_indx ( 1 : num_flx ) / ), 1.d0 , sol , b , petsc_ier ) CALL syncwrite_log_time (); curnt_time = MPI_WTIME () !-- clean up -- DEALLOCATE ( flx_indx ) !-- log time to construct RHS -- CALL syncwrite_log_time () IF ( num_flx == 0 ) lcl_exit_cond = . TRUE . CALL MPI_ALLREDUCE ( lcl_exit_cond , exit_cond , 1 , MPI_LOGICAL , MPI_LAND , MPI_COMM_WORLD , ier ) RETURN END SUBROUTINE ! darcy_mod/identify_crack !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* darcy_mod/identify_crack2 !* SYNOPSIS SUBROUTINE identify_crack2 ( exit_cond ) !* PURPOSE !*   identify faces to crack based on threshold value !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> identify faces to crack based on threshold value !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- LOGICAL , INTENT ( OUT ) :: exit_cond !> !-- local variables -- INTEGER :: i , j , indx !> loop and temporary indicies INTEGER :: type_bc !> type of boundary condition INTEGER :: num_flx , lcl_num_flx , glb_num_flx !> number of flux boundary faces INTEGER , ALLOCATABLE :: flx_indx (:) !> flx boundary condition indices REAL ( KIND = iwp ), ALLOCATABLE :: flxs (:) !> flx boundary condition indices REAL ( KIND = iwp ) :: flx , tmp_crck_thrshld !> flx boundary condition indices LOGICAL :: lcl_exit_cond = . FALSE . !> CHARACTER ( LEN = slen ) :: msg !> !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ CALL syncwrite_log ( '> update_LRHS_darcy_crack()' ); curnt_time = MPI_WTIME () !-- set new cracks -- lcl_num_flx = 0 ALLOCATE ( flx_indx ( num_elm ( dim_cmplx )), flxs ( num_elm ( dim_cmplx ))) DO i = 1 , num_elm ( dim_cmplx ) IF ( lcl_complex ( dim_cmplx )% bc_type ( i ) == 0 ) THEN flx = ABS ( lcl_complex ( dim_cmplx )% prml_sol ( i , 1 ) / lcl_complex ( dim_cmplx )% prml_volume ( i )) IF ( flx > crck_thrshld ) THEN lcl_num_flx = lcl_num_flx + 1 flx_indx ( lcl_num_flx ) = i flxs ( lcl_num_flx ) = flx END IF END IF END DO CALL MPI_ALLREDUCE ( lcl_num_flx , glb_num_flx , 1 , MPI_INTEGER , MPI_SUM , MPI_COMM_WORLD , ier ) write ( msg , * ) glb_num_flx , crck_thrshld CALL syncwrite_log ( msg ) tmp_crck_thrshld = 1.05 * crck_thrshld DO WHILE ( glb_num_flx > crcks_pstep ) num_flx = lcl_num_flx lcl_num_flx = 0 DO j = 1 , num_flx i = flx_indx ( j ) IF ( flxs ( j ) > tmp_crck_thrshld ) THEN lcl_num_flx = lcl_num_flx + 1 flx_indx ( lcl_num_flx ) = flx_indx ( j ) flxs ( lcl_num_flx ) = flxs ( j ) END IF END DO CALL MPI_ALLREDUCE ( lcl_num_flx , glb_num_flx , 1 , MPI_INTEGER , MPI_SUM , MPI_COMM_WORLD , ier ) write ( msg , * ) glb_num_flx , tmp_crck_thrshld CALL syncwrite_log ( msg ) IF ( glb_num_flx == 0 ) THEN tmp_crck_thrshld = 0.99 * tmp_crck_thrshld glb_num_flx = crcks_pstep + 1 lcl_num_flx = num_flx ELSE tmp_crck_thrshld = 1.05 * tmp_crck_thrshld END IF END DO DO j = 1 , lcl_num_flx !-- impermeable boundary -- i = flx_indx ( j ) CALL VecSetValues ( sol , 1 , lcl_complex ( dim_cmplx )% glb_indx ( i ) - 1 , 0.d0 ,& INSERT_VALUES , petsc_ier ) flx_indx ( j ) = lcl_complex ( dim_cmplx )% glb_indx ( i ) - 1 lcl_complex ( dim_cmplx )% bc_type ( i ) = - 1 END DO !-- assemble solution vector  -- CALL VecAssemblyBegin ( sol , petsc_ier ); CALL VecAssemblyEnd ( sol , petsc_ier ) CALL syncwrite_log ( '> identify_max_flux(): update LHS' ); !-- set RHS (b vector) from known values and remove relevant equations from LHS (A matrix) -- CALL MatZeroRowsColumns ( A , lcl_num_flx ,( / flx_indx ( 1 : lcl_num_flx ) / ), 1.d0 , sol , b , petsc_ier ) CALL syncwrite_log_time (); curnt_time = MPI_WTIME () !-- clean up -- DEALLOCATE ( flx_indx , flxs ) !-- log time to construct RHS -- CALL syncwrite_log_time () IF ( lcl_num_flx == 0 ) lcl_exit_cond = . TRUE . CALL MPI_ALLREDUCE ( lcl_exit_cond , exit_cond , 1 , MPI_LOGICAL , MPI_LAND , MPI_COMM_WORLD , ier ) RETURN END SUBROUTINE ! darcy_mod/identify_crack2 !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* darcy_mod/identify_crack3 !* SYNOPSIS SUBROUTINE identify_crack3 ( iter , exit_cond ) !* PURPOSE !*   identify faces to crack based on max value !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> identify faces to crack based on max value !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- INTEGER , INTENT ( IN ) :: iter !> LOGICAL , INTENT ( OUT ) :: exit_cond !> !-- local variables -- INTEGER :: i , junk , max_indx ( 2 ) !> loop and temporary indicies REAL ( KIND = iwp ) :: glb_max_flx ( 3 ) !> REAL ( KIND = iwp ) :: max_flx ( 3 ) !> REAL ( KIND = iwp ) :: flx !> REAL ( KIND = iwp ) :: area !> CHARACTER ( LEN = slen ) :: msg !> INTEGER :: status ( MPI_STATUS_SIZE ) !> size of MPI comm buffer !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ CALL syncwrite_log ( '> update_LRHS_darcy_crack()' ); curnt_time = MPI_WTIME () !-- find new crack -- max_flx = ( / 0.d0 , dble ( rank ), 0.d0 / ) max_indx = - 1 area = 0.d0 DO i = 1 , num_elm ( dim_cmplx ) IF ( lcl_complex ( dim_cmplx )% bc_type ( i ) == 0 ) THEN area = lcl_complex ( dim_cmplx )% prml_volume ( i ) flx = ABS ( lcl_complex ( dim_cmplx )% prml_sol ( i , 1 ) / area ) IF ( flx > max_flx ( 1 )) THEN max_indx = ( / i , lcl_complex ( dim_cmplx )% glb_indx ( i ) - 1 / ) max_flx ( 1 ) = flx max_flx ( 2 ) = dble ( max_indx ( 2 )) max_flx ( 3 ) = area END IF END IF END DO CALL MPI_ALLREDUCE ( MPI_IN_PLACE , max_flx ( 1 : 2 ), 2 , MPI_2DOUBLE_PRECISION , MPI_MAXLOC , MPI_COMM_WORLD , ier ) !-- impermeable boundary -- IF ( max_flx ( 1 ) > crck_thrshld ) THEN IF ( max_indx ( 2 ) == int ( max_flx ( 2 ))) THEN CALL VecSetValues ( sol , 1 , max_indx ( 2 ), 0.d0 , INSERT_VALUES , petsc_ier ) CALL VecAssemblyBegin ( sol , petsc_ier ); CALL VecAssemblyEnd ( sol , petsc_ier ) CALL MatZeroRowsColumns ( A , 1 ,( / max_indx ( 2 ) / ), 1.d0 , sol , b , petsc_ier ) lcl_complex ( dim_cmplx )% bc_type ( max_indx ( 1 )) = - 1 ELSE CALL VecAssemblyBegin ( sol , petsc_ier ); CALL VecAssemblyEnd ( sol , petsc_ier ) CALL MatZeroRowsColumns ( A , 0 ,( / 0 / ), 1.d0 , sol , b , petsc_ier ) max_flx = 0.d0 END IF CALL MPI_REDUCE ( max_flx , glb_max_flx , 3 , MPI_DOUBLE_PRECISION , MPI_MAX , 0 , MPI_COMM_WORLD , ier ) IF ( rank == root ) THEN area = glb_max_flx ( 3 ) crck_area = crck_area + area WRITE ( ulog_unit , * ) iter , glb_max_flx ( 1 ), int ( glb_max_flx ( 2 )), area , area ** ( 3.d0 / 2.d0 ), crck_area , crck_area ** ( 3.d0 / 2.d0 ) CALL FLUSH ( ulog_unit ) END IF exit_cond = . FALSE . ELSE exit_cond = . TRUE . END IF !-- log time to construct RHS -- CALL syncwrite_log_time () RETURN END SUBROUTINE ! darcy_mod/identify_crack3 !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* darcy_mod/exchange_bndry_cond !* SYNOPSIS SUBROUTINE exchange_bndry_cond ( k ) !* PURPOSE !*   exchange boundary conditions !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> exchange boundary conditions !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- INTEGER , INTENT ( IN ) :: k !> simplicial order !-- local variables -- INTEGER :: kp !> simplicial order INTEGER :: ptr , ptrp !> pointer INTEGER :: i , j !> loop index INTEGER :: junk !> junk comm variable INTEGER :: buffer_size !> size of comm buffer INTEGER , ALLOCATABLE :: sbuffer (:) !> send buffer INTEGER , ALLOCATABLE :: rbuffer (:) !> receive buffer INTEGER , ALLOCATABLE :: req (:) !> request variable for non-blocking comms INTEGER , ALLOCATABLE :: status (:,:) !> size of MPI comm buffer !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- pack send buffer -- kp = k + 1 ALLOCATE ( sbuffer ( lcl_complex ( kp )% num_send )) DO j = 1 , lcl_complex ( kp )% num_send sbuffer ( j ) = lcl_complex ( k )% bc_type ( lcl_complex ( kp )% send_indx ( j , 2 )) END DO !-- send data to adjacent processes -- ptr = 1 DO j = 1 , num_adj_proc IF ( num_send ( k , j ) == 0 ) CYCLE buffer_size = num_send ( k , j ) CALL MPI_ISEND ( sbuffer ( ptr : ptr + buffer_size - 1 ), buffer_size ,& MPI_INTEGER , adj_proc ( j ), 0 , MPI_COMM_WORLD , junk , ier ) CALL MPI_REQUEST_FREE ( junk , ier ) ptr = ptr + buffer_size END DO !-- receive data from adjacent processes -- ALLOCATE (& req ( num_adj_proc ),& status ( MPI_STATUS_SIZE , num_adj_proc ),& rbuffer ( lcl_complex ( kp )% num_recv )) ptr = 1 DO j = 1 , num_adj_proc IF ( num_recv ( k , j ) == 0 ) THEN req ( j ) = MPI_REQUEST_NULL CYCLE END IF buffer_size = num_recv ( k , j ) CALL MPI_IRECV ( rbuffer ( ptr : ptr + buffer_size - 1 ), buffer_size ,& MPI_INTEGER , adj_proc ( j ), 0 , MPI_COMM_WORLD , req ( j ), ier ) ptr = ptr + buffer_size END DO !-- unpack receive buffer -- CALL MPI_WAITALL ( num_adj_proc , req , status , ier ) DO j = 1 , lcl_complex ( kp )% num_recv lcl_complex ( k )% bc_type ( lcl_complex ( kp )% recv_indx ( j , 2 )) = rbuffer ( j ) END DO !-- clean up -- DEALLOCATE ( req , status ) !-- make sure all comms have completed -- CALL MPI_BARRIER ( MPI_COMM_WORLD , ier ) RETURN END SUBROUTINE ! io_mod|exchange_bndry_cond !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* darcy_mod/initialise_darcy2 !* SYNOPSIS SUBROUTINE initialise_darcy2 () !* PURPOSE !*   initialise matrices and vectors for Darcy simulations !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> initialise matrices and vectors for Darcy simulations !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- local variables -- INTEGER :: m !> number of rows/columns in global solution variables INTEGER :: offset !> loop index !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ CALL syncwrite_log ( '> initialise_darcy()' ) curnt_time = MPI_WTIME () !-- allocate local solution variables -- ALLOCATE (& lcl_complex ( dim_cmplx )% prml_sol ( num_elm ( dim_cmplx ), 1 ),& lcl_complex ( dim_cmplx + 1 )% whtny_sol ( num_elm ( dim_cmplx + 1 ), dim_embbd ),& lcl_complex ( dim_cmplx + 1 )% dual_sol ( num_elm ( dim_cmplx + 1 ), 1 )) !-- log time to initialise the darcy flow problem -- CALL syncwrite_log_time () RETURN END SUBROUTINE ! darcy_mod/initialise_darcy2 !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* darcy_mod/finalise_darcy2 !* SYNOPSIS SUBROUTINE finalise_darcy2 () !* PURPOSE !*   clean up matrices and vectors for Darcy simulations !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> clean up matrices and vectors for Darcy simulations !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !--deallocate vectors, matrices, and solver -- !-- vectors -- CALL VecDestroy ( b , petsc_ier ) CALL VecDestroy ( sol , petsc_ier ) !-- matrices -- CALL MatDestroy ( Asub ( 1 ), petsc_ier ) CALL MatDestroy ( Asub ( 2 ), petsc_ier ) CALL MatDestroy ( Asub ( 3 ), petsc_ier ) CALL MatDestroy ( Asub ( 4 ), petsc_ier ) CALL MatDestroy ( A , petsc_ier ) CALL MatDestroy ( Sp , petsc_ier ) !-- index set -- CALL ISDestroy ( isg ( 1 ), petsc_ier ) CALL ISDestroy ( isg ( 2 ), petsc_ier ) !-- linear solver -- CALL KSPDestroy ( ksp_id , petsc_ier ) RETURN END SUBROUTINE ! darcy_mod/finalise_darcy2 !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* darcy_mod/get_RHS_darcy2 !* SYNOPSIS SUBROUTINE get_RHS_darcy2 () !* PURPOSE !*   setup right hand side vector and boundary conditions for darcy simulations !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> setup right hand side vector and boundary conditions for darcy simulations !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- local variables -- INTEGER :: i , indx !> loop and temporary indicies INTEGER :: offset !> vector offset for volumes (after faces) INTEGER :: type_bc !> type of boundary condition LOGICAL :: use_pref , l_use_pref !> use reference pressure (if no pressure bc used) INTEGER :: num_flx !> number of flux boundary faces INTEGER , ALLOCATABLE :: flx_indx (:) !> flx boundary condition indices CHARACTER ( LEN = slen ) :: fname !> file name INTEGER :: m , n !> REAL ( KIND = iwp ) :: flx !> Vec :: b1 , b2 , sol1 , sol2 , wrk !> !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ CALL syncwrite_log ( '> get_RHS_darcy()' ); curnt_time = MPI_WTIME () CALL syncwrite_log ( '> get_RHS_darcy(): read_bndry_cond()' ); ! CALL read_bndry_cond() ! CALL exchange_bndry_cond(dim_cmplx) CALL read_bndry_cond2 () CALL syncwrite_log_time (); curnt_time = MPI_WTIME () !-- vectors (RHS:b, solution:sol) -- CALL MatGetOwnershipRange ( A , m , n , petsc_ier ) CALL VecCreateMPI ( MPI_COMM_WORLD , n - m , glb_num_elm ( dim_cmplx + 1 ) + glb_num_elm ( dim_cmplx ), sol , petsc_ier ) CALL VecSetFromOptions ( sol , petsc_ier ); CALL VecDuplicate ( sol , b , petsc_ier ) CALL VecGetSubVector ( b , isg ( 1 ), b1 , petsc_ier ); CALL VecGetSubVector ( b , isg ( 2 ), b2 , petsc_ier ) CALL VecGetSubVector ( sol , isg ( 1 ), sol1 , petsc_ier ); CALL VecGetSubVector ( sol , isg ( 2 ), sol2 , petsc_ier ) CALL MatGetOwnershipRange ( Asub ( 1 ), m , n , petsc_ier ) CALL VecCreateMPI ( MPI_COMM_WORLD , n - m , glb_num_elm ( dim_cmplx ), wrk , petsc_ier ) CALL VecSetFromOptions ( wrk , petsc_ier ) !--  set known values for initial and boundary conditions (fluxes) -- num_flx = 0 ALLOCATE ( flx_indx ( num_elm ( dim_cmplx ))) DO i = 1 , num_elm ( dim_cmplx ) type_bc = lcl_complex ( dim_cmplx )% bc_type ( i ) IF ( type_bc == - 1 ) THEN !-- impermeable boundary -- CALL VecSetValues ( sol1 , 1 , lcl_complex ( dim_cmplx )% glb_indx ( i ) - 1 , 0.d0 ,& INSERT_VALUES , petsc_ier ) CALL MatSetValues ( Asub ( 1 ), 1 , lcl_complex ( dim_cmplx )% glb_indx ( i ) - 1 ,& 1 , lcl_complex ( dim_cmplx )% glb_indx ( i ) - 1 , - 1.d0 , INSERT_VALUES , petsc_ier ) CALL MatSetValues ( Asub ( 2 ), 1 , lcl_complex ( dim_cmplx )% glb_indx ( i ) - 1 ,& lcl_complex ( dim_cmplx )% num_cobndry ( i ),& lcl_complex ( dim_cmplx + 1 )% glb_indx (& lcl_complex ( dim_cmplx )% cobndry ( i )% indx ) - 1 ,& ( / ( 0.d0 , m = 1 , lcl_complex ( dim_cmplx )% num_cobndry ( i )) / ),& INSERT_VALUES , petsc_ier ) ELSEIF ( type_bc == - 3 ) THEN !-- impermeable boundary -- CALL VecSetValues ( sol1 , 1 , lcl_complex ( dim_cmplx )% glb_indx ( i ) - 1 , 0.d0 ,& INSERT_VALUES , petsc_ier ) CALL MatSetValues ( Asub ( 1 ), 1 , lcl_complex ( dim_cmplx )% glb_indx ( i ) - 1 ,& 1 , lcl_complex ( dim_cmplx )% glb_indx ( i ) - 1 , - 1.d0 , INSERT_VALUES , petsc_ier ) CALL MatSetValues ( Asub ( 2 ), 1 , lcl_complex ( dim_cmplx )% glb_indx ( i ) - 1 ,& lcl_complex ( dim_cmplx )% num_cobndry ( i ),& lcl_complex ( dim_cmplx + 1 )% glb_indx (& lcl_complex ( dim_cmplx )% cobndry ( i )% indx ) - 1 ,& ( / ( 0.d0 , m = 1 , lcl_complex ( dim_cmplx )% num_cobndry ( i )) / ),& INSERT_VALUES , petsc_ier ) ELSEIF ( type_bc == - 2 ) THEN !-- flux boundary -- flx = - sum ( vel * lcl_complex ( dim_cmplx )% dual_dir ( i , 1 : 3 )) * lcl_complex ( dim_cmplx )% prml_volume ( i ) CALL VecSetValues ( sol1 , 1 , lcl_complex ( dim_cmplx )% glb_indx ( i ) - 1 , flx ,& INSERT_VALUES , petsc_ier ) CALL VecSetValues ( wrk , 1 , lcl_complex ( dim_cmplx )% glb_indx ( i ) - 1 , - flx ,& INSERT_VALUES , petsc_ier ) CALL VecSetValues ( b1 , 1 , lcl_complex ( dim_cmplx )% glb_indx ( i ) - 1 , flx ,& INSERT_VALUES , petsc_ier ) CALL MatSetValues ( Asub ( 1 ), 1 , lcl_complex ( dim_cmplx )% glb_indx ( i ) - 1 ,& 1 , lcl_complex ( dim_cmplx )% glb_indx ( i ) - 1 , 1.d0 , INSERT_VALUES , petsc_ier ) CALL MatSetValues ( Asub ( 2 ), 1 , lcl_complex ( dim_cmplx )% glb_indx ( i ) - 1 ,& lcl_complex ( dim_cmplx )% num_cobndry ( i ), lcl_complex ( dim_cmplx + 1 )% glb_indx (& lcl_complex ( dim_cmplx )% cobndry ( i )% indx ) - 1 ,& ( / ( 0.d0 , m = 1 , lcl_complex ( dim_cmplx )% num_cobndry ( i )) / ), INSERT_VALUES , petsc_ier ) ELSEIF ( type_bc >= 0 . AND . nz_init ) THEN CALL VecSetValues ( sol1 , 1 , lcl_complex ( dim_cmplx )% glb_indx ( i ) - 1 , - sum ( vel * & lcl_complex ( dim_cmplx )% dual_dir ( i , 1 : 3 )) * lcl_complex ( dim_cmplx )% prml_volume ( i ),& INSERT_VALUES , petsc_ier ) ! ELSE !   WRITE(log_unit,'(A,A,A,I5)')'Error : undefined boundary type = ',type_bc !   CALL end_mpi() END IF END DO !--  set known values for initial conditions (pressures) -- IF ( nz_init ) THEN DO i = 1 , num_elm ( dim_cmplx + 1 ) CALL VecSetValues ( sol2 , 1 , lcl_complex ( dim_cmplx + 1 )% glb_indx ( i ) - 1 ,& - sum ( vel * lcl_complex ( dim_cmplx + 1 )% centers ( i , 1 : 3 )) + p_ref , INSERT_VALUES , petsc_ier ) END DO END IF !-- set known values for boundary conditions (pressures) -- CALL syncwrite_log ( '> get_RHS_darcy(): assign bndry_cond()' ); DO i = 1 , lcl_complex ( dim_cmplx + 1 )% num_surf indx = lcl_complex ( dim_cmplx + 1 )% surf_indx ( i , 2 ) type_bc = lcl_complex ( dim_cmplx + 1 )% surf_indx ( i , 4 ) IF ( type_bc > 0 ) THEN !-- pressure boundary condition -- CALL VecSetValues ( b1 , 1 , lcl_complex ( dim_cmplx )% glb_indx ( indx ) - 1 ,& - lcl_complex ( dim_cmplx + 1 )% surf_indx ( i , 1 ) * bc_press ( type_bc ),& INSERT_VALUES , petsc_ier ) END IF END DO !-- assemble sub-vectors -- CALL VecAssemblyBegin ( sol1 , petsc_ier ); CALL VecAssemblyEnd ( sol1 , petsc_ier ) CALL VecAssemblyBegin ( sol2 , petsc_ier ); CALL VecAssemblyEnd ( sol2 , petsc_ier ) CALL VecAssemblyBegin ( b1 , petsc_ier ); CALL VecAssemblyEnd ( b1 , petsc_ier ) CALL VecAssemblyBegin ( b2 , petsc_ier ); CALL VecAssemblyEnd ( b2 , petsc_ier ) CALL VecAssemblyBegin ( wrk , petsc_ier ); CALL VecAssemblyEnd ( wrk , petsc_ier ) CALL syncwrite_log ( '> get_RHS_darcy(): update LHS' ); !-- set RHS (b vector) from known values and remove relevant equations from LHS (A matrix) -- CALL MatAssemblyBegin ( Asub ( 1 ), MAT_FINAL_ASSEMBLY , petsc_ier ); CALL MatAssemblyEnd ( Asub ( 1 ), MAT_FINAL_ASSEMBLY , petsc_ier ) !CALL MatMultAdd(Asub(1),wrk,b1,b1,petsc_ier) CALL MatMultAdd ( Asub ( 3 ), wrk , b2 , b2 , petsc_ier ) CALL MatAssemblyBegin ( Asub ( 2 ), MAT_FINAL_ASSEMBLY , petsc_ier ); CALL MatAssemblyEnd ( Asub ( 2 ), MAT_FINAL_ASSEMBLY , petsc_ier ) CALL MatTranspose ( Asub ( 2 ), MAT_INITIAL_MATRIX , Asub ( 3 ), petsc_ier ) CALL MatAssemblyBegin ( Asub ( 3 ), MAT_FINAL_ASSEMBLY , petsc_ier ); CALL MatAssemblyEnd ( Asub ( 3 ), MAT_FINAL_ASSEMBLY , petsc_ier ) !-- rebuild matrices and vectors -- CALL MatCreateNest ( MPI_COMM_WORLD , 2 , PETSC_NULL_IS , 2 , PETSC_NULL_IS , Asub , A , petsc_ier ) CALL VecRestoreSubVector ( b , isg ( 1 ), b1 , petsc_ier ); CALL VecRestoreSubVector ( b , isg ( 2 ), b2 , petsc_ier ) CALL VecRestoreSubVector ( sol , isg ( 1 ), sol1 , petsc_ier ); CALL VecRestoreSubVector ( sol , isg ( 2 ), sol2 , petsc_ier ) CALL VecDestroy ( wrk , petsc_ier ) !-- clean up -- DEALLOCATE ( flx_indx ) !-- log time to construct RHS -- CALL syncwrite_log_time () RETURN END SUBROUTINE ! darcy_mod/get_RHS_darcy2 !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* darcy_mod/get_LHS_darcy2 !* SYNOPSIS SUBROUTINE get_LHS_darcy2 () !* PURPOSE !*   setup left hand side matrix for darcy simulations !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> setup left hand side matrix for darcy simulations !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- local variables -- INTEGER :: m !> number of rows/columns in global solution variables INTEGER :: i , j , offset !> REAL ( KIND = iwp ) :: rnd !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ CALL syncwrite_log ( '> get_LHS_darcy()' ) curnt_time = MPI_WTIME () !-- predefine some useful variables -- offset = glb_num_elm ( dim_cmplx ) m = glb_num_elm ( dim_cmplx + 1 ) + glb_num_elm ( dim_cmplx ) !-- system matrix - block 00 -- CALL MatCreate ( MPI_COMM_WORLD , Asub ( 1 ), petsc_ier ) CALL MatSetSizes ( Asub ( 1 ), PETSC_DECIDE , PETSC_DECIDE , glb_num_elm ( dim_cmplx ), glb_num_elm ( dim_cmplx ), petsc_ier ) CALL MatSetType ( Asub ( 1 ), MATMPIAIJ , petsc_ier ) CALL MatMPIAIJSetPreallocation ( Asub ( 1 ), 1 , PETSC_NULL_INTEGER , 0 , PETSC_NULL_INTEGER , petsc_ier ) CALL MatSetFromOptions ( Asub ( 1 ), petsc_ier ) ! DO i = 1,num_elm(dim_cmplx+1) !   lcl_complex(dim_cmplx+1)%prml_sol(i,1) = -mu/k*(rand()-0.5d0) ! END DO ! CALL exchange_prml_sol(dim_cmplx+1) ! DO i = 1,num_elm(dim_cmplx) !   IF (lcl_complex(dim_cmplx)%num_cobndry(i) == 1) THEN !     lcl_complex(dim_cmplx)%hdg_star(i) = lcl_complex(dim_cmplx)%hdg_star(i) * & !       lcl_complex(dim_cmplx+1)%prml_sol(lcl_complex(dim_cmplx)%cobndry(i)%indx(1),1) !   ELSE !     indx1 = lcl_complex(dim_cmplx)%cobndry(i)%indx(1) !     pt1 = lcl_complex(dim_cmplx+1)%centers(indx1,:) !     s1 = lcl_complex(dim_cmplx+1)%prml_sol(indx1,1) ! !     pt2 = lcl_complex(dim_cmplx)%centers(i,:) ! !     indx2 = lcl_complex(dim_cmplx)%cobndry(i)%indx(2) !     pt3 = lcl_complex(dim_cmplx+1)%centers(indx2,:) !     s2 = lcl_complex(dim_cmplx+1)%prml_sol(,1) ! !     dx1 = pt2 - pt1 !     dx2 = pt3 - pt2 !     dx1 = sqrt(dot_product(dx1,dx1)) !     dx1 = sqrt(dot_product(dx2,dx2)) ! !     lcl_complex(dim_cmplx)%hdg_star(i) = (dx1*s1 + dx2*s2) / (dx1+dx2) ! !   END IF ! ! END DO !-- equations for Darcy's law -- CALL RANDOM_SEED () DO i = 1 , num_elm ( dim_cmplx ) CALL RANDOM_NUMBER ( rnd ) IF ( lcl_complex ( dim_cmplx )% num_cobndry ( i ) > 0 ) & CALL MatSetValues ( Asub ( 1 ), 1 , lcl_complex ( dim_cmplx )% glb_indx ( i ) - 1 ,& 1 , lcl_complex ( dim_cmplx )% glb_indx ( i ) - 1 , - ( 1.d0 + rnd_max * 2.0d0 * ( rnd - 0.5d0 )) * & mu / k * sign ( 1.d0 , lcl_complex ( dim_cmplx )% hdg_star ( i )) * & max ( abs ( lcl_complex ( dim_cmplx )% hdg_star ( i )), smallh ), INSERT_VALUES , petsc_ier ) END DO CALL MatAssemblyBegin ( Asub ( 1 ), MAT_FINAL_ASSEMBLY , petsc_ier ) CALL MatAssemblyEnd ( Asub ( 1 ), MAT_FINAL_ASSEMBLY , petsc_ier ) CALL MatSetOption ( Asub ( 1 ), MAT_SYMMETRIC , PETSC_TRUE , petsc_ier ) !-- system matrix - block 10 -- CALL MatCreate ( MPI_COMM_WORLD , Asub ( 3 ), petsc_ier ) CALL MatSetSizes ( Asub ( 3 ), PETSC_DECIDE , PETSC_DECIDE , glb_num_elm ( dim_cmplx + 1 ), glb_num_elm ( dim_cmplx ), petsc_ier ) CALL MatSetType ( Asub ( 3 ), MATMPIAIJ , petsc_ier ) CALL MatMPIAIJSetPreallocation ( Asub ( 3 ), 4 , PETSC_NULL_INTEGER , 4 , PETSC_NULL_INTEGER , petsc_ier ) CALL MatSetOption ( Asub ( 3 ), MAT_IGNORE_ZERO_ENTRIES , PETSC_TRUE , petsc_ier ) CALL MatSetFromOptions ( Asub ( 3 ), petsc_ier ) !-- continuity equations: dual edges corresponding to internal primal faces -- DO i = 1 , num_elm ( dim_cmplx + 1 ) CALL MatSetValues ( Asub ( 3 ), 1 , lcl_complex ( dim_cmplx + 1 )% glb_indx ( i ) - 1 ,& lcl_complex ( dim_cmplx + 1 )% num_bndry ( i ),& lcl_complex ( dim_cmplx )% glb_indx ( lcl_complex ( dim_cmplx + 1 )% bndry ( i )% indx ) - 1 ,& real ( lcl_complex ( dim_cmplx + 1 )% bndry ( i )% sgn , iwp ), INSERT_VALUES , petsc_ier ) END DO !-- continuity equations: dual edges corresponding to surface primal faces -- DO i = 1 , lcl_complex ( dim_cmplx + 1 )% num_surf CALL MatSetValues ( Asub ( 3 ),& 1 , lcl_complex ( dim_cmplx + 1 )% glb_indx ( lcl_complex ( dim_cmplx + 1 )% surf_indx ( i , 3 )) - 1 ,& 1 , lcl_complex ( dim_cmplx )% glb_indx ( lcl_complex ( dim_cmplx + 1 )% surf_indx ( i , 2 )) - 1 ,& - real ( lcl_complex ( dim_cmplx + 1 )% surf_indx ( i , 1 ), iwp ), INSERT_VALUES , petsc_ier ) END DO CALL MatAssemblyBegin ( Asub ( 3 ), MAT_FINAL_ASSEMBLY , petsc_ier ) CALL MatAssemblyEnd ( Asub ( 3 ), MAT_FINAL_ASSEMBLY , petsc_ier ) !-- system matrix - block 01 -- CALL MatTranspose ( Asub ( 3 ), MAT_INITIAL_MATRIX , Asub ( 2 ), petsc_ier ) CALL MatSetOption ( Asub ( 2 ), MAT_IGNORE_ZERO_ENTRIES , PETSC_TRUE , petsc_ier ) CALL MatSetFromOptions ( Asub ( 2 ), petsc_ier ) !-- system matrix - block 11 -- CALL MatCreate ( MPI_COMM_WORLD , Asub ( 4 ), petsc_ier ) CALL MatSetSizes ( Asub ( 4 ), PETSC_DECIDE , PETSC_DECIDE , glb_num_elm ( dim_cmplx + 1 ), glb_num_elm ( dim_cmplx + 1 ), petsc_ier ) CALL MatSetType ( Asub ( 4 ), MATMPIAIJ , petsc_ier ) CALL MatMPIAIJSetPreallocation ( Asub ( 4 ), 0 , PETSC_NULL_INTEGER , 0 , PETSC_NULL_INTEGER , petsc_ier ) CALL MatSetFromOptions ( Asub ( 4 ), petsc_ier ) CALL MatAssemblyBegin ( Asub ( 4 ), MAT_FINAL_ASSEMBLY , petsc_ier ) CALL MatAssemblyEnd ( Asub ( 4 ), MAT_FINAL_ASSEMBLY , petsc_ier ) CALL MatSetOption ( Asub ( 4 ), MAT_SYMMETRIC , PETSC_TRUE , petsc_ier ) !-- system matrix (LHS:A) -- CALL MatCreateNest ( MPI_COMM_WORLD , 2 , PETSC_NULL_IS , 2 , PETSC_NULL_IS , Asub , A , petsc_ier ) CALL MatSetOption ( A , MAT_SYMMETRIC , PETSC_TRUE , petsc_ier ) CALL MatNestGetISs ( A , isg , PETSC_NULL_IS , petsc_ier ) !-- system Schur Complement -- ! CALL MatCreate(MPI_COMM_WORLD,Sp,petsc_ier) ! CALL MatSetSizes(Sp,PETSC_DECIDE,PETSC_DECIDE,& !   glb_num_elm(dim_cmplx)+glb_num_elm(dim_cmplx+1),& !   glb_num_elm(dim_cmplx)+glb_num_elm(dim_cmplx+1),petsc_ier) ! CALL MatSetType(Sp, MATMPIAIJ,petsc_ier) ! CALL MatMPIAIJSetPreallocation(Sp,5,PETSC_NULL_INTEGER,5,PETSC_NULL_INTEGER,petsc_ier) ! CALL MatSetFromOptions(Sp,petsc_ier) !-- log time to build LHS -- CALL syncwrite_log_time () RETURN END SUBROUTINE ! darcy_mod/get_LHS_darcy2 !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* darcy_mod/identify_crack4 !* SYNOPSIS SUBROUTINE identify_crack4 ( iter , exit_cond ) !* PURPOSE !*   identify faces to crack based on threshold value !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> identify faces to crack based on threshold value !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- INTEGER , INTENT ( IN ) :: iter !> LOGICAL , INTENT ( OUT ) :: exit_cond !> !-- local variables -- INTEGER :: i , j !> loop and temporary indicies INTEGER :: type_bc !> type of boundary condition INTEGER :: num_flx !> number of flux boundary faces INTEGER , ALLOCATABLE :: flx_indx (:) !> flx boundary condition indices LOGICAL :: lcl_exit_cond = . FALSE . !> !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ CALL syncwrite_log ( '> update_LRHS_darcy_crack()' ); curnt_time = MPI_WTIME () !-- set new cracks -- num_flx = 0 DO i = 1 , num_elm ( dim_cmplx ) IF ( lcl_complex ( dim_cmplx )% bc_type ( i ) == 0 . AND . & ABS ( lcl_complex ( dim_cmplx )% prml_sol ( i , 1 ) / & lcl_complex ( dim_cmplx )% prml_volume ( i )) > crck_thrshld ) THEN !-- impermeable boundary -- CALL VecSetValues ( sol , 1 , lcl_complex ( dim_cmplx )% glb_indx ( i ) - 1 , 0.d0 ,& INSERT_VALUES , petsc_ier ) CALL MatSetValues ( Asub ( 1 ), 1 , lcl_complex ( dim_cmplx )% glb_indx ( i ) - 1 ,& 1 , lcl_complex ( dim_cmplx )% glb_indx ( i ) - 1 , 1.d0 , INSERT_VALUES , petsc_ier ) CALL MatSetValues ( Asub ( 2 ), 1 , lcl_complex ( dim_cmplx )% glb_indx ( i ) - 1 ,& lcl_complex ( dim_cmplx )% num_cobndry ( i ), lcl_complex ( dim_cmplx + 1 )% glb_indx (& lcl_complex ( dim_cmplx )% cobndry ( i )% indx ) - 1 ,& ( / ( small , j = 1 , lcl_complex ( dim_cmplx )% num_cobndry ( i )) / ), INSERT_VALUES , petsc_ier ) END IF END DO !-- assemble solution vector  -- CALL VecAssemblyBegin ( sol , petsc_ier ); CALL VecAssemblyEnd ( sol , petsc_ier ) !-- assemble solution vector  -- CALL VecAssemblyBegin ( sol , petsc_ier ); CALL VecAssemblyEnd ( sol , petsc_ier ) CALL MatAssemblyBegin ( Asub ( 1 ), MAT_FINAL_ASSEMBLY , petsc_ier ) CALL MatAssemblyEnd ( Asub ( 1 ), MAT_FINAL_ASSEMBLY , petsc_ier ) CALL MatAssemblyBegin ( Asub ( 2 ), MAT_FINAL_ASSEMBLY , petsc_ier ); CALL MatAssemblyEnd ( Asub ( 2 ), MAT_FINAL_ASSEMBLY , petsc_ier ) CALL MatTranspose ( Asub ( 2 ), MAT_REUSE_MATRIX , Asub ( 3 ), petsc_ier ) CALL MatAssemblyBegin ( Asub ( 3 ), MAT_FINAL_ASSEMBLY , petsc_ier ); CALL MatAssemblyEnd ( Asub ( 3 ), MAT_FINAL_ASSEMBLY , petsc_ier ) ! CALL MatCreateNest(MPI_COMM_WORLD,2,PETSC_NULL_IS,2,PETSC_NULL_IS,Asub,A,petsc_ier) !-- clean up -- DEALLOCATE ( flx_indx ) !-- log time to construct RHS -- CALL syncwrite_log_time () IF ( num_flx == 0 ) lcl_exit_cond = . TRUE . CALL MPI_ALLREDUCE ( lcl_exit_cond , exit_cond , 1 , MPI_LOGICAL , MPI_LAND , MPI_COMM_WORLD , ier ) RETURN END SUBROUTINE ! darcy_mod/identify_crack4 !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* darcy_mod/identify_crack5 !* SYNOPSIS SUBROUTINE identify_crack5 ( iter , exit_cond ) !* PURPOSE !*    identify faces to crack based on max value !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> identify faces to crack based on max value !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- INTEGER , INTENT ( IN ) :: iter !> LOGICAL , INTENT ( OUT ) :: exit_cond !> !-- local variables -- INTEGER :: i , junk , max_indx ( 2 ) !> loop and temporary indicies REAL ( KIND = iwp ) :: glb_max_flx ( 3 ) !> REAL ( KIND = iwp ) :: max_flx ( 3 ) !> REAL ( KIND = iwp ) :: flx !> REAL ( KIND = iwp ) :: area !> CHARACTER ( LEN = slen ) :: msg !> INTEGER :: status ( MPI_STATUS_SIZE ) !> size of MPI comm buffer !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ CALL syncwrite_log ( '> update_LRHS_darcy_crack()' ); curnt_time = MPI_WTIME () !-- find new crack -- max_flx = ( / 0.d0 , dble ( rank ), 0.d0 / ) max_indx = - 1 area = 0.d0 DO i = 1 , num_elm ( dim_cmplx ) IF ( lcl_complex ( dim_cmplx )% bc_type ( i ) == 0 ) THEN area = lcl_complex ( dim_cmplx )% prml_volume ( i ) flx = ABS ( lcl_complex ( dim_cmplx )% prml_sol ( i , 1 ) / area ) IF ( flx > max_flx ( 1 )) THEN max_indx = ( / i , lcl_complex ( dim_cmplx )% glb_indx ( i ) - 1 / ) max_flx ( 1 ) = flx max_flx ( 2 ) = dble ( max_indx ( 2 )) max_flx ( 3 ) = area END IF END IF END DO CALL MPI_ALLREDUCE ( MPI_IN_PLACE , max_flx ( 1 : 2 ), 2 , MPI_2DOUBLE_PRECISION ,& MPI_MAXLOC , MPI_COMM_WORLD , ier ) !-- impermeable boundary -- IF ( max_flx ( 1 ) > crck_thrshld ) THEN IF ( max_indx ( 2 ) == int ( max_flx ( 2 ))) THEN CALL VecSetValues ( sol , 1 , max_indx ( 2 ), 0.d0 , INSERT_VALUES , petsc_ier ) CALL MatSetValues ( Asub ( 1 ), 1 , max_indx ( 2 ), 1 , max_indx ( 2 ), 1.d0 , INSERT_VALUES ,& petsc_ier ) CALL MatSetValues ( Asub ( 2 ), 1 , max_indx ( 2 ),& lcl_complex ( dim_cmplx )% num_cobndry ( max_indx ( 1 )),& lcl_complex ( dim_cmplx + 1 )% glb_indx (& lcl_complex ( dim_cmplx )% cobndry ( max_indx ( 1 ))% indx ) - 1 ,& ( / ( small , i = 1 , lcl_complex ( dim_cmplx )% num_cobndry ( max_indx ( 1 ))) / ),& INSERT_VALUES , petsc_ier ) lcl_complex ( dim_cmplx )% bc_type ( max_indx ( 1 )) = - 1 ELSE max_flx = 0.d0 END IF CALL MPI_REDUCE ( max_flx , glb_max_flx , 3 , MPI_DOUBLE_PRECISION , MPI_MAX , 0 ,& MPI_COMM_WORLD , ier ) IF ( rank == root ) THEN area = glb_max_flx ( 3 ) crck_area = crck_area + area WRITE ( ulog_unit , * ) iter , glb_max_flx ( 1 ), int ( glb_max_flx ( 2 )), area , & area ** ( 3.d0 / 2.d0 ), crck_area , crck_area ** ( 3.d0 / 2.d0 ) CALL FLUSH ( ulog_unit ) END IF !-- assemble solution vector  -- CALL VecAssemblyBegin ( sol , petsc_ier ); CALL VecAssemblyEnd ( sol , petsc_ier ) CALL MatAssemblyBegin ( Asub ( 1 ), MAT_FINAL_ASSEMBLY , petsc_ier ) CALL MatAssemblyEnd ( Asub ( 1 ), MAT_FINAL_ASSEMBLY , petsc_ier ) CALL MatAssemblyBegin ( Asub ( 2 ), MAT_FINAL_ASSEMBLY , petsc_ier ) CALL MatAssemblyEnd ( Asub ( 2 ), MAT_FINAL_ASSEMBLY , petsc_ier ) CALL MatTranspose ( Asub ( 2 ), MAT_REUSE_MATRIX , Asub ( 3 ), petsc_ier ) CALL MatAssemblyBegin ( Asub ( 3 ), MAT_FINAL_ASSEMBLY , petsc_ier ) CALL MatAssemblyEnd ( Asub ( 3 ), MAT_FINAL_ASSEMBLY , petsc_ier ) ! CALL MatCreateNest(MPI_COMM_WORLD,2,PETSC_NULL_IS,2,PETSC_NULL_IS,Asub,A,petsc_ier) exit_cond = . FALSE . ELSE exit_cond = . TRUE . END IF !-- log time to construct RHS -- CALL syncwrite_log_time () RETURN END SUBROUTINE ! darcy_mod/identify_crack5 !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* darcy_mod/identify_crack6 !* SYNOPSIS SUBROUTINE identify_crack6 ( iter , exit_cond ) !* PURPOSE !*   Introduce random crack to face not on the boundary or already cracked !* INPUTS !*   Name                    Description !*   iter !*   exit_cond !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2020/12/09: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2020/12/09 !> !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- INTEGER , INTENT ( IN ) :: iter !> LOGICAL , INTENT ( OUT ) :: exit_cond !> !-- local variables -- INTEGER :: i , j , junk , indx !> loop and temporary indicies REAL ( KIND = iwp ) :: area !> CHARACTER ( LEN = slen ) :: msg !> INTEGER :: status ( MPI_STATUS_SIZE ) !> size of MPI comm buffer REAL ( KIND = iwp ) :: rnd LOGICAL :: search !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ CALL syncwrite_log ( '> update_LRHS_darcy_crack()' ); curnt_time = MPI_WTIME () search = . TRUE . area = 0.d0 DO WHILE ( search ) !-- find random face -- IF ( rank == root ) THEN CALL RANDOM_SEED () CALL RANDOM_NUMBER ( rnd ) indx = nint ( rnd * ( glb_num_elm ( dim_cmplx ) - 1 )) + 1 END IF !-- check if it is an interface -- CALL MPI_BCAST ( indx , 1 , MPI_INTEGER , 0 , MPI_COMM_WORLD , ier ) !-- if local and interface, then end search -- DO i = 1 , num_elm ( dim_cmplx ) IF ( lcl_complex ( dim_cmplx )% glb_indx ( i ) == indx . AND . & lcl_complex ( dim_cmplx )% bc_type ( i ) == 0 ) THEN area = lcl_complex ( dim_cmplx )% prml_volume ( i ) CALL VecSetValues ( sol , 1 , indx - 1 , 0.d0 , INSERT_VALUES , petsc_ier ) CALL MatSetValues ( Asub ( 1 ), 1 , indx - 1 , 1 , indx - 1 , 1.d0 , INSERT_VALUES ,& petsc_ier ) CALL MatSetValues ( Asub ( 2 ), 1 , indx - 1 ,& lcl_complex ( dim_cmplx )% num_cobndry ( i ),& lcl_complex ( dim_cmplx + 1 )% glb_indx (& lcl_complex ( dim_cmplx )% cobndry ( i )% indx ) - 1 ,& ( / ( 0.d0 , j = 1 , lcl_complex ( dim_cmplx )% num_cobndry ( i )) / ),& INSERT_VALUES , petsc_ier ) lcl_complex ( dim_cmplx )% bc_type ( i ) = - 1 search = . FALSE . END IF END DO !-- end search? -- CALL MPI_ALLREDUCE ( MPI_IN_PLACE , search , 1 , MPI_LOGICAL , MPI_LAND ,& MPI_COMM_WORLD , ier ) END DO IF ( rank == root ) THEN CALL MPI_REDUCE ( MPI_IN_PLACE , area , 1 , MPI_DOUBLE_PRECISION , MPI_MAX , 0 ,& MPI_COMM_WORLD , ier ) crck_area = crck_area + area WRITE ( ulog_unit , * ) iter , 0.d0 , indx , area , area ** ( 3.d0 / 2.d0 ), & crck_area , crck_area ** ( 3.d0 / 2.d0 ) CALL FLUSH ( ulog_unit ) ELSE CALL MPI_REDUCE ( area , area , 1 , MPI_DOUBLE_PRECISION , MPI_MAX , 0 ,& MPI_COMM_WORLD , ier ) END IF !-- assemble solution vector  -- CALL VecAssemblyBegin ( sol , petsc_ier ); CALL VecAssemblyEnd ( sol , petsc_ier ) CALL MatAssemblyBegin ( Asub ( 1 ), MAT_FINAL_ASSEMBLY , petsc_ier ) CALL MatAssemblyEnd ( Asub ( 1 ), MAT_FINAL_ASSEMBLY , petsc_ier ) CALL MatAssemblyBegin ( Asub ( 2 ), MAT_FINAL_ASSEMBLY , petsc_ier ) CALL MatAssemblyEnd ( Asub ( 2 ), MAT_FINAL_ASSEMBLY , petsc_ier ) CALL MatTranspose ( Asub ( 2 ), MAT_REUSE_MATRIX , Asub ( 3 ), petsc_ier ) CALL MatAssemblyBegin ( Asub ( 3 ), MAT_FINAL_ASSEMBLY , petsc_ier ) CALL MatAssemblyEnd ( Asub ( 3 ), MAT_FINAL_ASSEMBLY , petsc_ier ) ! CALL MatCreateNest(MPI_COMM_WORLD,2,PETSC_NULL_IS,2,PETSC_NULL_IS,Asub,A,petsc_ier) exit_cond = . FALSE . !-- log time to construct RHS -- CALL syncwrite_log_time () RETURN END SUBROUTINE ! darcy_mod/identify_crack6 !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* darcy_mod/identify_crack7 !* SYNOPSIS SUBROUTINE identify_crack7 ( iter , exit_cond ) !* PURPOSE !* !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- INTEGER , INTENT ( IN ) :: iter !> LOGICAL , INTENT ( OUT ) :: exit_cond !> !-- local variables -- INTEGER :: i , junk , max_indx ( 2 ) !> loop and temporary indicies REAL ( KIND = iwp ) :: glb_max_flx ( 3 ) !> REAL ( KIND = iwp ) :: max_flx ( 3 ) !> REAL ( KIND = iwp ) :: flx !> REAL ( KIND = iwp ) :: area !> CHARACTER ( LEN = slen ) :: msg !> INTEGER :: status ( MPI_STATUS_SIZE ) !> size of MPI comm buffer !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ CALL syncwrite_log ( '> update_LRHS_darcy_crack()' ); curnt_time = MPI_WTIME () !-- find new crack -- max_flx = ( / large , dble ( rank ), 0.d0 / ) max_indx = - 1 area = 0.d0 write ( * , * ) \"----------------------------\" DO i = 1 , num_elm ( dim_cmplx ) IF ( lcl_complex ( dim_cmplx )% bc_type ( i ) == 0 ) THEN area = lcl_complex ( dim_cmplx )% prml_volume ( i ) flx = ABS ( lcl_complex ( dim_cmplx )% prml_sol ( i , 1 ) / area ) IF ( flx < max_flx ( 1 )) THEN max_indx = ( / i , lcl_complex ( dim_cmplx )% glb_indx ( i ) - 1 / ) max_flx ( 1 ) = flx max_flx ( 2 ) = dble ( max_indx ( 2 )) max_flx ( 3 ) = area END IF END IF END DO write ( * , * ) max_flx , max_indx CALL MPI_ALLREDUCE ( MPI_IN_PLACE , max_flx ( 1 : 2 ), 2 , MPI_2DOUBLE_PRECISION ,& MPI_MINLOC , MPI_COMM_WORLD , ier ) write ( * , * ) max_flx , max_indx !-- impermeable boundary -- IF ( max_indx ( 2 ) == int ( max_flx ( 2 ))) THEN CALL VecSetValues ( sol , 1 , max_indx ( 2 ), 0.d0 , INSERT_VALUES , petsc_ier ) CALL MatSetValues ( Asub ( 1 ), 1 , max_indx ( 2 ), 1 , max_indx ( 2 ), 1.d0 , INSERT_VALUES ,& petsc_ier ) CALL MatSetValues ( Asub ( 2 ), 1 , max_indx ( 2 ),& lcl_complex ( dim_cmplx )% num_cobndry ( max_indx ( 1 )),& lcl_complex ( dim_cmplx + 1 )% glb_indx (& lcl_complex ( dim_cmplx )% cobndry ( max_indx ( 1 ))% indx ) - 1 ,& ( / ( small , i = 1 , lcl_complex ( dim_cmplx )% num_cobndry ( max_indx ( 1 ))) / ),& INSERT_VALUES , petsc_ier ) lcl_complex ( dim_cmplx )% bc_type ( max_indx ( 1 )) = - 1 ELSE max_flx = large END IF CALL MPI_REDUCE ( max_flx , glb_max_flx , 3 , MPI_DOUBLE_PRECISION , MPI_MIN , 0 ,& MPI_COMM_WORLD , ier ) write ( * , * ) max_flx , max_indx IF ( rank == root ) THEN area = glb_max_flx ( 3 ) crck_area = crck_area + area WRITE ( ulog_unit , * ) iter , glb_max_flx ( 1 ), int ( glb_max_flx ( 2 )), area , & area ** ( 3.d0 / 2.d0 ), crck_area , crck_area ** ( 3.d0 / 2.d0 ) CALL FLUSH ( ulog_unit ) END IF !-- assemble solution vector  -- CALL VecAssemblyBegin ( sol , petsc_ier ); CALL VecAssemblyEnd ( sol , petsc_ier ) CALL MatAssemblyBegin ( Asub ( 1 ), MAT_FINAL_ASSEMBLY , petsc_ier ) CALL MatAssemblyEnd ( Asub ( 1 ), MAT_FINAL_ASSEMBLY , petsc_ier ) CALL MatAssemblyBegin ( Asub ( 2 ), MAT_FINAL_ASSEMBLY , petsc_ier ) CALL MatAssemblyEnd ( Asub ( 2 ), MAT_FINAL_ASSEMBLY , petsc_ier ) CALL MatTranspose ( Asub ( 2 ), MAT_REUSE_MATRIX , Asub ( 3 ), petsc_ier ) CALL MatAssemblyBegin ( Asub ( 3 ), MAT_FINAL_ASSEMBLY , petsc_ier ) CALL MatAssemblyEnd ( Asub ( 3 ), MAT_FINAL_ASSEMBLY , petsc_ier ) ! CALL MatCreateNest(MPI_COMM_WORLD,2,PETSC_NULL_IS,2,PETSC_NULL_IS,Asub,A,petsc_ier) exit_cond = . FALSE . IF ( glb_max_flx ( 1 ) == large ) exit_cond = . TRUE . !-- log time to construct RHS -- CALL syncwrite_log_time () RETURN END SUBROUTINE ! darcy_mod/identify_crack7 !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! END MODULE ! darcy_mod !=============================================================================== !","tags":"","loc":"sourcefile/darcy_mod.f90.html","title":"darcy_mod.f90 – ParaGEMS"},{"text":"This file depends on sourcefile~~test_darcy_mod.f90~~EfferentGraph sourcefile~test_darcy_mod.f90 test_darcy_mod.f90 sourcefile~darcy_mod.f90 darcy_mod.f90 sourcefile~test_darcy_mod.f90:->sourcefile~darcy_mod.f90: sourcefile~common_mod.f90 common_mod.f90 sourcefile~darcy_mod.f90:->sourcefile~common_mod.f90: sourcefile~mpi_mod.f90 mpi_mod.f90 sourcefile~darcy_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~io_mod.f90 io_mod.f90 sourcefile~darcy_mod.f90:->sourcefile~io_mod.f90: sourcefile~solver_mod.f90 solver_mod.f90 sourcefile~darcy_mod.f90:->sourcefile~solver_mod.f90: sourcefile~mpi_mod.f90:->sourcefile~common_mod.f90: sourcefile~io_mod.f90:->sourcefile~common_mod.f90: sourcefile~io_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~io_mod.f90:->sourcefile~solver_mod.f90: sourcefile~dec_mod.f90 dec_mod.f90 sourcefile~io_mod.f90:->sourcefile~dec_mod.f90: sourcefile~solver_mod.f90:->sourcefile~common_mod.f90: sourcefile~solver_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~dec_mod.f90:->sourcefile~common_mod.f90: sourcefile~dec_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~math_mod.f90 math_mod.f90 sourcefile~dec_mod.f90:->sourcefile~math_mod.f90: sourcefile~math_mod.f90:->sourcefile~common_mod.f90: sourcefile~math_mod.f90:->sourcefile~mpi_mod.f90: Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules test_darcy_mod Source Code test_darcy_mod.f90 Source Code ! !=============================================================================== ! /****m*/ /src/modules/common/common_test MODULE test_darcy_mod ! ! PURPOSE:    Tests for common_mod module ! ! TESTS: ! ! UPDATES:  created (PDB) :: 2019/08/21 ! !=============================================================================== !----------------------------------------------------------------------------- ! use statements and implicit none !----------------------------------------------------------------------------- USE darcy_mod IMPLICIT NONE !=============================================================================== CONTAINS !=============================================================================== ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! /****s*/ /src/common/common_test|test_vars SUBROUTINE test_vars ( cnt ) ! ! PURPOSE: ! ! UPDATES:  created (PDB) :: 2019/08/21 ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! TO DO: ! - global variables? ! - primary element data structure ! - clean up !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !--------------------------------------------------------------------- ! arguments !--------------------------------------------------------------------- INTEGER , INTENT ( INOUT ) :: cnt ! test counter !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !--------------------------------------------------------------------- ! 1. precision !--------------------------------------------------------------------- cnt = cnt + 1 IF ( 1.d0 - 9.999999999999999d-1 > 0.d0 ) THEN WRITE ( * , '(A,I5,A)' ) 'common_test : ' , cnt ,& ' : v/      PASSED : precision accurate to 16 decimal places' ELSE WRITE ( * , '(A,I5,A)' ) 'common_test : ' , cnt ,& ' : XXXXXX  FAILED : precision NOT accurate to 16 decimal places' END IF !--------------------------------------------------------------------- ! 2. global variables ! 3. global mpi ! 4. global io ! 5. global mesh ! 6. primary element ! 7. variable clean up !--------------------------------------------------------------------- END SUBROUTINE ! common_test|test_vars !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! END MODULE ! common_test !=============================================================================== !","tags":"","loc":"sourcefile/test_darcy_mod.f90.html","title":"test_darcy_mod.f90 – ParaGEMS"},{"text":"Module for global variables, included library, and common code Files dependent on this one sourcefile~~common_mod.f90~~AfferentGraph sourcefile~common_mod.f90 common_mod.f90 sourcefile~darcy.f90 darcy.f90 sourcefile~darcy.f90:->sourcefile~common_mod.f90: sourcefile~darcy_mod.f90 darcy_mod.f90 sourcefile~darcy.f90:->sourcefile~darcy_mod.f90: sourcefile~solver_mod.f90 solver_mod.f90 sourcefile~darcy.f90:->sourcefile~solver_mod.f90: sourcefile~partition_mod.f90 partition_mod.f90 sourcefile~darcy.f90:->sourcefile~partition_mod.f90: sourcefile~mpi_mod.f90 mpi_mod.f90 sourcefile~darcy.f90:->sourcefile~mpi_mod.f90: sourcefile~test_common_mod.f90 test_common_mod.f90 sourcefile~test_common_mod.f90:->sourcefile~common_mod.f90: sourcefile~darcy_mod.f90:->sourcefile~common_mod.f90: sourcefile~io_mod.f90 io_mod.f90 sourcefile~darcy_mod.f90:->sourcefile~io_mod.f90: sourcefile~darcy_mod.f90:->sourcefile~solver_mod.f90: sourcefile~darcy_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~io_mod.f90:->sourcefile~common_mod.f90: sourcefile~io_mod.f90:->sourcefile~solver_mod.f90: sourcefile~dec_mod.f90 dec_mod.f90 sourcefile~io_mod.f90:->sourcefile~dec_mod.f90: sourcefile~io_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~solver_mod.f90:->sourcefile~common_mod.f90: sourcefile~solver_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~darcy_crkp_2f.f90 darcy_crkp_2f.f90 sourcefile~darcy_crkp_2f.f90:->sourcefile~common_mod.f90: sourcefile~darcy_crkp_2f.f90:->sourcefile~darcy_mod.f90: sourcefile~darcy_crkp_2f.f90:->sourcefile~solver_mod.f90: sourcefile~darcy_crkp_2f.f90:->sourcefile~partition_mod.f90: sourcefile~darcy_crkp_2f.f90:->sourcefile~mpi_mod.f90: sourcefile~darcy_crkp_2f_old.f90 darcy_crkp_2f_old.f90 sourcefile~darcy_crkp_2f_old.f90:->sourcefile~common_mod.f90: sourcefile~darcy_crkp_2f_old.f90:->sourcefile~darcy_mod.f90: sourcefile~darcy_crkp_2f_old.f90:->sourcefile~solver_mod.f90: sourcefile~darcy_crkp_2f_old.f90:->sourcefile~partition_mod.f90: sourcefile~darcy_crkp_2f_old.f90:->sourcefile~mpi_mod.f90: sourcefile~math_mod.f90 math_mod.f90 sourcefile~math_mod.f90:->sourcefile~common_mod.f90: sourcefile~math_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~darcy_crkp_1f.f90 darcy_crkp_1f.f90 sourcefile~darcy_crkp_1f.f90:->sourcefile~common_mod.f90: sourcefile~darcy_crkp_1f.f90:->sourcefile~darcy_mod.f90: sourcefile~darcy_crkp_1f.f90:->sourcefile~solver_mod.f90: sourcefile~darcy_crkp_1f.f90:->sourcefile~partition_mod.f90: sourcefile~darcy_crkp_1f.f90:->sourcefile~mpi_mod.f90: sourcefile~partition_mod.f90:->sourcefile~common_mod.f90: sourcefile~partition_mod.f90:->sourcefile~io_mod.f90: sourcefile~partition_mod.f90:->sourcefile~math_mod.f90: sourcefile~partition_mod.f90:->sourcefile~dec_mod.f90: sourcefile~partition_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~paragems_unit.f90 paragems_unit.f90 sourcefile~paragems_unit.f90:->sourcefile~common_mod.f90: sourcefile~paragems_unit.f90:->sourcefile~test_common_mod.f90: sourcefile~test_mpi.f90 test_mpi.f90 sourcefile~test_mpi.f90:->sourcefile~common_mod.f90: sourcefile~test_mpi.f90:->sourcefile~mpi_mod.f90: sourcefile~darcy_2f.f90 darcy_2f.f90 sourcefile~darcy_2f.f90:->sourcefile~common_mod.f90: sourcefile~darcy_2f.f90:->sourcefile~darcy_mod.f90: sourcefile~darcy_2f.f90:->sourcefile~solver_mod.f90: sourcefile~darcy_2f.f90:->sourcefile~partition_mod.f90: sourcefile~darcy_2f.f90:->sourcefile~mpi_mod.f90: sourcefile~dec_mod.f90:->sourcefile~common_mod.f90: sourcefile~dec_mod.f90:->sourcefile~math_mod.f90: sourcefile~dec_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~darcy_1f.f90 darcy_1f.f90 sourcefile~darcy_1f.f90:->sourcefile~common_mod.f90: sourcefile~darcy_1f.f90:->sourcefile~darcy_mod.f90: sourcefile~darcy_1f.f90:->sourcefile~solver_mod.f90: sourcefile~darcy_1f.f90:->sourcefile~partition_mod.f90: sourcefile~darcy_1f.f90:->sourcefile~mpi_mod.f90: sourcefile~mpi_mod.f90:->sourcefile~common_mod.f90: sourcefile~test_math_mod.f90 test_math_mod.f90 sourcefile~test_math_mod.f90:->sourcefile~math_mod.f90: sourcefile~test_io_mod.f90 test_io_mod.f90 sourcefile~test_io_mod.f90:->sourcefile~io_mod.f90: sourcefile~test_solver_mod.f90 test_solver_mod.f90 sourcefile~test_solver_mod.f90:->sourcefile~solver_mod.f90: sourcefile~test_dec_mod.f90 test_dec_mod.f90 sourcefile~test_dec_mod.f90:->sourcefile~dec_mod.f90: sourcefile~test_darcy_mod.f90 test_darcy_mod.f90 sourcefile~test_darcy_mod.f90:->sourcefile~darcy_mod.f90: sourcefile~test_partition_mod.f90 test_partition_mod.f90 sourcefile~test_partition_mod.f90:->sourcefile~partition_mod.f90: Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules common_mod Source Code common_mod.f90 Source Code ! !=============================================================================== !-- Common Module !> Module for global variables, included library, and common code !=============================================================================== !/****/h* modules|comon/common_mod !* SYNOPSIS MODULE common_mod !* PURPOSE !*   Module for global variables, included library, and common code !* INCLUDES !*   Name                  Purpose !*   petsc.h                 - petsc library (includes mpi execution !*                             environment library) !*   global_precision.inc    - precision of real numbers !*   global_vars.inc         - general global variables and parameters !*   global_mpi.inc          - global mpi variables !*   global_io.inc           - global IO variables !*   global_mesh.inc         - global mesh variables !*   global_element.inc      - global primary structures !* CONTAINS !*   Subroutine              Purpose !*   clean_up()              - deallocated global structures/variables !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/20: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/20 !> Module for global variables, included library, and common code !=============================================================================== !-- implicit none, and include files for variables and libraries -- #include <petsc/finclude/petsc.h> USE petsc IMPLICIT NONE #include \"global_precision.inc\" #include \"global_vars.inc\" #include \"global_mpi.inc\" #include \"global_io.inc\" #include \"global_mesh.inc\" #include \"global_element.inc\" #include \"global_petsc.inc\" !=============================================================================== CONTAINS !=============================================================================== ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* common_mod/clean_up !* SYNOPSIS SUBROUTINE clean_up () !* PURPOSE !*   Deallocate global structures/variables !* SIDE EFFECTS !*   Global structure (lcl_complex) deallocated !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/21: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/21 !> Deallocate global structures/variables !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- deallocate primary elements -- IF ( ALLOCATED ( lcl_complex )) DEALLOCATE ( lcl_complex ) RETURN END SUBROUTINE ! clean_up !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! END MODULE ! common_mod !=============================================================================== !","tags":"","loc":"sourcefile/common_mod.f90.html","title":"common_mod.f90 – ParaGEMS"},{"text":"Tests for common_mod module, primairly global variables This file depends on sourcefile~~test_common_mod.f90~~EfferentGraph sourcefile~test_common_mod.f90 test_common_mod.f90 sourcefile~common_mod.f90 common_mod.f90 sourcefile~test_common_mod.f90:->sourcefile~common_mod.f90: Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Files dependent on this one sourcefile~~test_common_mod.f90~~AfferentGraph sourcefile~test_common_mod.f90 test_common_mod.f90 sourcefile~paragems_unit.f90 paragems_unit.f90 sourcefile~paragems_unit.f90:->sourcefile~test_common_mod.f90: Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules test_common_mod Source Code test_common_mod.f90 Source Code ! !=============================================================================== !-- Common Module Tests !> Tests for common_mod module, primairly global variables !=============================================================================== !/****/h* modules|common/test_common_mod !* SYNOPSIS MODULE test_common_mod !* PURPOSE !*   Tests for common_mod module, primairly global variables !* INCLUDES !*   common_mod !* CONTAINS !*   Subroutine              Purpose !*   test_vars(cnt)          - deallocated global structures/variables !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/21: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/21 !> Tests for common_mod module, primairly global variables !=============================================================================== !----------------------------------------------------------------------------- ! use statements and implicit none !----------------------------------------------------------------------------- USE common_mod IMPLICIT NONE !=============================================================================== CONTAINS !=============================================================================== ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! /****s*/ /src/common/common_test|test_vars !* SYNOPSIS SUBROUTINE test_vars ( cnt ) !* PURPOSE !*   test global structures/variables !* INPUTS !*   test counter !* OUTPUTS !*   test counter !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/21: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/21 !> deallocate global structures/variables !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! TO DO: ! - global variables? ! - primary element data structure ! - clean up !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !--------------------------------------------------------------------- ! arguments !--------------------------------------------------------------------- INTEGER , INTENT ( INOUT ) :: cnt !> test counter !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !--------------------------------------------------------------------- ! 1. precision !--------------------------------------------------------------------- cnt = cnt + 1 IF ( 1.d0 - 9.999999999999999d-1 > 0.d0 ) THEN WRITE ( * , '(A,I5,A)' ) 'common_test : ' , cnt ,& ' : v/      PASSED : precision accurate to 16 decimal places' ELSE WRITE ( * , '(A,I5,A)' ) 'common_test : ' , cnt ,& ' : XXXXXX  FAILED : precision NOT accurate to 16 decimal places' END IF !--------------------------------------------------------------------- ! 2. global variables ! 3. global mpi ! 4. global io ! 5. global mesh ! 6. primary element ! 7. variable clean up !--------------------------------------------------------------------- END SUBROUTINE ! common_test|test_vars !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! END MODULE ! common_test !=============================================================================== !","tags":"","loc":"sourcefile/test_common_mod.f90.html","title":"test_common_mod.f90 – ParaGEMS"},{"text":"Module containing routines for performing DEC operations This file depends on sourcefile~~dec_mod.f90~~EfferentGraph sourcefile~dec_mod.f90 dec_mod.f90 sourcefile~common_mod.f90 common_mod.f90 sourcefile~dec_mod.f90:->sourcefile~common_mod.f90: sourcefile~math_mod.f90 math_mod.f90 sourcefile~dec_mod.f90:->sourcefile~math_mod.f90: sourcefile~mpi_mod.f90 mpi_mod.f90 sourcefile~dec_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~math_mod.f90:->sourcefile~common_mod.f90: sourcefile~math_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~mpi_mod.f90:->sourcefile~common_mod.f90: Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Files dependent on this one sourcefile~~dec_mod.f90~~AfferentGraph sourcefile~dec_mod.f90 dec_mod.f90 sourcefile~test_dec_mod.f90 test_dec_mod.f90 sourcefile~test_dec_mod.f90:->sourcefile~dec_mod.f90: sourcefile~partition_mod.f90 partition_mod.f90 sourcefile~partition_mod.f90:->sourcefile~dec_mod.f90: sourcefile~io_mod.f90 io_mod.f90 sourcefile~partition_mod.f90:->sourcefile~io_mod.f90: sourcefile~io_mod.f90:->sourcefile~dec_mod.f90: sourcefile~darcy.f90 darcy.f90 sourcefile~darcy.f90:->sourcefile~partition_mod.f90: sourcefile~darcy_mod.f90 darcy_mod.f90 sourcefile~darcy.f90:->sourcefile~darcy_mod.f90: sourcefile~darcy_1f.f90 darcy_1f.f90 sourcefile~darcy_1f.f90:->sourcefile~partition_mod.f90: sourcefile~darcy_1f.f90:->sourcefile~darcy_mod.f90: sourcefile~darcy_crkp_2f_old.f90 darcy_crkp_2f_old.f90 sourcefile~darcy_crkp_2f_old.f90:->sourcefile~partition_mod.f90: sourcefile~darcy_crkp_2f_old.f90:->sourcefile~darcy_mod.f90: sourcefile~darcy_crkp_2f.f90 darcy_crkp_2f.f90 sourcefile~darcy_crkp_2f.f90:->sourcefile~partition_mod.f90: sourcefile~darcy_crkp_2f.f90:->sourcefile~darcy_mod.f90: sourcefile~darcy_mod.f90:->sourcefile~io_mod.f90: sourcefile~test_io_mod.f90 test_io_mod.f90 sourcefile~test_io_mod.f90:->sourcefile~io_mod.f90: sourcefile~test_partition_mod.f90 test_partition_mod.f90 sourcefile~test_partition_mod.f90:->sourcefile~partition_mod.f90: sourcefile~darcy_crkp_1f.f90 darcy_crkp_1f.f90 sourcefile~darcy_crkp_1f.f90:->sourcefile~partition_mod.f90: sourcefile~darcy_crkp_1f.f90:->sourcefile~darcy_mod.f90: sourcefile~darcy_2f.f90 darcy_2f.f90 sourcefile~darcy_2f.f90:->sourcefile~partition_mod.f90: sourcefile~darcy_2f.f90:->sourcefile~darcy_mod.f90: sourcefile~test_darcy_mod.f90 test_darcy_mod.f90 sourcefile~test_darcy_mod.f90:->sourcefile~darcy_mod.f90: Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules dec_mod Source Code dec_mod.f90 Source Code ! !=============================================================================== !-- DEC Module !> Module containing routines for performing DEC operations !=============================================================================== !/****/h* modules|dec/dec_mod !* SYNOPSIS MODULE dec_mod !* PURPOSE !*   Module containing routines for performing DEC operations !* INCLUDES !*   Name                    Purpose !*   common_mod              variable definitions !*   mpi_mod                 ??? !*   math_mod                basic math functions !* CONTAINS !*   Subroutine              Purpose !*   calc_orientation        sort nodal indices and compute ±1 orientation !*   calc_bndry_cobndry      recursively compute element (co-)boundaries !*   build_bndry_work_array  build boundary data working array !*   count_bndry_cobndry     count [co-]boundaries: internal, external, surface !*   set_bndry_cobndry       set [co-]boundaries: internal, external, surface !*   initialise_geo          initialise geometric quantities for the given mesh !*   get_lcl_node_indx       create map between global and local node indices !*   calc_circumcenters      compute circumcenter of given elements !*   calc_prml_sgnd_vlm      compute signed volume of primal elements !*   calc_prml_unsgnd_vlm    compute unsigned volume of primal elements !*   calc_dual_vlm           compute volume for dual elements of all geometric order !*   calc_dual_vlm_i         recursively add to dual volume calculation !*   add_points_i            recursively adds points for dual volume calculation !*   calc_unsgnd_vlm         compute unsigned volume for given set of points !*   exchange_dual_vlm       exchange external dual volumes between adjacent processes !*   exchange_dual_dir       exchange external dual edge directions between adjacent processes !*   calc_prml_unsgnd_vlm    compute unsigned volume of primal elements !*   calc_hodge_star         compute hodge star and it's inverse from primal and dual volumes !*   calc_prml_dir           compute the unit direction of primal edges !*   calc_dual_dir           compute the unit direction of dual edges !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2020/09/16: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2020/09/16 !> Module containing routines for performing DEC operations !=============================================================================== !-- use statements and implicit none -- USE common_mod USE mpi_mod USE math_mod IMPLICIT NONE !=============================================================================== CONTAINS !=============================================================================== ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* dec_mod/calc_orientation !* SYNOPSIS SUBROUTINE calc_orientation ( elm , orientation ) !* PURPOSE !*   Sort nodal indices and compute ± orientation of an element !* INPUTS !*   Name                    Description !*   elem !*   orientation !* OUTPUTS !*   Name                    Description !*   elem !*   orientation !* SIDE EFFECTS !*   - the nodal indices of elm are sorted numerically in ascending order !*   - orientation contains the ±1 orientation of the element !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !* NOTES !*   Procedure adapted from: !*   https://rosettacode.org/wiki/Sorting_algorithms/Insertion_sort#Fortran !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Sort nodal indices and compute ± orientation of an element !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- INTEGER , INTENT ( INOUT ) :: elm (:) !> nodal indices of elements INTEGER , INTENT ( INOUT ) :: orientation !> orientations of the elements !-- local variables -- INTEGER :: i , j !> loop counters INTEGER :: length !> size of elm array INTEGER :: work !> work variable !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- check size of argument -- length = SIZE ( elm , DIM = 1 ) !------------------------------------------------------------------------- !-- sort indices using insertion sort and count the number of swaps -- !------------------------------------------------------------------------- orientation = 0 !-- loop through all indices, starting from the second -- DO i = 2 , length !-- get next value to insert -- work = elm ( i ) j = i - 1 !-- find location to insert given value -- DO WHILE ( j >= 1 ) !-- ??? found location ??? -- IF ( elm ( j ) <= work ) EXIT !-- shuffle (swap) values -- elm ( j + 1 ) = elm ( j ) j = j - 1 !-- count swaps -- orientation = orientation + 1 END DO !-- insert value -- elm ( j + 1 ) = work END DO !-- compute orientation from number of swaps -- orientation = 1 - 2 * MOD ( orientation , 2 ) RETURN END SUBROUTINE ! dec_mod/calc_orientation !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* dec_mod/calc_bndry_cobndry !* SYNOPSIS SUBROUTINE calc_bndry_cobndry () !* PURPOSE !*   Recursively compute element (co-)boundaries from highest to lowest geometric order !* ASSUMPTION !*   Mesh is a simplicial complex in TetGen format !* SIDE EFFECTS !*   - the nodal indices of elm are sorted numerically in ascending order !*   - orientation contains the ±1 orientation of the element !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2020/09/16: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/20 !> Recursively compute element (co-)boundaries from highest to lowest geometric order !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- local variables -- INTEGER :: k !> simplicial order INTEGER , ALLOCATABLE :: bndry (:,:) !> temp. boundary work array INTEGER , ALLOCATABLE :: bndry_cnt (:) !> counter local boundaries INTEGER , ALLOCATABLE :: cobndry_cnt (:) !> counter co-boundaries INTEGER :: cnt !> counter boundaries INTEGER :: ext_cnt ( 2 ) !> counter external boundaries INTEGER :: surf_cnt !> counter surface boundaries INTEGER , ALLOCATABLE :: work (:,:) !> work array for merge sort LOGICAL , ALLOCATABLE :: lwork (:) !> work array to determine locality !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- calculate signed adjacency [co-boundary operator] -- !-- loop for each geometric order in complex in descending order -- DO k = dim_cmplx + 1 , 2 , - 1 CALL syncwrite_log ( '===============================================================' ) !-- initial guess for the number of boundaries -- num_elm ( k - 1 ) = k * num_elm ( k ) !-- build working array with element/boundary data -- CALL syncwrite_log ( '> parallel_setup() - calc_bndry_cobndry ' // & '- build_bndry_work_array' ) ALLOCATE ( bndry ( num_elm ( k - 1 ), k + 2 )) CALL build_bndry_work_array ( bndry , k ); CALL syncwrite_log_time () !-- sort working array -- CALL syncwrite_log ( '> parallel_setup() - calc_bndry_cobndry ' // & '- int_merge_sort_rows' ) ALLOCATE ( work ( num_elm ( k - 1 ), k + 2 )) CALL int_merge_sort_rows ( bndry , num_elm ( k - 1 ), 1 , k - 1 , work ) DEALLOCATE ( work ); CALL syncwrite_log_time () !-- count number of unique (local/external/surface) [co-]boundaries -- CALL syncwrite_log ( '> parallel_setup() - calc_bndry_cobndry ' // & '- count_bndry_cobndry' ) ALLOCATE ( bndry_cnt ( num_elm ( k )), cobndry_cnt ( num_elm ( k - 1 )),& lwork ( num_elm ( k ) * k )) CALL count_bndry_cobndry ( bndry , k , cnt , bndry_cnt , cobndry_cnt , ext_cnt ,& surf_cnt , lwork ); CALL syncwrite_log_time () !-- allocate [co-]boundary structures and variables -- CALL syncwrite_log ( '> parallel_setup() - calc_bndry_cobndry ' // & '- allocate_bndry_cobndry' ) CALL allocate_bndry_cobndry ( k , cnt , bndry_cnt , cobndry_cnt , ext_cnt , surf_cnt ) CALL syncwrite_log_time () !-- setup boundaries external to the current process (not local), !   setup parallel mapping, setup node indices for (k-1)&#94;th order element !   structure, and setup co-boundaries for (k-1)&#94;th order element !   structure -- CALL syncwrite_log ( '> parallel_setup() - calc_bndry_cobndry ' // & '- set_bndry_cobndry' ) CALL set_bndry_cobndry ( bndry , k , cnt , bndry_cnt , cobndry_cnt , ext_cnt ,& surf_cnt , lwork ); CALL syncwrite_log_time () !-- clean up temporary variables -- DEALLOCATE ( bndry , bndry_cnt , cobndry_cnt , lwork ) END DO CALL syncwrite_log ( '===============================================================' ) RETURN END SUBROUTINE ! dec_mod/calc_bndry_cobndry !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* dec_mod/build_bndry_work_array !* SYNOPSIS SUBROUTINE build_bndry_work_array ( bndry , k ) !* PURPOSE !*   Build boundary data working array !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Build boundary data working array !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- INTEGER , INTENT ( INOUT ) :: bndry (:,:) !> boundary work array INTEGER , INTENT ( IN ) :: k !> simplicial order !-- local variables -- INTEGER :: i , j !> loop index INTEGER :: iib , fib !> array initial/final index !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ iib = 1 ; fib = num_elm ( k ) DO i = 1 , k !-- get boundary node indices -- bndry ( iib : fib , 1 : i - 1 ) = lcl_complex ( k )% node_indx (:, 1 : i - 1 ) bndry ( iib : fib , i : k - 1 ) = lcl_complex ( k )% node_indx (:, i + 1 : k ) !-- get simplex indices -- bndry ( iib : fib , k ) = ( / ( j , j = 1 , num_elm ( k )) / ) bndry ( iib : fib , k + 1 ) = i !-- get orientation -- IF ( k == dim_cmplx + 1 ) THEN bndry ( iib : fib , k + 2 ) = ( - 1 ) ** i * lcl_complex ( k )% orientation ELSE bndry ( iib : fib , k + 2 ) = ( - 1 ) ** i END IF !-- update array indices -- iib = iib + num_elm ( k ) fib = fib + num_elm ( k ) END DO RETURN END SUBROUTINE ! dec_mod/build_bndry_work_array !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* dec_mod/count_bndry_cobndry !* SYNOPSIS SUBROUTINE count_bndry_cobndry ( bndry , k , cnt , bndry_cnt , cobndry_cnt , ext_cnt ,& surf_cnt , local ) !* PURPOSE !*   Count [co-]boundaries: internal, external, surface !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Count [co-]boundaries: internal, external, surface !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- INTEGER , INTENT ( IN ) :: k !> simplicial order INTEGER , INTENT ( IN ) :: bndry (:,:) !> boundary work array INTEGER , INTENT ( INOUT ) :: bndry_cnt (:) !> counter local boundaries INTEGER , INTENT ( INOUT ) :: cobndry_cnt (:) !> counter co-boundaries INTEGER , INTENT ( INOUT ) :: cnt !> counter (k-1) element INTEGER , INTENT ( INOUT ) :: ext_cnt ( 2 ) !> counter external boundaries INTEGER , INTENT ( INOUT ) :: surf_cnt !> counter surface boundaries LOGICAL , INTENT ( INOUT ) :: local (:) !> locality work array !-- local variables -- INTEGER :: i !> counter !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- reset counters -- cnt = 0 ; bndry_cnt = 0 ; cobndry_cnt = 0 ; ext_cnt = 0 ; surf_cnt = 0 !-- loop for all boundaries -- DO i = 1 , num_elm ( k ) * k IF ( i == 1 . OR . ANY ( bndry ( i , 1 : k - 1 ) /= bndry ( max ( 1 , i - 1 ), 1 : k - 1 ))) THEN !-- boundary different from previous: new boundary -- cnt = cnt + 1 IF ( any_element_in_list ( bndry ( i , 1 : k - 1 ), lcl_complex ( 1 )% glb_indx )) THEN !-- boundary is local -- local ( i ) = . TRUE . IF ( ANY ( bndry ( i , 1 : k - 1 ) /= bndry ( min ( i + 1 , num_elm ( k ) * k ), 1 : k - 1 )) . OR . & i == num_elm ( k ) * k ) THEN !-- boundary is unique: count surface boundaries -- surf_cnt = surf_cnt + 1 ELSE !-- boundary is NOT unique: count local boundaries -- bndry_cnt ( bndry ( i , k )) = bndry_cnt ( bndry ( i , k )) + 1 END IF !-- count number of co-boundaries for each local boundary -- cobndry_cnt ( cnt ) = cobndry_cnt ( cnt ) + 1 ELSE local ( i ) = . FALSE . !-- boundary is non-local: count external boundaries -- ext_cnt = ext_cnt + 1 END IF ELSE !-- boundary same as previous -- IF ( any_element_in_list ( bndry ( i , 1 : k - 1 ), lcl_complex ( 1 )% glb_indx )) THEN !-- boundary is local -- local ( i ) = . TRUE . !-- count local boundaries -- bndry_cnt ( bndry ( i , k )) = bndry_cnt ( bndry ( i , k )) + 1 !-- count number of co-boundaries for each local boundary -- cobndry_cnt ( cnt ) = cobndry_cnt ( cnt ) + 1 ELSE local ( i ) = . FALSE . !-- boundary is non-local: count external boundaries -- ext_cnt ( 1 ) = ext_cnt ( 1 ) + 1 END IF END IF END DO RETURN END SUBROUTINE ! dec_mod/count_bndry_cobndry !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* dec_mod/allocate_bndry_cobndry !* SYNOPSIS SUBROUTINE allocate_bndry_cobndry ( k , cnt , bndry_cnt , cobndry_cnt , ext_cnt ,& surf_cnt ) !* PURPOSE !*   Allocate [co-]boundaries structures/variables !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Allocate [co-]boundaries structures/variables !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- INTEGER , INTENT ( IN ) :: k !> simplicial order INTEGER , INTENT ( IN ) :: bndry_cnt (:) !> temp # local boundaries INTEGER , INTENT ( IN ) :: cobndry_cnt (:) !> temp # co-boundaries INTEGER , INTENT ( IN ) :: cnt !> counter (k-1) element INTEGER , INTENT ( IN ) :: ext_cnt ( 2 ) !> counter external boundaries INTEGER , INTENT ( IN ) :: surf_cnt !> counter surface boundaries !-- local variables -- INTEGER :: i !> counter !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- allocate boundary structures and variables: k&#94;th order -- !-- # external boundaries and # expect MPI receives -- lcl_complex ( k )% num_ext = ext_cnt ( 1 ) lcl_complex ( k )% num_recv = ext_cnt ( 2 ) ALLOCATE (& lcl_complex ( k )% ext_indx ( ext_cnt ( 1 ), 3 ),& lcl_complex ( k )% recv_indx ( ext_cnt ( 2 ), 2 )) !-- # surface boundaries -- lcl_complex ( k )% num_surf = surf_cnt ALLOCATE ( lcl_complex ( k )% surf_indx ( surf_cnt , 4 )) !-- # internal boundaries -- ALLOCATE (& lcl_complex ( k )% num_bndry ( num_elm ( k )),& lcl_complex ( k )% bndry ( num_elm ( k ))) lcl_complex ( k )% num_bndry = bndry_cnt DO i = 1 , num_elm ( k ) ALLOCATE (& lcl_complex ( k )% bndry ( i )% sgn ( lcl_complex ( k )% num_bndry ( i )),& lcl_complex ( k )% bndry ( i )% indx ( lcl_complex ( k )% num_bndry ( i ))) END DO !-- allocate [co-]boundary structures and variables: (k-1))&#94;th order -- !-- # (k-1))&#94;th order elements -- num_elm ( k - 1 ) = cnt ALLOCATE ( lcl_complex ( k - 1 )% node_indx ( num_elm ( k - 1 ), k - 1 )) !-- # (k-1))&#94;th order coboundaries -- ALLOCATE (& lcl_complex ( k - 1 )% num_cobndry ( num_elm ( k - 1 )),& lcl_complex ( k - 1 )% cobndry ( num_elm ( k - 1 ))) lcl_complex ( k - 1 )% num_cobndry = cobndry_cnt ( 1 : num_elm ( k - 1 )) DO i = 1 , num_elm ( k - 1 ) ALLOCATE (& lcl_complex ( k - 1 )% cobndry ( i )% sgn ( cobndry_cnt ( i )),& lcl_complex ( k - 1 )% cobndry ( i )% indx ( cobndry_cnt ( i ))) END DO RETURN END SUBROUTINE ! dec_mod/allocate_bndry_cobndry !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* dec_mod/set_bndry_cobndry !* SYNOPSIS SUBROUTINE set_bndry_cobndry ( bndry , k , cnt , bndry_cnt , cobndry_cnt , ext_cnt , surf_cnt , local ) !* PURPOSE !*   Set [co-]boundaries: internal, external, surface !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Set [co-]boundaries: internal, external, surface !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- INTEGER , INTENT ( IN ) :: k !> simplicial order INTEGER , INTENT ( INOUT ) :: bndry (:,:) !> boundary work array INTEGER , INTENT ( INOUT ) :: bndry_cnt (:) !> temp # local boundaries INTEGER , INTENT ( INOUT ) :: cobndry_cnt (:) !> temp # co-boundaries INTEGER , INTENT ( INOUT ) :: cnt !> counter (k-1) element INTEGER , INTENT ( INOUT ) :: ext_cnt ( 2 ) !> counter external boundaries INTEGER , INTENT ( INOUT ) :: surf_cnt !> counter surface boundaries LOGICAL , INTENT ( INOUT ) :: local (:) !> locality work array !-- local variables -- INTEGER :: i !> counter !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- reset counters -- cnt = 0 ; bndry_cnt = 0 ; cobndry_cnt = 0 ; ext_cnt = 0 ; surf_cnt = 0 !-- loop for all boundaries -- DO i = 1 , num_elm ( k ) * k IF ( i == 1 . OR . ANY ( bndry ( i , 1 : k - 1 ) /= bndry ( max ( 1 , i - 1 ), 1 : k - 1 ))) THEN !-- boundary different from previous: new boundary -- cnt = cnt + 1 IF ( local ( i )) THEN !-- boundary is local -- IF ( ANY ( bndry ( i , 1 : k - 1 ) /= bndry ( min ( i + 1 , num_elm ( k ) * k ), 1 : k - 1 )) . OR . & i == num_elm ( k ) * k ) THEN !-- boundary is unique: set signed adjacency for surface boundaries -- surf_cnt = surf_cnt + 1 lcl_complex ( k )% surf_indx ( surf_cnt ,:) = ( / - bndry ( i , k + 2 ), cnt , bndry ( i , k ) / ) ELSE !-- boundary is NOT unique: set signed adjacency for local boundaries -- bndry_cnt ( bndry ( i , k )) = bndry_cnt ( bndry ( i , k )) + 1 lcl_complex ( k )% bndry ( bndry ( i , k ))% sgn ( bndry_cnt ( bndry ( i , k ))) = bndry ( i , k + 2 ) lcl_complex ( k )% bndry ( bndry ( i , k ))% indx ( bndry_cnt ( bndry ( i , k ))) = cnt END IF !-- set signed adjacency for co-boundaries for each local boundary -- cobndry_cnt ( cnt ) = 1 lcl_complex ( k - 1 )% cobndry ( cnt )% sgn ( 1 ) = bndry ( i , k + 2 ) lcl_complex ( k - 1 )% cobndry ( cnt )% indx ( 1 ) = bndry ( i , k ) ELSE !-- boundary is non-local: set signed adjacency for external boundaries -- ext_cnt = ext_cnt + 1 lcl_complex ( k )% ext_indx ( ext_cnt ( 1 ),:) = ( / bndry ( i , k + 2 ), cnt , bndry ( i , k ) / ) lcl_complex ( k )% recv_indx ( ext_cnt ( 2 ),:) = ( / elm2proc ( bndry ( i , 1 )), cnt / ) !????? find actual global index END IF !-- set node indices for (k-1)&#94;th order elements -- lcl_complex ( k - 1 )% node_indx ( cnt ,:) = bndry ( i , 1 : k - 1 ) ELSE !-- boundary same as previous -- IF ( local ( i )) THEN !-- boundary is local: set signed adjacency for local boundaries -- bndry_cnt ( bndry ( i , k )) = bndry_cnt ( bndry ( i , k )) + 1 lcl_complex ( k )% bndry ( bndry ( i , k ))% sgn ( bndry_cnt ( bndry ( i , k ))) = bndry ( i , k + 2 ) lcl_complex ( k )% bndry ( bndry ( i , k ))% indx ( bndry_cnt ( bndry ( i , k ))) = cnt !-- set signed adjacency for co-boundaries for each local boundary -- cobndry_cnt ( cnt ) = cobndry_cnt ( cnt ) + 1 lcl_complex ( k - 1 )% cobndry ( cnt )% sgn ( cobndry_cnt ( cnt )) = bndry ( i , k + 2 ) lcl_complex ( k - 1 )% cobndry ( cnt )% indx ( cobndry_cnt ( cnt )) = bndry ( i , k ) ELSE !-- boundary is non-local: set signed adjacency for external boundaries -- ext_cnt ( 1 ) = ext_cnt ( 1 ) + 1 lcl_complex ( k )% ext_indx ( ext_cnt ( 1 ),:) = ( / bndry ( i , k + 2 ), cnt , bndry ( i , k ) / ) END IF END IF END DO RETURN END SUBROUTINE ! dec_mod/set_bndry_cobndry !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* dec_mod/initialise_geo !* SYNOPSIS SUBROUTINE initialise_geo () !* PURPOSE !*   Initialise geometric quantities for the given mesh !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Initialise geometric quantities for the given mesh !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- local variables -- INTEGER :: i , k !> loop counters REAL ( KIND = iwp ) :: minV , maxV , absminV !> min/max volumes CHARACTER ( LEN = slen ) :: str !> string for writing to log file !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ CALL syncwrite_log ( '> initialise_geo()' ) curnt_time = MPI_WTIME () !-- log element information -- DO k = 1 , dim_cmplx + 1 write ( str , '(A,I1,A,I10)' ) '   - number of C' , k - 1 , ': ' , glb_num_elm ( k ) CALL rootwrite_log ( str ) END DO !-- get local node indices -- CALL get_lcl_node_indx () !-- compute circumcenters -- DO k = 2 , dim_cmplx + 1 CALL calc_circumcenters ( k ) END DO !-- compute signed primal volumes -- IF (. NOT . ALLOCATED ( lcl_complex ( 1 )% prml_volume )) & ALLOCATE ( lcl_complex ( 1 )% prml_volume ( num_elm ( 1 ))) lcl_complex ( 1 )% prml_volume = 1 DO k = 2 , dim_cmplx + 1 IF ( k - 1 == dim_embbd ) THEN CALL calc_prml_sgnd_vlm ( k ) ELSE CALL calc_prml_unsgnd_vlm ( k ) END IF END DO !-- compute signed dual volumes -- CALL calc_dual_vlm () DO k = 1 , dim_cmplx CALL exchange_dual_vlm ( k ) END DO !-- assess maximumm and minimum volumes -- DO k = 1 , dim_cmplx + 1 CALL MPI_REDUCE ( minval ( lcl_complex ( k )% prml_volume ), minV , 1 ,& MPI_DOUBLE_PRECISION , MPI_MIN , 0 , MPI_COMM_WORLD , ier ) CALL MPI_REDUCE ( minval ( abs ( lcl_complex ( k )% prml_volume )), absminV , 1 ,& MPI_DOUBLE_PRECISION , MPI_MIN , 0 , MPI_COMM_WORLD , ier ) CALL MPI_REDUCE ( maxval ( lcl_complex ( k )% prml_volume ), maxV , 1 ,& MPI_DOUBLE_PRECISION , MPI_MAX , 0 , MPI_COMM_WORLD , ier ) IF ( rank == root ) & write ( str , '(A,I1,A,ES10.3,A,ES10.3,A,ES10.3)' ) '   - The min/abs(min)/max C' , k - 1 ,& ' volume is: ' , minV , '/' , absminV , '/' , maxV CALL syncwrite_log ( str ) CALL MPI_REDUCE ( minval ( lcl_complex ( k )% dual_volume ), minV , 1 ,& MPI_DOUBLE_PRECISION , MPI_MIN , 0 , MPI_COMM_WORLD , ier ) CALL MPI_REDUCE ( minval ( abs ( lcl_complex ( k )% dual_volume )), absminV , 1 ,& MPI_DOUBLE_PRECISION , MPI_MIN , 0 , MPI_COMM_WORLD , ier ) CALL MPI_REDUCE ( maxval ( lcl_complex ( k )% dual_volume ), maxV , 1 ,& MPI_DOUBLE_PRECISION , MPI_MAX , 0 , MPI_COMM_WORLD , ier ) IF ( rank == root ) & write ( str , '(A,I1,A,ES10.3,A,ES10.3,A,ES10.3)' ) '   - The min/abs(min)/max D' , dim_cmplx + 1 - k ,& ' volume is: ' , minV , '/' , absminV , '/' , maxV CALL syncwrite_log ( str ) END DO !-- compute Hodge stars -- DO k = 1 , dim_cmplx + 1 CALL calc_hodge_star ( k ) END DO !-- compute primal and dual edge directions -- CALL calc_prml_dir () CALL calc_dual_dir () !-- log time to initialise the geometric information -- CALL syncwrite_log_time () RETURN END SUBROUTINE ! dec_mod/initialise_geo !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* dec_mod/get_lcl_node_indx !* SYNOPSIS SUBROUTINE get_lcl_node_indx () !* PURPOSE !*   Create map between global and local node indices !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Create map between global and local node indices !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- local variables -- INTEGER :: i , k , m , n !> loop counters !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- allocate array for map and assign default value -- DO k = 2 , dim_cmplx + 1 IF (. NOT . ALLOCATED ( lcl_complex ( k )% lcl_node_indx )) & ALLOCATE ( lcl_complex ( k )% lcl_node_indx ( num_elm ( k ), k )) lcl_complex ( k )% lcl_node_indx = - 1 END DO !-- build map -- DO k = 2 , dim_cmplx + 1 !-- simplicial order -- DO i = 1 , num_elm ( 1 ) !-- local node index -- DO n = 1 , k !-- element node index -- DO m = 1 , num_elm ( k ) !-- element index -- IF ( lcl_complex ( k )% node_indx ( m , n ) == lcl_complex ( 1 )% glb_indx ( i )) & lcl_complex ( k )% lcl_node_indx ( m , n ) = i END DO END DO END DO END DO RETURN END SUBROUTINE ! dec_mod/get_lcl_node_indx !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* dec_mod/calc_circumcenters !* SYNOPSIS SUBROUTINE calc_circumcenters ( k ) !* PURPOSE !*   Compute circumcenter of given elements !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Compute circumcenter of given elements !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- INTEGER :: k !> simplicial order !-- local variables -- INTEGER :: kp !> simplicial order plus one INTEGER :: i , j !> loop indices REAL ( KIND = iwp ), ALLOCATABLE :: A (:,:), b (:) !> solution variables REAL ( KIND = iwp ), ALLOCATABLE :: pts (:,:) !> bounding points REAL ( KIND = iwp ), ALLOCATABLE :: ipiv (:), work (:) !> work arrays !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- allocate centers and temp. variables -- kp = k + 1 IF (. NOT . ALLOCATED ( lcl_complex ( k )% centers )) & ALLOCATE ( lcl_complex ( k )% centers ( num_elm ( k ), dim_embbd )) ALLOCATE ( pts ( k , dim_embbd ), A ( kp , kp ), b ( kp ), ipiv ( kp ), work ( kp )) !-- compute circumcenters for all elements -- !-- loop through all elements of this order -- DO i = 1 , num_elm ( k ) !-- get bounding points of the element -- DO j = 1 , k pts ( j ,:) = lcl_complex ( 1 )% centers ( lcl_complex ( k )% lcl_node_indx ( i , j ),:) END DO !-- setup system to compute circumcenter -- A ( 1 : k , 1 : k ) = 2 * MATMUL ( pts , TRANSPOSE ( pts )) A ( 1 : k , kp ) = 1 ; A ( kp , 1 : k ) = 1 ; A ( kp , kp ) = 0 b ( 1 : k ) = sum ( pts * pts , DIM = 2 ) b ( kp ) = 1 !-- solve system to compute circumcenter -- CALL DSYSV ( 'L' , kp , 1 , A , kp , ipiv , b , kp , work , kp , ier ) !-- assign circumcenter -- lcl_complex ( k )% centers ( i ,:) = MATMUL ( b ( 1 : k ), pts ) END DO !-- clean up -- DEALLOCATE ( pts , A , b , ipiv , work ) RETURN END SUBROUTINE ! dec_mod/calc_circumcenters !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* dec_mod/calc_prml_sgnd_vlm !* SYNOPSIS SUBROUTINE calc_prml_sgnd_vlm ( k ) !* PURPOSE !*   Compute signed volume of primal elements !* INPUTS !*   Name                    Description !*   k !* OUTPUTS !*   Name                    Description !*   k !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Compute signed volume of primal elements !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- INTEGER :: k !> simplicial order !-- local variables -- INTEGER :: i , j !> loop counters INTEGER :: fac !> factorial REAL ( KIND = iwp ), ALLOCATABLE :: A (:,:) !> solution variable REAL ( KIND = iwp ), ALLOCATABLE :: pts (:) !> bounding points !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- allocate volumes and temp. variables -- IF (. NOT . ALLOCATED ( lcl_complex ( k )% prml_volume )) & ALLOCATE ( lcl_complex ( k )% prml_volume ( num_elm ( k ))) ALLOCATE ( pts ( dim_embbd ), A ( dim_embbd , dim_embbd )) !-- compute factorial of embedding dimension -- fac = 1 DO i = 2 , dim_embbd fac = fac * i END DO !-- compute signed volumes -- !-- loop through all elements -- DO i = 1 , num_elm ( k ) !-- get a bounding point of the element -- pts (:) = lcl_complex ( 1 )% centers ( lcl_complex ( k )% lcl_node_indx ( i , 1 ),:) !-- build system matrix from bounding points of the element -- DO j = 1 , k - 1 A ( j ,:) = lcl_complex ( 1 )% centers ( lcl_complex ( k )% lcl_node_indx ( i , j + 1 ),:) - pts END DO !-- compute and assign signed volume -- lcl_complex ( k )% prml_volume ( i ) = lcl_complex ( k )% orientation ( i ) * determinant ( A , dim_embbd ) / fac END DO !-- clean up -- DEALLOCATE ( pts , A ) RETURN END SUBROUTINE ! dec_mod/calc_prml_sgnd_vlm !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* dec_mod/calc_prml_unsgnd_vlm !* SYNOPSIS SUBROUTINE calc_prml_unsgnd_vlm ( k ) !* PURPOSE !*   Compute unsigned volume of primal elements !* INPUTS !*   Name                    Description !*   k !* OUTPUTS !*   Name                    Description !*   k !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Compute unsigned volume of primal elements !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- INTEGER :: k !> loop counters !-- local variables -- INTEGER :: i , j !> loop counters INTEGER :: fac !> factorial REAL ( KIND = iwp ), ALLOCATABLE :: A (:,:) !> solution variable REAL ( KIND = iwp ), ALLOCATABLE :: pts (:) !> bounding points !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- allocate volumes and temp. variables -- IF (. NOT . ALLOCATED ( lcl_complex ( k )% prml_volume )) & ALLOCATE ( lcl_complex ( k )% prml_volume ( num_elm ( k ))) ALLOCATE ( pts ( dim_embbd ), A ( k - 1 , dim_embbd )) !-- compute necessary factorial -- fac = 1 DO i = 2 , k - 1 fac = fac * i END DO !-- compute unsigned volumes -- !-- loop for all elements -- DO i = 1 , num_elm ( k ) !-- get a bounding point of the element -- pts (:) = lcl_complex ( 1 )% centers ( lcl_complex ( k )% lcl_node_indx ( i , 1 ),:) !-- build system matrix from remaining bounding points of the element -- DO j = 1 , k - 1 A ( j ,:) = lcl_complex ( 1 )% centers ( lcl_complex ( k )% lcl_node_indx ( i , j + 1 ),:) - pts END DO !-- compute and assign unsigned volume -- lcl_complex ( k )% prml_volume ( i ) = SQRT ( ABS ( determinant ( MATMUL ( A , TRANSPOSE ( A )), k - 1 ))) / fac END DO !-- clean up -- DEALLOCATE ( pts , A ) RETURN END SUBROUTINE ! dec_mod/calc_prml_unsgnd_vlm !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* dec_mod/calc_dual_vlm !* SYNOPSIS SUBROUTINE calc_dual_vlm () !* PURPOSE !*   Compute volume for dual elements of all geometric order !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Compute volume for dual elements of all geometric order !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- local variables -- INTEGER :: k !> simplicial order INTEGER :: i , indx !> loop counters REAL ( KIND = iwp ), ALLOCATABLE :: pts (:,:) !> bounding points REAL ( KIND = iwp ), ALLOCATABLE :: sgn (:) !> sign of volume elements !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- allocate volumes and temp. variables -- DO k = 1 , dim_cmplx + 1 IF (. NOT . ALLOCATED ( lcl_complex ( k )% dual_volume )) & ALLOCATE ( lcl_complex ( k )% dual_volume ( num_elm ( k ))) lcl_complex ( k )% dual_volume = 0.d0 END DO ALLOCATE ( pts ( dim_cmplx + 1 , dim_embbd ), sgn ( dim_cmplx )) sgn = 1.d0 !-- start recursive computation of dual volumes on the interior -- k = dim_cmplx + 1 DO i = 1 , num_elm ( k ) CALL calc_dual_vlm_i ( pts , sgn , i , i , k ) END DO !-- start recurve addition of contributions from the surfaces -- k = dim_cmplx + 1 DO i = 1 , lcl_complex ( k )% num_surf indx = lcl_complex ( k )% surf_indx ( i , 2 ) CALL add_points_i ( pts , indx , k - 1 ) CALL calc_dual_vlm_i ( pts , sgn , indx , lcl_complex ( k )% surf_indx ( i , 3 ), k - 1 ) END DO !-- clean up -- DEALLOCATE ( pts , sgn ) RETURN END SUBROUTINE ! dec_mod/calc_dual_vlm !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* dec_mod/calc_dual_vlm_i !* SYNOPSIS RECURSIVE SUBROUTINE calc_dual_vlm_i ( pts , sgn , indx , p_indx , k ) !* PURPOSE !*   Recursively add to dual volume calculation !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Recursively add to dual volume calculation !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- INTEGER , INTENT ( IN ) :: k !> simplicial order INTEGER , INTENT ( IN ) :: indx , p_indx !> element and parent index REAL ( KIND = iwp ), INTENT ( INOUT ) :: pts (:,:) !> bounding points REAL ( KIND = iwp ), INTENT ( INOUT ) :: sgn (:) !> sign of volume elements !-- local variables -- INTEGER :: i !> loop counters INTEGER :: t_indx !> tmp index INTEGER :: n !> number of points !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- add to array of bounding points -- pts ( k ,:) = lcl_complex ( k )% centers ( indx ,:) !-- get sign -- IF ( k > 1 . AND . k <= dim_cmplx ) THEN DO i = 1 , k + 1 t_indx = lcl_complex ( k + 1 )% node_indx ( p_indx , i ) IF (. NOT . ANY ( lcl_complex ( k )% node_indx ( indx ,:) == t_indx )) THEN pts ( k - 1 ,:) = lcl_complex ( 1 )% centers ( lcl_complex ( k + 1 )% lcl_node_indx ( p_indx , i ),:) sgn ( k - 1 ) = sign ( 1.d0 , dot_product ( pts ( k + 1 ,:) - pts ( k ,:), pts ( k - 1 ,:) - pts ( k ,:))) EXIT END IF END DO END IF !-- compute number of points to be used in volume calculation -- n = dim_cmplx + 1 - k !-- add to k&#94;th order dual volume lcl_complex ( k )% dual_volume ( indx ) = lcl_complex ( k )% dual_volume ( indx ) + & product ( sgn ) * calc_unsgnd_vlm ( pts ( k :,:), n ) !-- recursively add to dual volumes from lower order elements -- IF ( k > 1 ) THEN DO i = 1 , lcl_complex ( k )% num_bndry ( indx ) CALL calc_dual_vlm_i ( pts , sgn , lcl_complex ( k )% bndry ( indx )% indx ( i ), indx , k - 1 ) END DO END IF IF ( k > 1 . AND . k <= dim_cmplx ) sgn ( k - 1 ) = 1.d0 RETURN END SUBROUTINE ! dec_mod/calc_dual_vlm_i !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* dec_mod/add_points_i !* SYNOPSIS RECURSIVE SUBROUTINE add_points_i ( pts , indx , k ) !* PURPOSE !*   Recursively adds points for dual volume calculation !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Recursively adds points for dual volume calculation !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- INTEGER , INTENT ( IN ) :: k !> simplicial order INTEGER , INTENT ( IN ) :: indx !> element index REAL ( KIND = iwp ), INTENT ( INOUT ) :: pts (:,:) !> bounding points !-- local variables -- INTEGER :: i !> loop counters INTEGER :: n !> number of points !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- add to array of points -- pts ( k ,:) = lcl_complex ( k )% centers ( indx ,:) !-- recursively add points from lower order elements -- IF ( k < dim_cmplx + 1 ) THEN DO i = 1 , lcl_complex ( k )% num_cobndry ( indx ) CALL add_points_i ( pts , lcl_complex ( k )% cobndry ( indx )% indx ( i ), k + 1 ) END DO END IF RETURN END SUBROUTINE ! dec_mod/add_points_i !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/f* dec_mod/calc_unsgnd_vlm !* SYNOPSIS FUNCTION calc_unsgnd_vlm ( pts , n ) !* PURPOSE !*   Compute unsigned volume for given set of points !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Compute unsigned volume for given set of points !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- REAL ( KIND = iwp ), INTENT ( IN ) :: pts (:,:) !> bounding points INTEGER , INTENT ( IN ) :: n !> # points to use !-- local variables -- INTEGER :: i !> loop counters INTEGER :: fac !> factorial REAL ( KIND = iwp ), ALLOCATABLE :: A (:,:) !> solution variable REAL ( KIND = iwp ) :: calc_unsgnd_vlm !> function value !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IF ( n == 0 ) THEN !-- assign default value when no points are used -- calc_unsgnd_vlm = 1.d0 ELSE !-- allocate temp. variables -- ALLOCATE ( A ( n , dim_embbd )) !-- compute necessary factorial and build system matrix -- fac = 1 DO i = 1 , n fac = fac * i A ( i ,:) = pts ( i + 1 ,:) - pts ( 1 ,:) END DO !-- compute and assign unsigned volume -- calc_unsgnd_vlm = SQRT ( ABS ( determinant ( MATMUL ( A , TRANSPOSE ( A )), n ))) / fac !-- clean up -- DEALLOCATE ( A ) END IF RETURN END FUNCTION ! dec_mod/calc_unsgnd_vlm !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* dec_mod/exchange_dual_vlm !* SYNOPSIS SUBROUTINE exchange_dual_vlm ( k ) !* PURPOSE !*   Exchange external dual volumes between adjacent processes !* INPUTS !*   Name                    Description !*   k !* OUTPUTS !*   Name                    Description !*   k !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/22: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/22 !> Exchange external dual volumes between adjacent processes !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- INTEGER , INTENT ( IN ) :: k !> simplicial order !-- local variables -- INTEGER :: kp !> simplicial order plus one INTEGER :: cnt !> counter INTEGER :: i !> loop index INTEGER :: junk !> junk comm variable INTEGER :: buffer_size !> size of comm buffer REAL ( KIND = iwp ), ALLOCATABLE :: sbuffer (:) !> send buffer REAL ( KIND = iwp ), ALLOCATABLE :: rbuffer (:) !> receive buffer INTEGER , ALLOCATABLE :: req (:) !> request variable for non-blocking comms INTEGER , ALLOCATABLE :: status (:,:) !> size of MPI comm buffer !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- allocate and pack send buffer -- kp = k + 1 ALLOCATE ( sbuffer ( lcl_complex ( kp )% num_send )) DO i = 1 , lcl_complex ( kp )% num_send sbuffer ( i ) = lcl_complex ( k )% dual_volume ( lcl_complex ( kp )% send_indx ( i , 2 )) END DO !-- send data to adjacent processes -- cnt = 1 DO i = 1 , num_adj_proc IF ( num_send ( k , i ) == 0 ) CYCLE buffer_size = num_send ( k , i ) CALL MPI_ISEND ( sbuffer ( cnt : cnt + buffer_size - 1 ), buffer_size ,& MPI_DOUBLE_PRECISION , adj_proc ( i ), 0 , MPI_COMM_WORLD , junk , ier ) CALL MPI_REQUEST_FREE ( junk , ier ) cnt = cnt + buffer_size END DO !-- allocate recieve buffer and receive data from adjacent processes -- ALLOCATE ( rbuffer ( lcl_complex ( kp )% num_recv ),& req ( num_adj_proc ), status ( MPI_STATUS_SIZE , num_adj_proc )) cnt = 1 DO i = 1 , num_adj_proc IF ( num_recv ( k , i ) == 0 ) THEN req ( i ) = MPI_REQUEST_NULL CYCLE END IF buffer_size = num_recv ( k , i ) CALL MPI_IRECV ( rbuffer ( cnt : cnt + buffer_size - 1 ), buffer_size ,& MPI_DOUBLE_PRECISION , adj_proc ( i ), 0 , MPI_COMM_WORLD , req ( i ), ier ) cnt = cnt + buffer_size END DO !-- wait for communication to finish, then unpack receive buffer -- CALL MPI_WAITALL ( num_adj_proc , req , status , ier ) DO i = 1 , lcl_complex ( kp )% num_recv lcl_complex ( k )% dual_volume ( lcl_complex ( kp )% recv_indx ( i , 2 )) = rbuffer ( i ) END DO !-- clean up -- DEALLOCATE ( req , status ) !-- make sure all comms have completed -- CALL MPI_BARRIER ( MPI_COMM_WORLD , ier ) RETURN END SUBROUTINE ! io_mod|exchange_dual_vlm !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* dec_mod/exchange_dual_dir !* SYNOPSIS SUBROUTINE exchange_dual_dir ( k ) !* PURPOSE !*   Exchange external dual edge directions between adjacent processes !* INPUTS !*   Name                    Description !*   k !* OUTPUTS !*   Name                    Description !*   k !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/22: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/22 !> Exchange external dual edge directions between adjacent processes !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- INTEGER , INTENT ( IN ) :: k !> simplicial order !-- local variables -- INTEGER :: kp !> simplicial order INTEGER :: cnt !> counter INTEGER :: i !> loop index INTEGER :: junk !> junk comm variable INTEGER :: buffer_size !> size of comm buffer REAL ( KIND = iwp ), ALLOCATABLE :: sbuffer (:) !> send buffer REAL ( KIND = iwp ), ALLOCATABLE :: rbuffer (:) !> receive buffer INTEGER , ALLOCATABLE :: req (:) !> request variable for non-blocking comms INTEGER , ALLOCATABLE :: status (:,:) !> size of MPI comm buffer !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- allocate and pack send buffer -- kp = k + 1 ALLOCATE ( sbuffer ( dim_embbd * lcl_complex ( kp )% num_send )) DO i = 1 , lcl_complex ( kp )% num_send sbuffer (( i - 1 ) * dim_embbd + 1 : i * dim_embbd ) = & lcl_complex ( k )% dual_dir ( lcl_complex ( kp )% send_indx ( i , 2 ),:) END DO !-- send data to adjacent processes -- cnt = 1 DO i = 1 , num_adj_proc IF ( num_send ( k , i ) == 0 ) CYCLE buffer_size = dim_embbd * num_send ( k , i ) CALL MPI_ISEND ( sbuffer ( cnt : cnt + buffer_size - 1 ), buffer_size ,& MPI_DOUBLE_PRECISION , adj_proc ( i ), 0 , MPI_COMM_WORLD , junk , ier ) CALL MPI_REQUEST_FREE ( junk , ier ) cnt = cnt + buffer_size END DO !-- allocate recieve buffer and receive data from adjacent processes -- ALLOCATE (& req ( num_adj_proc ),& status ( MPI_STATUS_SIZE , num_adj_proc ),& rbuffer ( dim_embbd * lcl_complex ( kp )% num_recv )) cnt = 1 DO i = 1 , num_adj_proc IF ( num_recv ( k , i ) == 0 ) THEN req ( i ) = MPI_REQUEST_NULL CYCLE END IF buffer_size = dim_embbd * num_recv ( k , i ) CALL MPI_IRECV ( rbuffer ( cnt : cnt + buffer_size - 1 ), buffer_size ,& MPI_DOUBLE_PRECISION , adj_proc ( i ), 0 , MPI_COMM_WORLD , req ( i ), ier ) cnt = cnt + buffer_size END DO !-- wait for communication to finish, then unpack receive buffer -- CALL MPI_WAITALL ( num_adj_proc , req , status , ier ) DO i = 1 , lcl_complex ( kp )% num_recv lcl_complex ( k )% dual_dir ( lcl_complex ( kp )% recv_indx ( i , 2 ),:) = & rbuffer (( i - 1 ) * dim_embbd + 1 : i * dim_embbd ) END DO !-- clean up -- DEALLOCATE ( req , status ) !-- make sure all comms have completed -- CALL MPI_BARRIER ( MPI_COMM_WORLD , ier ) RETURN END SUBROUTINE ! io_mod|exchange_dual_dir !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* dec_mod/calc_hodge_star !* SYNOPSIS SUBROUTINE calc_hodge_star ( k ) !* PURPOSE !*   Compute hodge star and it's inverse from primal and dual volumes !* INPUTS !*   Name                    Description !*   k !* OUTPUTS !*   Name                    Description !*   k !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Compute hodge star and it's inverse from primal and dual volumes !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- INTEGER , INTENT ( IN ) :: k !> geometric order !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- allocate and compute hodge stars and it's invers -- IF (. NOT . ALLOCATED ( lcl_complex ( k )% hdg_star )) ALLOCATE ( lcl_complex ( k )% hdg_star ( num_elm ( k ))) IF (. NOT . ALLOCATED ( lcl_complex ( k )% inv_hdg_star )) ALLOCATE ( lcl_complex ( k )% inv_hdg_star ( num_elm ( k ))) lcl_complex ( k )% hdg_star = lcl_complex ( k )% dual_volume / lcl_complex ( k )% prml_volume lcl_complex ( k )% inv_hdg_star = lcl_complex ( k )% prml_volume / lcl_complex ( k )% dual_volume RETURN END SUBROUTINE ! dec_mod/calc_hodge_star !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* dec_mod/calc_prml_dir !* SYNOPSIS SUBROUTINE calc_prml_dir () !* PURPOSE !*   Compute the unit direction of primal edges !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Compute the unit direction of primal edges !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- local variables -- INTEGER :: i !> loop counter !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- allocate direction variables -- IF (. NOT . ALLOCATED ( lcl_complex ( 2 )% prml_dir )) & ALLOCATE ( lcl_complex ( 2 )% prml_dir ( num_elm ( 2 ), dim_embbd )) !-- compute edge direction -- DO i = 1 , num_elm ( 2 ) !-- add contribution from end-points (primal vertices) -- lcl_complex ( 2 )% prml_dir ( i ,:) = & lcl_complex ( 1 )% centers ( lcl_complex ( 2 )% lcl_node_indx ( i , 1 ),:) - & lcl_complex ( 1 )% centers ( lcl_complex ( 2 )% lcl_node_indx ( i , 2 ),:) !-- normalise -- lcl_complex ( 2 )% prml_dir ( i ,:) = lcl_complex ( 2 )% prml_dir ( i ,:) / & max ( sqrt ( dot_product ( lcl_complex ( 2 )% prml_dir ( i ,:), lcl_complex ( 2 )% prml_dir ( i ,:))), small ) END DO RETURN END SUBROUTINE ! dec_mod/calc_prml_dir !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* dec_mod/calc_dual_dir !* SYNOPSIS SUBROUTINE calc_dual_dir () !* PURPOSE !*   Compute the unit direction of dual edges !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Compute the unit direction of dual edges !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- local variables -- INTEGER :: i , j !> loop counters !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- allocate direction variables and zero -- IF (. NOT . ALLOCATED ( lcl_complex ( dim_cmplx )% dual_dir )) & ALLOCATE ( lcl_complex ( dim_cmplx )% dual_dir ( num_elm ( dim_cmplx ), dim_embbd )) lcl_complex ( dim_cmplx )% dual_dir = 0.d0 !-- compute edge direction for each local dual edge -- DO i = 1 , num_elm ( dim_cmplx ) !-- add contribution from internal end-points (dual vertices) -- DO j = 1 , lcl_complex ( dim_cmplx )% num_cobndry ( i ) lcl_complex ( dim_cmplx )% dual_dir ( i ,:) = lcl_complex ( dim_cmplx )% dual_dir ( i ,:) + & lcl_complex ( dim_cmplx )% cobndry ( i )% sgn ( j ) * & lcl_complex ( dim_cmplx + 1 )% centers ( lcl_complex ( dim_cmplx )% cobndry ( i )% indx ( j ),:) END DO END DO !-- add contribution from surface end-points -- DO i = 1 , lcl_complex ( dim_cmplx + 1 )% num_surf lcl_complex ( dim_cmplx )% dual_dir ( lcl_complex ( dim_cmplx + 1 )% surf_indx ( i , 2 ),:) = & lcl_complex ( dim_cmplx )% dual_dir ( lcl_complex ( dim_cmplx + 1 )% surf_indx ( i , 2 ),:) + & lcl_complex ( dim_cmplx + 1 )% surf_indx ( i , 1 ) * & lcl_complex ( dim_cmplx )% centers ( lcl_complex ( dim_cmplx + 1 )% surf_indx ( i , 2 ),:) END DO !-- exchange external dual edge directions to adjacent processes -- CALL exchange_dual_dir ( dim_cmplx ) !-- normalise direction vectors -- DO i = 1 , num_elm ( dim_cmplx ) lcl_complex ( dim_cmplx )% dual_dir ( i ,:) = lcl_complex ( dim_cmplx )% dual_dir ( i ,:) / & max ( sqrt ( dot_product ( lcl_complex ( dim_cmplx )% dual_dir ( i ,:), lcl_complex ( dim_cmplx )% dual_dir ( i ,:))), small ) END DO RETURN END SUBROUTINE ! dec_mod/calc_dual_dir !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* dec_mod/calc_whitney_C2_BC !* SYNOPSIS SUBROUTINE calc_whitney_C2_BC () !* PURPOSE !*   Compute the Whitney interpolation of primal faces to primal volume barycentric centers !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Compute the Whitney interpolation of primal faces to primal volume barycentric centers !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- local variables -- INTEGER :: i , j , k !> loop indices INTEGER :: vrts ( 4 ) !> volume vertex indices INTEGER :: f_vrts ( 3 ) !> face vertex indices INTEGER :: indx !> face index LOGICAL :: not_found !> is boundary face index found REAL ( KIND = iwp ) :: grads ( 4 , 3 ) !> barycentric gradients !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- zero solution array -- lcl_complex ( dim_cmplx + 1 )% whtny_sol = 0.d0 !-- loop through volumes computing whitney interpolation -- DO i = 1 , num_elm ( dim_cmplx + 1 ) !-- get local vertex indices of volume -- vrts = lcl_complex ( dim_cmplx + 1 )% lcl_node_indx ( i ,:) !-- compute barycentric gradients of volume -- grads = lcl_complex ( 1 )% centers ( vrts ,:) CALL calc_barycentric_grad ( grads , 4 ) !-- compute contribution from each face of volume -- DO j = 1 , 4 !-- get face vertices -- f_vrts = ( / vrts ( 1 : 4 - j ), vrts ( 4 - j + 2 : 4 ) / ) !-- find face local index of face -- not_found = . TRUE . !-- check local faces -- DO k = 1 , lcl_complex ( dim_cmplx + 1 )% num_bndry ( i ) indx = lcl_complex ( dim_cmplx + 1 )% bndry ( i )% indx ( k ) IF ( ALL ( f_vrts == lcl_complex ( dim_cmplx )% lcl_node_indx ( indx ,:))) THEN !-- match found; exit search -- not_found = . FALSE .; EXIT END IF END DO IF ( not_found ) THEN !-- check non-local internal faces -- DO k = 1 , lcl_complex ( dim_cmplx + 1 )% num_ext indx = lcl_complex ( dim_cmplx + 1 )% ext_indx ( k , 2 ) IF ( ALL ( f_vrts == lcl_complex ( dim_cmplx )% lcl_node_indx ( indx ,:))) THEN !-- match found; exit search -- not_found = . FALSE .; EXIT END IF END DO IF ( not_found ) THEN !-- check surface faces -- DO k = 1 , lcl_complex ( dim_cmplx + 1 )% num_surf indx = lcl_complex ( dim_cmplx + 1 )% surf_indx ( k , 2 ) IF ( ALL ( f_vrts == lcl_complex ( dim_cmplx )% lcl_node_indx ( indx ,:))) THEN !-- match found; exit search -- not_found = . FALSE .; EXIT END IF END DO END IF END IF !-- compute contribution from face at barycenter using Whitney interpolation -- !-- build index array for computation f_vrts = ( / ( k , k = 1 , 4 - j ), ( k , k = 4 - j + 2 , 4 ) / ) ! write(*,*) '  -',i,j,indx,lcl_complex(dim_cmplx+1)% whtny_sol(i,:),& !   0.5d0*lcl_complex(dim_cmplx)% prml_sol(indx,1)*(& !   cross_product(grads(f_vrts(1),:),grads(f_vrts(3),:)) + & !   cross_product(grads(f_vrts(2),:),grads(f_vrts(1),:)) + & !   cross_product(grads(f_vrts(3),:),grads(f_vrts(2),:)) ) !-- add contribution -- lcl_complex ( dim_cmplx + 1 )% whtny_sol ( i ,:) = lcl_complex ( dim_cmplx + 1 )% whtny_sol ( i ,:) + & 0.5d0 * lcl_complex ( dim_cmplx )% prml_sol ( indx , 1 ) * (& cross_product ( grads ( f_vrts ( 1 ),:), grads ( f_vrts ( 3 ),:)) + & cross_product ( grads ( f_vrts ( 2 ),:), grads ( f_vrts ( 1 ),:)) + & cross_product ( grads ( f_vrts ( 3 ),:), grads ( f_vrts ( 2 ),:)) ) END DO !write(*,*) '  -',i,lcl_complex(dim_cmplx+1)% whtny_sol(i,:) END DO RETURN END SUBROUTINE ! dec_mod/calc_whitney_C2_BC !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !/****/s* dec_mod/calc_barycentric_grad !* SYNOPSIS SUBROUTINE calc_barycentric_grad ( pts , n ) !* PURPOSE !*   Compute barycentric gradients !* INPUTS !*   Name                    Description !* !* OUTPUTS !*   Name                    Description !* !* SIDE EFFECTS !*   - !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/23: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ !> author: Pieter Boom !> date: 2019/08/23 !> Compute barycentric gradients !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !-- arguments -- REAL ( KIND = iwp ), INTENT ( INOUT ) :: pts (:,:) !> vertices of simplex INTEGER , INTENT ( IN ) :: n !> number of vertices !-- local variables -- INTEGER :: i , j !> loop counters !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !-- compute n-1 edge gradients -- DO i = 1 , n - 1 ; pts ( i ,:) = pts ( i ,:) - pts ( n ,:); END DO SELECT CASE ( n ) CASE ( 2 ) !-- do nothing: already finished -- CASE ( 3 ) pts ( 1 : n - 1 ,:) = MATMUL ( matinv2 ( MATMUL ( pts ( 1 : n - 1 ,:), TRANSPOSE ( pts ( 1 : n - 1 ,:)))), pts ( 1 : n - 1 ,:)) CASE ( 4 ) pts ( 1 : n - 1 ,:) = MATMUL ( matinv3 ( MATMUL ( pts ( 1 : n - 1 ,:), TRANSPOSE ( pts ( 1 : n - 1 ,:)))), pts ( 1 : n - 1 ,:)) CASE DEFAULT !-- not setup for higher dimension simplices -- END SELECT !-- compute last gradient using the fact that the sum of gradients must be zero -- pts ( n ,:) = - SUM ( pts ( 1 : n - 1 ,:), DIM = 1 ) RETURN END SUBROUTINE ! dec_mod/calc_barycentric_grad !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! END MODULE ! dec_mod !=============================================================================== !","tags":"","loc":"sourcefile/dec_mod.f90.html","title":"dec_mod.f90 – ParaGEMS"},{"text":"This file depends on sourcefile~~test_dec_mod.f90~~EfferentGraph sourcefile~test_dec_mod.f90 test_dec_mod.f90 sourcefile~dec_mod.f90 dec_mod.f90 sourcefile~test_dec_mod.f90:->sourcefile~dec_mod.f90: sourcefile~common_mod.f90 common_mod.f90 sourcefile~dec_mod.f90:->sourcefile~common_mod.f90: sourcefile~math_mod.f90 math_mod.f90 sourcefile~dec_mod.f90:->sourcefile~math_mod.f90: sourcefile~mpi_mod.f90 mpi_mod.f90 sourcefile~dec_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~math_mod.f90:->sourcefile~common_mod.f90: sourcefile~math_mod.f90:->sourcefile~mpi_mod.f90: sourcefile~mpi_mod.f90:->sourcefile~common_mod.f90: Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules test_dec_mod Source Code test_dec_mod.f90 Source Code ! !=============================================================================== ! /****m*/ /src/modules/common/common_test MODULE test_dec_mod ! ! PURPOSE:    Tests for common_mod module ! ! TESTS: ! ! UPDATES:  created (PDB) :: 2019/08/21 ! !=============================================================================== !----------------------------------------------------------------------------- ! use statements and implicit none !----------------------------------------------------------------------------- USE dec_mod IMPLICIT NONE !=============================================================================== CONTAINS !=============================================================================== ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! /****s*/ /src/common/common_test|test_vars SUBROUTINE test_vars ( cnt ) ! ! PURPOSE: ! ! UPDATES:  created (PDB) :: 2019/08/21 ! !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! TO DO: ! - global variables? ! - primary element data structure ! - clean up !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPLICIT NONE !--------------------------------------------------------------------- ! arguments !--------------------------------------------------------------------- INTEGER , INTENT ( INOUT ) :: cnt ! test counter !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! MAIN EXECUTION !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ !--------------------------------------------------------------------- ! 1. precision !--------------------------------------------------------------------- cnt = cnt + 1 IF ( 1.d0 - 9.999999999999999d-1 > 0.d0 ) THEN WRITE ( * , '(A,I5,A)' ) 'common_test : ' , cnt ,& ' : v/      PASSED : precision accurate to 16 decimal places' ELSE WRITE ( * , '(A,I5,A)' ) 'common_test : ' , cnt ,& ' : XXXXXX  FAILED : precision NOT accurate to 16 decimal places' END IF !--------------------------------------------------------------------- ! 2. global variables ! 3. global mpi ! 4. global io ! 5. global mesh ! 6. primary element ! 7. variable clean up !--------------------------------------------------------------------- END SUBROUTINE ! common_test|test_vars !~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ! END MODULE ! common_test !=============================================================================== !","tags":"","loc":"sourcefile/test_dec_mod.f90.html","title":"test_dec_mod.f90 – ParaGEMS"},{"text":"Define global variables for solvers (linear and nonlinear) Contents Source Code solver_vars.inc Source Code ! ! =============================================================================== ! /****v* modules|solvers/solver_vars !* PURPOSE !*   Define global variables for solvers (linear and nonlinear) !* CONTAINS !*   Structure/Variable      Purpose !*   PETSc error variable !*   PETSc solution variables !*   PETSc KSP variables !*   PETSc viewer variables !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/21: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ ! > author : Pieter Boom ! > date : 2019 / 08 / 21 ! > Define global variables for solvers ( linear and nonlinear ) ! =============================================================================== ! ----------------------------------------------------------------------------- ! variables ! ----------------------------------------------------------------------------- ! -- KSP variables -- KSP :: ksp_id ! linear solver id KSPType :: ksp_type = KSPGMRES ! PETSC linear solver PetscReal :: rel_tol = PETSC_DEFAULT_REAL , & ! Relative termination tolerance abs_tol = PETSC_DEFAULT_REAL , & ! Absolute termination tolerance div_tol = PETSC_DEFAULT_REAL ! divergence tolerance PetscInt :: max_iter = PETSC_DEFAULT_REAL ! Maximum major iterations ! -- use zero initial guess -- LOGICAL :: nz_init = . FALSE . ! use non - zero initial soluiton ! -- Preconditioner variables -- PC :: pc_id ! preconditioner id PCType :: pc_type = PCASM ! PETSC preconditioner ( disabled ) MatOrderingType :: pc_reorder = MATORDERINGRCM ! PETSC reordering ( disabled ) ! -- PETSc viewer variables -- PetscViewer :: output_view PetscViewerAndFormat :: output_vfrmt ! -- PETSc monitor output -- LOGICAL :: solver_monitor = . FALSE . ! output convergence data to command line ? ! -- Solution output -- LOGICAL :: sol_output = . FALSE . ! Output solution data to file ? INTEGER :: output_frqcy = 1 ! Number of crack iterations between writing solution data to file ! ----------------------------------------------------------------------------- ! variables ! ----------------------------------------------------------------------------- NAMELIST / solver_param / ksp_type , rel_tol , abs_tol , div_tol , max_iter , & pc_type , pc_reorder , nz_init , solver_monitor , sol_output , output_frqcy ! solver_vars ! =============================================================================== !","tags":"","loc":"sourcefile/solver_vars.inc.html","title":"solver_vars.inc – ParaGEMS"},{"text":"Define global variables for Darcy flow equations Contents Source Code darcy_vars.inc Source Code ! ! =============================================================================== ! /****v* modules|phy_darcy_flow/darcy_vars !* PURPOSE !*   Define global variables for Darcy flow equations !* CONTAINS !*   Structure/Variable      Purpose !*   Darcy parameters !*   Darcy namelist !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/21: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ ! > author : Pieter Boom ! > date : 2019 / 08 / 21 ! > Define global variables for Darcy flow equations ! =============================================================================== ! ----------------------------------------------------------------------------- ! variables ! ----------------------------------------------------------------------------- ! -- Darcy parameters -- REAL ( KIND = iwp ) :: mu = 1. d0 ! viscosity REAL ( KIND = iwp ) :: k = 1. d0 ! permeability REAL ( KIND = iwp ) :: re = 1. d0 ! Reynold number REAL ( KIND = iwp ) :: vel ( 3 ) = ( / 1. d0 , 0. d0 , 0. d0 / ) ! velocity REAL ( KIND = iwp ) :: bc_press ( 30 ) = 0. d0 ! boundary press ( max 30 ) REAL ( KIND = iwp ) :: p_ref = 1. d0 ! reference pressure ! -- cracking variables -- INTEGER :: crck_type = 0 ! cracking process INTEGER :: max_crcks = 1 ! max cracks INTEGER :: crcks_pstep = 1 ! max faces cracked / step REAL ( KIND = iwp ) :: crck_thrshld = 0.0 d0 ! min vel for cracking REAL ( KIND = iwp ) :: crck_area = 0.0 d0 ! cumulative crack area REAL ( KIND = iwp ) :: rnd_max = 0.0 d0 ! face variation ! -- Darcy namelist -- NAMELIST / darcy_param / mu , k , re , vel , p_ref , bc_press , & crck_type , max_crcks , crcks_pstep , crck_thrshld , rnd_max ! darcy_vars ! =============================================================================== !","tags":"","loc":"sourcefile/darcy_vars.inc.html","title":"darcy_vars.inc – ParaGEMS"},{"text":"Define global IO variables MPI-IO Use MPI-IO? use parallel IO file prefexes Default file prefexes file prefix: input file prefix: mesh file prefix: solution file prefix: unsteady file prefix: log IO units Default IO units IO unit: input IO unit: mesh files IO unit: node files IO unit: edge files IO unit: face files IO unit: volm files IO unit: solution IO unit: unsteady IO unit: unsteady log IO unit: logs IO namelist IO namelist for input file index offsets Offsets to account entity indices not starting from 1 index offsets in files Contents Source Code global_io.inc Source Code ! ! =============================================================================== ! /****v* modules|common/global_io !* PURPOSE !*   Define global IO variables !* CONTAINS !*   Structure/Variable      Purpose !*   MPI-IO                  Use MPI-IO? !*   file prefixes           Default file prefexes !*   IO units                Default IO units !*   IO namelist             IO namelist for input file !*   index offsets           Offsets to account entity indices not starting from 1 !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/20: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ ! > author : Pieter Boom ! > date : 2019 / 08 / 20 ! > Define global IO variables ! =============================================================================== ! > # MPI - IO ! > Use MPI - IO ? ! > ## LOGICAL :: use_mpiio = . FALSE . ! > - use parallel IO ! > # file prefexes ! > Default file prefexes ! > ## CHARACTER ( LEN = slen ) :: input_file = ' input . param ' ! > - file prefix : input CHARACTER ( LEN = slen ) :: mesh_prefix = ' mesh ' ! > - file prefix : mesh CHARACTER ( LEN = slen ) :: sol_prefix = ' solution ' ! > - file prefix : solution CHARACTER ( LEN = slen ) :: unstdy_prefix = ' unsteady ' ! > - file prefix : unsteady CHARACTER ( LEN = slen ) :: log_prefix = ' paragems ' ! > - file prefix : log ! > # IO units ! > Default IO units ! > ## INTEGER , PARAMETER :: input_unit = 10 ! > - IO unit : input INTEGER , PARAMETER :: mesh_unit = 20 ! > - IO unit : mesh files INTEGER , PARAMETER :: node_unit = 22 ! > - IO unit : node files INTEGER , PARAMETER :: edge_unit = 24 ! > - IO unit : edge files INTEGER , PARAMETER :: face_unit = 26 ! > - IO unit : face files INTEGER , PARAMETER :: volm_unit = 28 ! > - IO unit : volm files INTEGER , PARAMETER :: sol_unit = 30 ! > - IO unit : solution INTEGER , PARAMETER :: unstdy_unit = 40 ! > - IO unit : unsteady INTEGER , PARAMETER :: ulog_unit = 45 ! > - IO unit : unsteady log INTEGER , PARAMETER :: log_unit = 90 ! > - IO unit : logs ! > # IO namelist ! > IO namelist for input file ! > ## NAMELIST / io_param / mesh_prefix , sol_prefix , unstdy_prefix , log_prefix ! > # index offsets ! > Offsets to account entity indices not starting from 1 ! > ## INTEGER :: indx_offset ( 4 ) = 0 ! > - index offsets in files ! global_io ! =============================================================================== !","tags":"","loc":"sourcefile/global_io.inc.html","title":"global_io.inc – ParaGEMS"},{"text":"Define global variables and parameters for PETSc PETSc error variable PETSc error variable PETSc solution variables Matries: LHS, sub-matrices, Schur complement Vectors: RHS and solution Index set: LHS w/ schur complement Contents Source Code global_petsc.inc Source Code ! ! =============================================================================== ! /****v* modules|common/global_petsc !* PURPOSE !*   Define global variables and parameters for PETSc !* CONTAINS !*   Structure/Variable      Purpose !*   PETSc error variable !*   PETSc solution variables !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2020/07/08: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ ! > author : Pieter Boom ! > date : 2020 / 07 / 08 ! > Define global variables and parameters for PETSc ! =============================================================================== ! > # PETSc error variable ! > ## PetscErrorCode :: petsc_ier ! > - PETSc error variable ! > # PETSc solution variables ! > ## Mat :: A , Asub ( 4 ), Sp ! > - Matries : LHS , sub - matrices , Schur complement Vec :: b , sol ! > - Vectors : RHS and solution IS :: isg ( 2 ) ! > - Index set : LHS w / schur complement ! global_vars ! =============================================================================== !","tags":"","loc":"sourcefile/global_petsc.inc.html","title":"global_petsc.inc – ParaGEMS"},{"text":"Define general global variables and parameters constants general physical constants pi reference values general reference values default string length large value (dp) large value (sp) large value (hp) small value (dp) small value (sp) small value (hp) solution variables number of solution variables (prml,dual) num prml/dual solution variables for each\ngeometric order timing variables global timing variables program start time current time error checking variables error variable error flag Contents Source Code global_vars.inc Source Code ! ! =============================================================================== ! /****v* modules|common/global_vars !* PURPOSE !*   Define general global variables and parameters !* CONTAINS !*   Structure/Variable      Purpose !*   constants               - general physical constants !*   reference values        - general reference values !*   solution                - number of solution variables (prml,dual) !*   timing                  - global timing variables !*   error checking          - error variable !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/21: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ ! > author : Pieter Boom ! > date : 2019 / 08 / 21 ! > Define general global variables and parameters ! =============================================================================== ! > # constants ! > general physical constants ! > ## REAL ( KIND = iwp ), PARAMETER :: & pi = 3.14159265358979 d0 ! > - pi ! > # reference values ! > general reference values ! > ## INTEGER , PARAMETER :: slen = 128 ! > - default string length REAL ( KIND = iwp ), PARAMETER :: large = 1. d + 15 ! > - large value ( dp ) REAL ( KIND = iwp ), PARAMETER :: larges = 1. d + 8 ! > - large value ( sp ) REAL ( KIND = iwp ), PARAMETER :: largeh = 1. d + 4 ! > - large value ( hp ) REAL ( KIND = iwp ), PARAMETER :: small = 1. d - 15 ! > - small value ( dp ) REAL ( KIND = iwp ), PARAMETER :: smalls = 1. d - 8 ! > - small value ( sp ) REAL ( KIND = iwp ), PARAMETER :: smallh = 1. d - 4 ! > - small value ( hp ) ! > # solution variables ! > number of solution variables ( prml , dual ) ! > ## INTEGER :: num_vars ( 2 , 4 ) ! > - num prml / dual solution ! > - variables for each ! > geometric order ! > # timing variables ! > global timing variables ! > ## REAL ( KIND = iwp ) :: start_time ! > - program start time REAL ( KIND = iwp ) :: curnt_time ! > - current time ! > # error checking variables ! > error variable ! > ## INTEGER :: ier ! > - error flag ! global_vars ! =============================================================================== !","tags":"","loc":"sourcefile/global_vars.inc.html","title":"global_vars.inc – ParaGEMS"},{"text":"Define real precision real precision Define real precision real precision Contents Source Code global_precision.inc Source Code ! ! =============================================================================== ! /****v* modules|common/global_precision !* PURPOSE !*   Define real precision !* CONTAINS !*   Structure/Variable      Purpose !*   real precision          - Define real precision !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/20: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ ! > author : Pieter Boom ! > date : 2019 / 08 / 20 ! > Define real precision ! =============================================================================== ! > # real precision ! > Define real precision ! > ## INTEGER , PARAMETER :: iwp = SELECTED_REAL_KIND ( 15 , 300 ) ! > - real precision ! END MODULE ! global_precision ! =============================================================================== !","tags":"","loc":"sourcefile/global_precision.inc.html","title":"global_precision.inc – ParaGEMS"},{"text":"Define global primary structures Index structure Structure for defining (co)boundary operators sign index Local complex structure Primary structure for simplicial complex and solution global information global index local information global node indices local node indices orientation default ordering boundary information boundary type DEC information location of centers primal volume dual volume hodge star hodge star inverse num boundaries num co-boundaries boundary indices co-boundary indices prml unit direction dual unit direction solution information primal solution dual solution Whitney interpolated solution external (process non-local) and MPI comms information num external boundaries external info:\n1 sign;\n2 external boundary index;\n3 local entity index num boundaries to send send info:\n1 send process rank;\n2 local boundary index num boundaries to recv recieve info:\n1 recieve process rank;\n2 local boundary index surface information num surface boundaries surface info:\n1 sign;\n2 surface boundary index;\n3 local entity index;\n4 boundary type Contents Source Code global_element.inc Source Code ! ! =============================================================================== ! /****v* modules|common/global_element !* PURPOSE !*   Define global primary structures !* CONTAINS !*   Structure/Variable      Purpose !*   index structure         - Structure for defining (co)boundary operators !*   local complex structure - Primary structure for simplicial complex and solution !*        > global information !*        > local information !*        > DEC information !*        > solution information !*        > external and MPI comms information !*        > surface information !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/21: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ ! > author : Pieter Boom ! > date : 2019 / 08 / 21 ! > Define global primary structures ! =============================================================================== ! > # Index structure ! > Structure for defining ( co ) boundary operators TYPE indx_stuct INTEGER , ALLOCATABLE :: sgn ( : ) ! > - sign INTEGER , ALLOCATABLE :: indx ( : ) ! > - index END TYPE ! > # Local complex structure ! > Primary structure for simplicial complex and solution TYPE lcl_complex_struc ! > ## global information INTEGER , ALLOCATABLE :: glb_indx ( : ) ! > - global index ! > ## local information INTEGER , ALLOCATABLE :: node_indx ( : , : ) ! > - global node indices INTEGER , ALLOCATABLE :: lcl_node_indx ( : , : ) ! > - local node indices INTEGER , ALLOCATABLE :: orientation ( : ) ! > - orientation INTEGER , ALLOCATABLE :: deflt_order ( : , : ) ! > - default ordering ! > ## boundary information INTEGER , ALLOCATABLE :: bc_type ( : ) ! > - boundary type ! > ## DEC information REAL ( KIND = iwp ), ALLOCATABLE :: centers ( : , : ) ! > - location of centers REAL ( KIND = iwp ), ALLOCATABLE :: prml_volume ( : ) ! > - primal volume REAL ( KIND = iwp ), ALLOCATABLE :: dual_volume ( : ) ! > - dual volume REAL ( KIND = iwp ), ALLOCATABLE :: hdg_star ( : ) ! > - hodge star REAL ( KIND = iwp ), ALLOCATABLE :: inv_hdg_star ( : ) ! > - hodge star inverse INTEGER , ALLOCATABLE :: num_bndry ( : ) ! > - num boundaries INTEGER , ALLOCATABLE :: num_cobndry ( : ) ! > - num co - boundaries TYPE ( indx_stuct ), ALLOCATABLE :: & bndry ( : ) ! > - boundary indices TYPE ( indx_stuct ), ALLOCATABLE :: & cobndry ( : ) ! > - co - boundary indices REAL ( KIND = iwp ), ALLOCATABLE :: prml_dir ( : , : ) ! > - prml unit direction REAL ( KIND = iwp ), ALLOCATABLE :: dual_dir ( : , : ) ! > - dual unit direction ! > ## solution information REAL ( KIND = iwp ), ALLOCATABLE :: prml_sol ( : , : ) ! > - primal solution REAL ( KIND = iwp ), ALLOCATABLE :: dual_sol ( : , : ) ! > - dual solution REAL ( KIND = iwp ), ALLOCATABLE :: whtny_sol ( : , : ) ! > - Whitney interpolated solution ! > ## external ( process non - local ) and MPI comms information INTEGER :: num_ext ! > - num external boundaries INTEGER , ALLOCATABLE :: ext_indx ( : , : ) ! > - external info : ! > 1 sign ; ! > 2 external boundary index ; ! > 3 local entity index INTEGER :: num_send ! > - num boundaries to send INTEGER , ALLOCATABLE :: send_indx ( : , : ) ! > - send info : ! > 1 send process rank ; ! > 2 local boundary index INTEGER :: num_recv ! > - num boundaries to recv INTEGER , ALLOCATABLE :: recv_indx ( : , : ) ! > - recieve info : ! > 1 recieve process rank ; ! > 2 local boundary index ! > ## surface information INTEGER :: num_surf = 0 ! > - num surface boundaries INTEGER , ALLOCATABLE :: surf_indx ( : , : ) ! > - surface info : ! > 1 sign ; ! > 2 surface boundary index ; ! > 3 local entity index ; ! > 4 boundary type ! -- crack information -- INTEGER :: num_crack = 0 ! # cracked elements INTEGER , ALLOCATABLE :: crack_indx ( : ) ! index END TYPE TYPE ( lcl_complex_struc ), ALLOCATABLE :: lcl_complex ( : ) ! global_element ! =============================================================================== !","tags":"","loc":"sourcefile/global_element.inc.html","title":"global_element.inc – ParaGEMS"},{"text":"Define global MPI variables general MPI information general MPI information root process id local process id (indexed from 0) total # processes MPI adjacent process (neighbour) information MPI adjacent process (neighbour) information num adjacent processes ids of adjacent processes num sends for: num receives for:\n1 geometric order k;\n2 adjacent process Contents Source Code global_mpi.inc Source Code ! ! =============================================================================== ! /****v* modules|common/global_mpi !* PURPOSE !*   Define global MPI variables !* CONTAINS !*   Structure/Variable      Purpose !*   general MPI info        - general MPI information !*   MPI adjacent info       - MPI adjacent process (neighbour) information !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/20: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ ! > author : Pieter Boom ! > date : 2019 / 08 / 20 ! > Define global MPI variables ! =============================================================================== ! > # general MPI information ! > general MPI information ! > ## INTEGER , PARAMETER :: root = 0 ! > - root process id INTEGER :: rank ! > - local process id ( indexed from 0 ) INTEGER :: num_procs ! > - total # processes ! > # MPI adjacent process ( neighbour ) information ! > MPI adjacent process ( neighbour ) information ! > ## INTEGER :: num_adj_proc ! > - num adjacent processes INTEGER , ALLOCATABLE :: adj_proc ( : ) ! > - ids of adjacent processes INTEGER , ALLOCATABLE :: num_send ( : , : ) ! > - num sends for : INTEGER , ALLOCATABLE :: num_recv ( : , : ) ! > - num receives for : ! > 1 geometric order k ; ! > 2 adjacent process ! global_mpi ! =============================================================================== !","tags":"","loc":"sourcefile/global_mpi.inc.html","title":"global_mpi.inc – ParaGEMS"},{"text":"Define global mesh variables general mesh information dimensions of complex and embedding space dimension of complex dimension of embedding global mesh information global mesh and partitioning variables global # of elements (p/e/f/v) num partitioning elements per node global offset of partitioning elements num extra partitioning elements after initial basic partitioning local mesh information number of local mesh entities num local elements (p/e/f/v) Contents Source Code global_mesh.inc Source Code ! ! =============================================================================== ! /****v* modules|common/global_mesh !* PURPOSE !*   Define global mesh variables !* CONTAINS !*   Structure/Variable      Purpose !*   general mesh info       - dimensions of complex and embedding space !*   global mesh info        - global mesh and partitioning variables !*   local mesh info         - number of local mesh entities !* AUTHOR !*   Pieter Boom !* MODIFICATION HISTORY !*   2019/08/20: Created (PB) !* COPYRIGHT !*   (c) University of Manchester !******/ ! > author : Pieter Boom ! > date : 2019 / 08 / 20 ! > Define global mesh variables ! =============================================================================== ! > # general mesh information ! > dimensions of complex and embedding space ! > ## INTEGER :: dim_cmplx ! > - dimension of complex INTEGER :: dim_embbd ! > - dimension of embedding ! > # global mesh information ! > global mesh and partitioning variables ! > ## INTEGER :: glb_num_elm ( 4 ) = 0 ! > - global # of elements ( p / e / f / v ) INTEGER , ALLOCATABLE :: num_pelm_pp ( : ) ! > - num partitioning elements per node INTEGER :: glb_offset = 0 ! > - global offset of partitioning elements INTEGER :: extra_pelm ! > - num extra partitioning elements ! > - after initial basic partitioning ! > # local mesh information ! > number of local mesh entities ! > ## INTEGER :: num_elm ( 4 ) = 0 ! > - num local elements ( p / e / f / v ) ! global_mesh ! =============================================================================== !","tags":"","loc":"sourcefile/global_mesh.inc.html","title":"global_mesh.inc – ParaGEMS"},{"text":"subroutine merge_rows(A, B, C, irow, frow) Arguments Type Intent Optional Attributes Name real(kind=iwp), intent(in) :: A (:,:) real(kind=iwp), intent(in) :: B (:,:) real(kind=iwp), intent(inout) :: C (:,:) integer, intent(in) :: irow integer, intent(in) :: frow Contents Variables i ir j k is_le Variables Type Visibility Attributes Name Initial integer, public :: i integer, public :: ir integer, public :: j integer, public :: k logical, public :: is_le","tags":"","loc":"proc/merge_rows.html","title":"merge_rows – ParaGEMS"},{"text":"subroutine merge_sort_rows(A, length, irow, frow, work) Arguments Type Intent Optional Attributes Name real(kind=iwp), intent(inout) :: A (:,:) integer, intent(in) :: length integer, intent(in) :: irow integer, intent(in) :: frow real(kind=iwp), intent(inout) :: work (:,:) Called by proc~~merge_sort_rows~~CalledByGraph proc~merge_sort_rows merge_sort_rows program~simpleloadbalance simpleLoadBalance program~simpleloadbalance:->proc~merge_sort_rows: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables half i ir Variables Type Visibility Attributes Name Initial integer, public :: half integer, public :: i integer, public :: ir","tags":"","loc":"proc/merge_sort_rows.html","title":"merge_sort_rows – ParaGEMS"},{"text":"public subroutine exchange_glb_indx(k) Exchange global indices to adjacent process (ghost nodes) Arguments Type Intent Optional Attributes Name integer, intent(in) :: k simplicial order Calls proc~~exchange_glb_indx~~CallsGraph proc~exchange_glb_indx exchange_glb_indx lcl_complex lcl_complex proc~exchange_glb_indx:->lcl_complex: mpi_waitall mpi_waitall proc~exchange_glb_indx:->mpi_waitall: mpi_request_free mpi_request_free proc~exchange_glb_indx:->mpi_request_free: mpi_barrier mpi_barrier proc~exchange_glb_indx:->mpi_barrier: num_send num_send proc~exchange_glb_indx:->num_send: adj_proc adj_proc proc~exchange_glb_indx:->adj_proc: num_recv num_recv proc~exchange_glb_indx:->num_recv: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~exchange_glb_indx~~CalledByGraph proc~exchange_glb_indx exchange_glb_indx proc~get_glb_indx get_glb_indx proc~get_glb_indx:->proc~exchange_glb_indx: proc~parallel_setup parallel_setup proc~parallel_setup:->proc~get_glb_indx: program~darcy_1f darcy_1f program~darcy_1f:->proc~parallel_setup: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~parallel_setup: program~darcy darcy program~darcy:->proc~parallel_setup: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->proc~parallel_setup: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~parallel_setup: program~darcy_2f darcy_2f program~darcy_2f:->proc~parallel_setup: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables buffer_size i j junk kp ptr ptrp rbuffer req sbuffer status Variables Type Visibility Attributes Name Initial integer, public :: buffer_size size of comm buffer integer, public :: i loop index integer, public :: j loop index integer, public :: junk junk comm variable integer, public :: kp simplicial order integer, public :: ptr pointer integer, public :: ptrp pointer integer, public, ALLOCATABLE :: rbuffer (:) receive buffer integer, public, ALLOCATABLE :: req (:) request var (non-blking comm) integer, public, ALLOCATABLE :: sbuffer (:) send buffer integer, public, ALLOCATABLE :: status (:,:) size of MPI comm buffer","tags":"","loc":"proc/exchange_glb_indx.html","title":"exchange_glb_indx – ParaGEMS"},{"text":"public subroutine exchange_prml_nodes() Exchange nodal locations to adjacent process (ghost nodes) Arguments None Calls proc~~exchange_prml_nodes~~CallsGraph proc~exchange_prml_nodes exchange_prml_nodes lcl_complex lcl_complex proc~exchange_prml_nodes:->lcl_complex: mpi_waitall mpi_waitall proc~exchange_prml_nodes:->mpi_waitall: mpi_request_free mpi_request_free proc~exchange_prml_nodes:->mpi_request_free: mpi_barrier mpi_barrier proc~exchange_prml_nodes:->mpi_barrier: num_send num_send proc~exchange_prml_nodes:->num_send: adj_proc adj_proc proc~exchange_prml_nodes:->adj_proc: num_recv num_recv proc~exchange_prml_nodes:->num_recv: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~exchange_prml_nodes~~CalledByGraph proc~exchange_prml_nodes exchange_prml_nodes proc~parallel_setup parallel_setup proc~parallel_setup:->proc~exchange_prml_nodes: program~darcy_1f darcy_1f program~darcy_1f:->proc~parallel_setup: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~parallel_setup: program~darcy darcy program~darcy:->proc~parallel_setup: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->proc~parallel_setup: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~parallel_setup: program~darcy_2f darcy_2f program~darcy_2f:->proc~parallel_setup: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables buffer_size j junk ptr req status rbuffer sbuffer Variables Type Visibility Attributes Name Initial integer, public :: buffer_size size of comm buffer integer, public :: j loop index integer, public :: junk junk comm variable integer, public :: ptr pointer integer, public, ALLOCATABLE :: req (:) request var (non-blking comm) integer, public, ALLOCATABLE :: status (:,:) size of MPI comm buffer real(kind=iwp), public, ALLOCATABLE :: rbuffer (:) send buffer real(kind=iwp), public, ALLOCATABLE :: sbuffer (:) send buffer","tags":"","loc":"proc/exchange_prml_nodes.html","title":"exchange_prml_nodes – ParaGEMS"},{"text":"public subroutine get_connectivity() Determine cross-process entity connections for all geometric orders Arguments None Calls proc~~get_connectivity~~CallsGraph proc~get_connectivity get_connectivity proc~rootwrite_log rootwrite_log proc~get_connectivity:->proc~rootwrite_log: lcl_complex lcl_complex proc~get_connectivity:->lcl_complex: adj_proc adj_proc proc~get_connectivity:->adj_proc: mpi_waitall mpi_waitall proc~get_connectivity:->mpi_waitall: mpi_request_free mpi_request_free proc~get_connectivity:->mpi_request_free: mpi_barrier mpi_barrier proc~get_connectivity:->mpi_barrier: num_send num_send proc~get_connectivity:->num_send: num_elm num_elm proc~get_connectivity:->num_elm: proc~index_in_list index_in_list proc~get_connectivity:->proc~index_in_list: num_recv num_recv proc~get_connectivity:->num_recv: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~get_connectivity~~CalledByGraph proc~get_connectivity get_connectivity proc~parallel_setup parallel_setup proc~parallel_setup:->proc~get_connectivity: program~darcy_1f darcy_1f program~darcy_1f:->proc~parallel_setup: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~parallel_setup: program~darcy darcy program~darcy:->proc~parallel_setup: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->proc~parallel_setup: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~parallel_setup: program~darcy_2f darcy_2f program~darcy_2f:->proc~parallel_setup: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables msg i j junk k km ptr ptr2 ptr3 rbuffer req sbuffer status Variables Type Visibility Attributes Name Initial character(len=slen), public :: msg integer, public :: i loop indices integer, public :: j loop indices integer, public :: junk junk variable integer, public :: k simplicial order integer, public :: km simplicial order integer, public :: ptr pointers integer, public :: ptr2 pointers integer, public :: ptr3 pointers integer, public, ALLOCATABLE :: rbuffer (:) boundary index integer, public, ALLOCATABLE :: req (:) request var (non-blking comms) integer, public, ALLOCATABLE :: sbuffer (:) boundary index integer, public, ALLOCATABLE :: status (:,:) status var (non-blking comms)","tags":"","loc":"proc/get_connectivity.html","title":"get_connectivity – ParaGEMS"},{"text":"public subroutine get_glb_indx() Get global entity indices from file Arguments None Calls proc~~get_glb_indx~~CallsGraph proc~get_glb_indx get_glb_indx proc~read_glb_indx2 read_glb_indx2 proc~get_glb_indx:->proc~read_glb_indx2: lcl_complex lcl_complex proc~get_glb_indx:->lcl_complex: num_elm num_elm proc~get_glb_indx:->num_elm: proc~exchange_glb_indx exchange_glb_indx proc~get_glb_indx:->proc~exchange_glb_indx: proc~read_glb_indx2:->lcl_complex: proc~read_glb_indx2:->num_elm: proc~end_mpi end_mpi proc~read_glb_indx2:->proc~end_mpi: proc~root_open_file_read root_open_file_read proc~read_glb_indx2:->proc~root_open_file_read: indx_offset indx_offset proc~read_glb_indx2:->indx_offset: mpi_bcast mpi_bcast proc~read_glb_indx2:->mpi_bcast: mpi_barrier mpi_barrier proc~read_glb_indx2:->mpi_barrier: proc~elm2proc elm2proc proc~read_glb_indx2:->proc~elm2proc: glb_num_elm glb_num_elm proc~read_glb_indx2:->glb_num_elm: proc~exchange_glb_indx:->lcl_complex: mpi_request_free mpi_request_free proc~exchange_glb_indx:->mpi_request_free: proc~exchange_glb_indx:->mpi_barrier: mpi_waitall mpi_waitall proc~exchange_glb_indx:->mpi_waitall: adj_proc adj_proc proc~exchange_glb_indx:->adj_proc: num_recv num_recv proc~exchange_glb_indx:->num_recv: num_send num_send proc~exchange_glb_indx:->num_send: proc~syncwrite_log syncwrite_log proc~end_mpi:->proc~syncwrite_log: mpi_finalize mpi_finalize proc~end_mpi:->mpi_finalize: proc~syncwrite_log_time syncwrite_log_time proc~end_mpi:->proc~syncwrite_log_time: proc~close_log close_log proc~end_mpi:->proc~close_log: proc~root_open_file_read:->mpi_bcast: num_pelm_pp num_pelm_pp proc~elm2proc:->num_pelm_pp: proc~syncwrite_log:->mpi_barrier: proc~syncwrite_log_time:->mpi_barrier: mpi_wtime mpi_wtime proc~syncwrite_log_time:->mpi_wtime: proc~rootwrite_log rootwrite_log proc~syncwrite_log_time:->proc~rootwrite_log: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~get_glb_indx~~CalledByGraph proc~get_glb_indx get_glb_indx proc~parallel_setup parallel_setup proc~parallel_setup:->proc~get_glb_indx: program~darcy_1f darcy_1f program~darcy_1f:->proc~parallel_setup: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~parallel_setup: program~darcy darcy program~darcy:->proc~parallel_setup: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->proc~parallel_setup: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~parallel_setup: program~darcy_2f darcy_2f program~darcy_2f:->proc~parallel_setup: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables k Variables Type Visibility Attributes Name Initial integer, public :: k simplicial order","tags":"","loc":"proc/get_glb_indx.html","title":"get_glb_indx – ParaGEMS"},{"text":"public subroutine get_glb_indx_dual_vlm() Set global index of nodes from predetermined node index Arguments None Calls proc~~get_glb_indx_dual_vlm~~CallsGraph proc~get_glb_indx_dual_vlm get_glb_indx_dual_vlm lcl_complex lcl_complex proc~get_glb_indx_dual_vlm:->lcl_complex: num_elm num_elm proc~get_glb_indx_dual_vlm:->num_elm: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~get_glb_indx_dual_vlm~~CalledByGraph proc~get_glb_indx_dual_vlm get_glb_indx_dual_vlm proc~parallel_setup parallel_setup proc~parallel_setup:->proc~get_glb_indx_dual_vlm: program~darcy_1f darcy_1f program~darcy_1f:->proc~parallel_setup: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~parallel_setup: program~darcy darcy program~darcy:->proc~parallel_setup: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->proc~parallel_setup: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~parallel_setup: program~darcy_2f darcy_2f program~darcy_2f:->proc~parallel_setup: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables k Variables Type Visibility Attributes Name Initial integer, public :: k simplicial order","tags":"","loc":"proc/get_glb_indx_dual_vlm.html","title":"get_glb_indx_dual_vlm – ParaGEMS"},{"text":"public subroutine parallel_setup() Sets up partitioning, read mesh and nodal locations, and sets connectivity Arguments None Calls proc~~parallel_setup~~CallsGraph proc~parallel_setup parallel_setup proc~syncwrite_log syncwrite_log proc~parallel_setup:->proc~syncwrite_log: proc~get_connectivity get_connectivity proc~parallel_setup:->proc~get_connectivity: proc~syncwrite_log_time syncwrite_log_time proc~parallel_setup:->proc~syncwrite_log_time: proc~calc_bndry_cobndry calc_bndry_cobndry proc~parallel_setup:->proc~calc_bndry_cobndry: proc~read_prml_elms2 read_prml_elms2 proc~parallel_setup:->proc~read_prml_elms2: proc~partition_dual partition_dual proc~parallel_setup:->proc~partition_dual: proc~read_nodes_prml read_nodes_prml proc~parallel_setup:->proc~read_nodes_prml: proc~exchange_prml_nodes exchange_prml_nodes proc~parallel_setup:->proc~exchange_prml_nodes: proc~get_glb_indx_dual_vlm get_glb_indx_dual_vlm proc~parallel_setup:->proc~get_glb_indx_dual_vlm: proc~get_glb_indx get_glb_indx proc~parallel_setup:->proc~get_glb_indx: mpi_barrier mpi_barrier proc~syncwrite_log:->mpi_barrier: mpi_waitall mpi_waitall proc~get_connectivity:->mpi_waitall: proc~get_connectivity:->mpi_barrier: num_elm num_elm proc~get_connectivity:->num_elm: proc~rootwrite_log rootwrite_log proc~get_connectivity:->proc~rootwrite_log: lcl_complex lcl_complex proc~get_connectivity:->lcl_complex: adj_proc adj_proc proc~get_connectivity:->adj_proc: proc~index_in_list index_in_list proc~get_connectivity:->proc~index_in_list: mpi_request_free mpi_request_free proc~get_connectivity:->mpi_request_free: num_send num_send proc~get_connectivity:->num_send: num_recv num_recv proc~get_connectivity:->num_recv: proc~syncwrite_log_time:->mpi_barrier: proc~syncwrite_log_time:->proc~rootwrite_log: mpi_wtime mpi_wtime proc~syncwrite_log_time:->mpi_wtime: proc~calc_bndry_cobndry:->proc~syncwrite_log: proc~calc_bndry_cobndry:->proc~syncwrite_log_time: proc~count_bndry_cobndry count_bndry_cobndry proc~calc_bndry_cobndry:->proc~count_bndry_cobndry: proc~calc_bndry_cobndry:->num_elm: proc~allocate_bndry_cobndry allocate_bndry_cobndry proc~calc_bndry_cobndry:->proc~allocate_bndry_cobndry: proc~build_bndry_work_array build_bndry_work_array proc~calc_bndry_cobndry:->proc~build_bndry_work_array: proc~set_bndry_cobndry set_bndry_cobndry proc~calc_bndry_cobndry:->proc~set_bndry_cobndry: proc~read_prml_elms2:->mpi_barrier: proc~read_prml_elms2:->num_elm: proc~root_open_file_read root_open_file_read proc~read_prml_elms2:->proc~root_open_file_read: proc~elm2proc elm2proc proc~read_prml_elms2:->proc~elm2proc: mpi_bcast mpi_bcast proc~read_prml_elms2:->mpi_bcast: proc~read_prml_elms2:->lcl_complex: num_pelm_pp num_pelm_pp proc~read_prml_elms2:->num_pelm_pp: glb_num_elm glb_num_elm proc~read_prml_elms2:->glb_num_elm: indx_offset indx_offset proc~read_prml_elms2:->indx_offset: proc~partition_dual:->num_elm: proc~partition_dual:->mpi_bcast: proc~partition_dual:->lcl_complex: proc~partition_dual:->num_pelm_pp: proc~partition_dual:->glb_num_elm: proc~partition_dual:->indx_offset: mpi_recv mpi_recv proc~read_nodes_prml:->mpi_recv: proc~read_nodes_prml:->num_elm: proc~read_nodes_prml:->proc~root_open_file_read: mpi_send mpi_send proc~read_nodes_prml:->mpi_send: proc~read_nodes_prml:->mpi_bcast: idnint idnint proc~read_nodes_prml:->idnint: proc~read_nodes_prml:->lcl_complex: proc~read_nodes_prml:->num_pelm_pp: proc~read_nodes_prml:->glb_num_elm: proc~read_nodes_prml:->indx_offset: proc~exchange_prml_nodes:->mpi_waitall: proc~exchange_prml_nodes:->mpi_barrier: proc~exchange_prml_nodes:->lcl_complex: proc~exchange_prml_nodes:->adj_proc: proc~exchange_prml_nodes:->mpi_request_free: proc~exchange_prml_nodes:->num_send: proc~exchange_prml_nodes:->num_recv: proc~get_glb_indx_dual_vlm:->num_elm: proc~get_glb_indx_dual_vlm:->lcl_complex: proc~get_glb_indx:->num_elm: proc~read_glb_indx2 read_glb_indx2 proc~get_glb_indx:->proc~read_glb_indx2: proc~get_glb_indx:->lcl_complex: proc~exchange_glb_indx exchange_glb_indx proc~get_glb_indx:->proc~exchange_glb_indx: proc~count_bndry_cobndry:->num_elm: proc~count_bndry_cobndry:->lcl_complex: proc~any_element_in_list any_element_in_list proc~count_bndry_cobndry:->proc~any_element_in_list: proc~read_glb_indx2:->mpi_barrier: proc~read_glb_indx2:->num_elm: proc~read_glb_indx2:->proc~root_open_file_read: proc~read_glb_indx2:->proc~elm2proc: proc~read_glb_indx2:->mpi_bcast: proc~read_glb_indx2:->lcl_complex: proc~read_glb_indx2:->glb_num_elm: proc~read_glb_indx2:->indx_offset: proc~end_mpi end_mpi proc~read_glb_indx2:->proc~end_mpi: proc~root_open_file_read:->mpi_bcast: proc~elm2proc:->num_pelm_pp: proc~allocate_bndry_cobndry:->num_elm: proc~allocate_bndry_cobndry:->lcl_complex: proc~build_bndry_work_array:->num_elm: proc~build_bndry_work_array:->lcl_complex: proc~exchange_glb_indx:->mpi_waitall: proc~exchange_glb_indx:->mpi_barrier: proc~exchange_glb_indx:->lcl_complex: proc~exchange_glb_indx:->adj_proc: proc~exchange_glb_indx:->mpi_request_free: proc~exchange_glb_indx:->num_send: proc~exchange_glb_indx:->num_recv: proc~set_bndry_cobndry:->num_elm: proc~set_bndry_cobndry:->proc~elm2proc: proc~set_bndry_cobndry:->lcl_complex: proc~end_mpi:->proc~syncwrite_log: proc~end_mpi:->proc~syncwrite_log_time: mpi_finalize mpi_finalize proc~end_mpi:->mpi_finalize: proc~close_log close_log proc~end_mpi:->proc~close_log: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~parallel_setup~~CalledByGraph proc~parallel_setup parallel_setup program~darcy_1f darcy_1f program~darcy_1f:->proc~parallel_setup: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~parallel_setup: program~darcy darcy program~darcy:->proc~parallel_setup: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->proc~parallel_setup: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~parallel_setup: program~darcy_2f darcy_2f program~darcy_2f:->proc~parallel_setup: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables i tmp_time tmp_time_int Variables Type Visibility Attributes Name Initial integer, public :: i counter real(kind=IWP), public :: tmp_time temporary timing variable real(kind=IWP), public :: tmp_time_int temporary timing variable","tags":"","loc":"proc/parallel_setup.html","title":"parallel_setup – ParaGEMS"},{"text":"public subroutine partition_dual() Partitions dual volumes across available processes\n Assumption: Mesh is a simplicial complex in TetGen format Arguments None Calls proc~~partition_dual~~CallsGraph proc~partition_dual partition_dual indx_offset indx_offset proc~partition_dual:->indx_offset: mpi_bcast mpi_bcast proc~partition_dual:->mpi_bcast: lcl_complex lcl_complex proc~partition_dual:->lcl_complex: num_pelm_pp num_pelm_pp proc~partition_dual:->num_pelm_pp: num_elm num_elm proc~partition_dual:->num_elm: glb_num_elm glb_num_elm proc~partition_dual:->glb_num_elm: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~partition_dual~~CalledByGraph proc~partition_dual partition_dual proc~parallel_setup parallel_setup proc~parallel_setup:->proc~partition_dual: program~darcy_1f darcy_1f program~darcy_1f:->proc~parallel_setup: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~parallel_setup: program~darcy darcy program~darcy:->proc~parallel_setup: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->proc~parallel_setup: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~parallel_setup: program~darcy_2f darcy_2f program~darcy_2f:->proc~parallel_setup: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables fname idx int_buffer junk k err Variables Type Visibility Attributes Name Initial character(len=slen), public :: fname file name integer, public :: idx index variable integer, public :: int_buffer (6) integer buffer for MPI comms integer, public :: junk junk IO variable integer, public :: k simplicial order logical, public :: err = .FALSE. error variable","tags":"","loc":"proc/partition_dual.html","title":"partition_dual – ParaGEMS"},{"text":"public subroutine t_end_mpi() Arguments None Calls proc~~t_end_mpi~~CallsGraph proc~t_end_mpi t_end_mpi mpi_finalize mpi_finalize proc~t_end_mpi:->mpi_finalize: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents None","tags":"","loc":"proc/t_end_mpi.html","title":"t_end_mpi – ParaGEMS"},{"text":"public subroutine t_parallel_setup() Arguments None Contents None","tags":"","loc":"proc/t_parallel_setup.html","title":"t_parallel_setup – ParaGEMS"},{"text":"public subroutine t_partition() Arguments None Contents Variables fname extras int_buffer junk Variables Type Visibility Attributes Name Initial character(len=slen), public :: fname integer, public :: extras integer, public :: int_buffer (2) integer, public :: junk","tags":"","loc":"proc/t_partition.html","title":"t_partition – ParaGEMS"},{"text":"public subroutine t_start_mpi() Arguments None Calls proc~~t_start_mpi~~CallsGraph proc~t_start_mpi t_start_mpi proc~end_mpi end_mpi proc~t_start_mpi:->proc~end_mpi: mpi_comm_size mpi_comm_size proc~t_start_mpi:->mpi_comm_size: mpi_init mpi_init proc~t_start_mpi:->mpi_init: mpi_comm_rank mpi_comm_rank proc~t_start_mpi:->mpi_comm_rank: proc~syncwrite_log syncwrite_log proc~end_mpi:->proc~syncwrite_log: mpi_finalize mpi_finalize proc~end_mpi:->mpi_finalize: proc~syncwrite_log_time syncwrite_log_time proc~end_mpi:->proc~syncwrite_log_time: proc~close_log close_log proc~end_mpi:->proc~close_log: mpi_barrier mpi_barrier proc~syncwrite_log:->mpi_barrier: mpi_wtime mpi_wtime proc~syncwrite_log_time:->mpi_wtime: proc~rootwrite_log rootwrite_log proc~syncwrite_log_time:->proc~rootwrite_log: proc~syncwrite_log_time:->mpi_barrier: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents None","tags":"","loc":"proc/t_start_mpi.html","title":"t_start_mpi – ParaGEMS"},{"text":"public subroutine test_vars(cnt) Arguments Type Intent Optional Attributes Name integer, intent(inout) :: cnt Contents None","tags":"","loc":"proc/test_vars.html","title":"test_vars – ParaGEMS"},{"text":"public function elm2proc(id) Convert partitioning element to process number (rank) Arguments Type Intent Optional Attributes Name integer, intent(in) :: id Return Value integer Calls proc~~elm2proc~~CallsGraph proc~elm2proc elm2proc num_pelm_pp num_pelm_pp proc~elm2proc:->num_pelm_pp: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~elm2proc~~CalledByGraph proc~elm2proc elm2proc proc~read_bndry_cond read_bndry_cond proc~read_bndry_cond:->proc~elm2proc: proc~read_prml_elms2 read_prml_elms2 proc~read_prml_elms2:->proc~elm2proc: proc~read_crack_faces read_crack_faces proc~read_crack_faces:->proc~elm2proc: proc~set_bndry_cobndry set_bndry_cobndry proc~set_bndry_cobndry:->proc~elm2proc: proc~read_prml_elms read_prml_elms proc~read_prml_elms:->proc~elm2proc: proc~read_glb_indx2 read_glb_indx2 proc~read_glb_indx2:->proc~elm2proc: proc~read_glb_indx read_glb_indx proc~read_glb_indx:->proc~elm2proc: proc~calc_bndry_cobndry calc_bndry_cobndry proc~calc_bndry_cobndry:->proc~set_bndry_cobndry: proc~parallel_setup parallel_setup proc~parallel_setup:->proc~read_prml_elms2: proc~parallel_setup:->proc~calc_bndry_cobndry: proc~get_glb_indx get_glb_indx proc~parallel_setup:->proc~get_glb_indx: proc~get_glb_indx:->proc~read_glb_indx2: program~darcy_1f darcy_1f program~darcy_1f:->proc~parallel_setup: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~parallel_setup: program~darcy darcy program~darcy:->proc~parallel_setup: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->proc~parallel_setup: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~parallel_setup: program~darcy_2f darcy_2f program~darcy_2f:->proc~parallel_setup: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents None","tags":"","loc":"proc/elm2proc.html","title":"elm2proc – ParaGEMS"},{"text":"public subroutine close_log() Close log file from root process Arguments None Called by proc~~close_log~~CalledByGraph proc~close_log close_log proc~end_mpi end_mpi proc~end_mpi:->proc~close_log: program~darcy_1f darcy_1f program~darcy_1f:->proc~end_mpi: proc~start_mpi start_mpi program~darcy_1f:->proc~start_mpi: proc~start_petsc start_petsc program~darcy_1f:->proc~start_petsc: proc~read_input_darcy read_input_darcy program~darcy_1f:->proc~read_input_darcy: proc~parallel_setup parallel_setup program~darcy_1f:->proc~parallel_setup: proc~start_mpi:->proc~end_mpi: proc~start_petsc:->proc~end_mpi: proc~read_bndry_cond read_bndry_cond proc~read_bndry_cond:->proc~end_mpi: proc~set_defaults_solver set_defaults_solver proc~set_defaults_solver:->proc~end_mpi: proc~read_input_darcy:->proc~end_mpi: proc~int_merge_rows int_merge_rows proc~int_merge_rows:->proc~end_mpi: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~end_mpi: program~darcy_crkp_2f:->proc~start_mpi: program~darcy_crkp_2f:->proc~start_petsc: program~darcy_crkp_2f:->proc~read_input_darcy: program~darcy_crkp_2f:->proc~parallel_setup: program~darcy darcy program~darcy:->proc~end_mpi: program~darcy:->proc~start_mpi: program~darcy:->proc~start_petsc: program~darcy:->proc~read_input_darcy: program~darcy:->proc~parallel_setup: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->proc~end_mpi: program~darcy_crkp_1f:->proc~start_mpi: program~darcy_crkp_1f:->proc~start_petsc: program~darcy_crkp_1f:->proc~read_input_darcy: program~darcy_crkp_1f:->proc~parallel_setup: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~end_mpi: program~darcy_crkp_2f~2:->proc~start_mpi: program~darcy_crkp_2f~2:->proc~start_petsc: program~darcy_crkp_2f~2:->proc~read_input_darcy: program~darcy_crkp_2f~2:->proc~parallel_setup: proc~read_glb_indx2 read_glb_indx2 proc~read_glb_indx2:->proc~end_mpi: proc~t_start_mpi t_start_mpi proc~t_start_mpi:->proc~end_mpi: proc~read_glb_indx read_glb_indx proc~read_glb_indx:->proc~end_mpi: program~darcy_2f darcy_2f program~darcy_2f:->proc~end_mpi: program~darcy_2f:->proc~start_mpi: program~darcy_2f:->proc~start_petsc: program~darcy_2f:->proc~read_input_darcy: program~darcy_2f:->proc~parallel_setup: proc~get_glb_indx get_glb_indx proc~get_glb_indx:->proc~read_glb_indx2: proc~parallel_setup:->proc~get_glb_indx: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents None","tags":"","loc":"proc/close_log.html","title":"close_log – ParaGEMS"},{"text":"public subroutine close_unsteady_log() Close log file for unsteady simulations from root Arguments None Called by proc~~close_unsteady_log~~CalledByGraph proc~close_unsteady_log close_unsteady_log program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~close_unsteady_log: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~close_unsteady_log: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents None","tags":"","loc":"proc/close_unsteady_log.html","title":"close_unsteady_log – ParaGEMS"},{"text":"public subroutine end_mpi() End MPI execution environment & stop ParaGEMS Arguments None Calls proc~~end_mpi~~CallsGraph proc~end_mpi end_mpi proc~syncwrite_log syncwrite_log proc~end_mpi:->proc~syncwrite_log: mpi_finalize mpi_finalize proc~end_mpi:->mpi_finalize: proc~syncwrite_log_time syncwrite_log_time proc~end_mpi:->proc~syncwrite_log_time: proc~close_log close_log proc~end_mpi:->proc~close_log: mpi_barrier mpi_barrier proc~syncwrite_log:->mpi_barrier: mpi_wtime mpi_wtime proc~syncwrite_log_time:->mpi_wtime: proc~rootwrite_log rootwrite_log proc~syncwrite_log_time:->proc~rootwrite_log: proc~syncwrite_log_time:->mpi_barrier: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~end_mpi~~CalledByGraph proc~end_mpi end_mpi program~darcy_1f darcy_1f program~darcy_1f:->proc~end_mpi: proc~start_mpi start_mpi program~darcy_1f:->proc~start_mpi: proc~start_petsc start_petsc program~darcy_1f:->proc~start_petsc: proc~read_input_darcy read_input_darcy program~darcy_1f:->proc~read_input_darcy: proc~parallel_setup parallel_setup program~darcy_1f:->proc~parallel_setup: proc~start_mpi:->proc~end_mpi: proc~start_petsc:->proc~end_mpi: proc~read_bndry_cond read_bndry_cond proc~read_bndry_cond:->proc~end_mpi: proc~set_defaults_solver set_defaults_solver proc~set_defaults_solver:->proc~end_mpi: proc~read_input_darcy:->proc~end_mpi: proc~int_merge_rows int_merge_rows proc~int_merge_rows:->proc~end_mpi: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~end_mpi: program~darcy_crkp_2f:->proc~start_mpi: program~darcy_crkp_2f:->proc~start_petsc: program~darcy_crkp_2f:->proc~read_input_darcy: program~darcy_crkp_2f:->proc~parallel_setup: program~darcy darcy program~darcy:->proc~end_mpi: program~darcy:->proc~start_mpi: program~darcy:->proc~start_petsc: program~darcy:->proc~read_input_darcy: program~darcy:->proc~parallel_setup: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->proc~end_mpi: program~darcy_crkp_1f:->proc~start_mpi: program~darcy_crkp_1f:->proc~start_petsc: program~darcy_crkp_1f:->proc~read_input_darcy: program~darcy_crkp_1f:->proc~parallel_setup: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~end_mpi: program~darcy_crkp_2f~2:->proc~start_mpi: program~darcy_crkp_2f~2:->proc~start_petsc: program~darcy_crkp_2f~2:->proc~read_input_darcy: program~darcy_crkp_2f~2:->proc~parallel_setup: proc~read_glb_indx2 read_glb_indx2 proc~read_glb_indx2:->proc~end_mpi: proc~t_start_mpi t_start_mpi proc~t_start_mpi:->proc~end_mpi: proc~read_glb_indx read_glb_indx proc~read_glb_indx:->proc~end_mpi: program~darcy_2f darcy_2f program~darcy_2f:->proc~end_mpi: program~darcy_2f:->proc~start_mpi: program~darcy_2f:->proc~start_petsc: program~darcy_2f:->proc~read_input_darcy: program~darcy_2f:->proc~parallel_setup: proc~get_glb_indx get_glb_indx proc~get_glb_indx:->proc~read_glb_indx2: proc~parallel_setup:->proc~get_glb_indx: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables str_out Variables Type Visibility Attributes Name Initial character(len=slen), public :: str_out STOP output string","tags":"","loc":"proc/end_mpi.html","title":"end_mpi – ParaGEMS"},{"text":"public subroutine open_log() Open log file from root process and write ParaGEMS header Arguments None Calls proc~~open_log~~CallsGraph proc~open_log open_log mpi_bcast mpi_bcast proc~open_log:->mpi_bcast: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~open_log~~CalledByGraph proc~open_log open_log proc~start_mpi start_mpi proc~start_mpi:->proc~open_log: program~darcy_1f darcy_1f program~darcy_1f:->proc~start_mpi: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~start_mpi: program~darcy darcy program~darcy:->proc~start_mpi: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->proc~start_mpi: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~start_mpi: program~darcy_2f darcy_2f program~darcy_2f:->proc~start_mpi: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables fname i err fexists Variables Type Visibility Attributes Name Initial character(len=slen), public :: fname file name integer, public :: i = 0 counter logical, public :: err = .FALSE. error variable logical, public :: fexists logical for file existence","tags":"","loc":"proc/open_log.html","title":"open_log – ParaGEMS"},{"text":"public subroutine open_unsteady_log() Open log file for unsteady simulations from root process and write header Arguments None Calls proc~~open_unsteady_log~~CallsGraph proc~open_unsteady_log open_unsteady_log mpi_bcast mpi_bcast proc~open_unsteady_log:->mpi_bcast: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~open_unsteady_log~~CalledByGraph proc~open_unsteady_log open_unsteady_log program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~open_unsteady_log: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~open_unsteady_log: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables fname err Variables Type Visibility Attributes Name Initial character(len=slen), public :: fname file name logical, public :: err = .FALSE. error variable","tags":"","loc":"proc/open_unsteady_log.html","title":"open_unsteady_log – ParaGEMS"},{"text":"public subroutine rootwrite_log(msg) Write unsynchronised message from root process to log file Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: msg message string Called by proc~~rootwrite_log~~CalledByGraph proc~rootwrite_log rootwrite_log proc~initialise_geo initialise_geo proc~initialise_geo:->proc~rootwrite_log: proc~syncwrite_log_time syncwrite_log_time proc~initialise_geo:->proc~syncwrite_log_time: proc~get_connectivity get_connectivity proc~get_connectivity:->proc~rootwrite_log: proc~syncwrite_log_time:->proc~rootwrite_log: program~darcy_1f darcy_1f program~darcy_1f:->proc~rootwrite_log: proc~read_input_darcy read_input_darcy program~darcy_1f:->proc~read_input_darcy: proc~end_mpi end_mpi program~darcy_1f:->proc~end_mpi: proc~parallel_setup parallel_setup program~darcy_1f:->proc~parallel_setup: proc~get_rhs_darcy get_RHS_darcy program~darcy_1f:->proc~get_rhs_darcy: proc~get_lhs_darcy get_LHS_darcy program~darcy_1f:->proc~get_lhs_darcy: proc~initialise_darcy initialise_darcy program~darcy_1f:->proc~initialise_darcy: proc~start_mpi start_mpi program~darcy_1f:->proc~start_mpi: proc~start_petsc start_petsc program~darcy_1f:->proc~start_petsc: proc~check_param_darcy check_param_darcy proc~check_param_darcy:->proc~rootwrite_log: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~rootwrite_log: program~darcy_crkp_2f:->proc~read_input_darcy: program~darcy_crkp_2f:->proc~end_mpi: program~darcy_crkp_2f:->proc~parallel_setup: proc~identify_crack6 identify_crack6 program~darcy_crkp_2f:->proc~identify_crack6: proc~initialise_darcy2 initialise_darcy2 program~darcy_crkp_2f:->proc~initialise_darcy2: proc~identify_crack7 identify_crack7 program~darcy_crkp_2f:->proc~identify_crack7: proc~identify_crack5 identify_crack5 program~darcy_crkp_2f:->proc~identify_crack5: proc~identify_crack4 identify_crack4 program~darcy_crkp_2f:->proc~identify_crack4: proc~get_lhs_darcy2 get_LHS_darcy2 program~darcy_crkp_2f:->proc~get_lhs_darcy2: proc~extract_sol_ksp2 extract_sol_KSP2 program~darcy_crkp_2f:->proc~extract_sol_ksp2: proc~get_rhs_darcy2 get_RHS_darcy2 program~darcy_crkp_2f:->proc~get_rhs_darcy2: program~darcy_crkp_2f:->proc~start_mpi: program~darcy_crkp_2f:->proc~start_petsc: proc~read_input_darcy:->proc~rootwrite_log: proc~read_input_darcy:->proc~check_param_darcy: proc~read_input_darcy:->proc~end_mpi: program~darcy darcy program~darcy:->proc~rootwrite_log: program~darcy:->proc~read_input_darcy: program~darcy:->proc~end_mpi: program~darcy:->proc~parallel_setup: program~darcy:->proc~get_rhs_darcy: program~darcy:->proc~get_lhs_darcy: program~darcy:->proc~initialise_darcy: program~darcy:->proc~start_mpi: program~darcy:->proc~start_petsc: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->proc~rootwrite_log: program~darcy_crkp_1f:->proc~read_input_darcy: program~darcy_crkp_1f:->proc~end_mpi: program~darcy_crkp_1f:->proc~parallel_setup: program~darcy_crkp_1f:->proc~get_rhs_darcy: program~darcy_crkp_1f:->proc~get_lhs_darcy: program~darcy_crkp_1f:->proc~initialise_darcy: program~darcy_crkp_1f:->proc~start_mpi: program~darcy_crkp_1f:->proc~start_petsc: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~rootwrite_log: program~darcy_crkp_2f~2:->proc~read_input_darcy: program~darcy_crkp_2f~2:->proc~end_mpi: proc~extract_sol_ksp extract_sol_KSP program~darcy_crkp_2f~2:->proc~extract_sol_ksp: program~darcy_crkp_2f~2:->proc~parallel_setup: program~darcy_crkp_2f~2:->proc~get_rhs_darcy: program~darcy_crkp_2f~2:->proc~get_lhs_darcy: proc~identify_crack identify_crack program~darcy_crkp_2f~2:->proc~identify_crack: program~darcy_crkp_2f~2:->proc~initialise_darcy: proc~identify_crack3 identify_crack3 program~darcy_crkp_2f~2:->proc~identify_crack3: program~darcy_crkp_2f~2:->proc~start_mpi: program~darcy_crkp_2f~2:->proc~start_petsc: program~darcy_2f darcy_2f program~darcy_2f:->proc~rootwrite_log: program~darcy_2f:->proc~read_input_darcy: program~darcy_2f:->proc~end_mpi: program~darcy_2f:->proc~parallel_setup: program~darcy_2f:->proc~initialise_darcy2: program~darcy_2f:->proc~get_lhs_darcy2: program~darcy_2f:->proc~extract_sol_ksp2: program~darcy_2f:->proc~get_rhs_darcy2: program~darcy_2f:->proc~start_mpi: program~darcy_2f:->proc~start_petsc: proc~end_mpi:->proc~syncwrite_log_time: proc~extract_sol_ksp:->proc~syncwrite_log_time: proc~parallel_setup:->proc~get_connectivity: proc~parallel_setup:->proc~syncwrite_log_time: proc~calc_bndry_cobndry calc_bndry_cobndry proc~parallel_setup:->proc~calc_bndry_cobndry: proc~get_glb_indx get_glb_indx proc~parallel_setup:->proc~get_glb_indx: proc~write_unsteady_d0s2 write_unsteady_D0S2 proc~write_unsteady_d0s2:->proc~syncwrite_log_time: proc~identify_crack6:->proc~syncwrite_log_time: proc~get_rhs_darcy:->proc~syncwrite_log_time: proc~write_solution_d0s2 write_solution_D0S2 proc~write_solution_d0s2:->proc~syncwrite_log_time: proc~get_lhs_darcy:->proc~syncwrite_log_time: proc~identify_crack:->proc~syncwrite_log_time: proc~initialise_darcy2:->proc~syncwrite_log_time: proc~identify_crack7:->proc~syncwrite_log_time: proc~calc_bndry_cobndry:->proc~syncwrite_log_time: proc~identify_crack5:->proc~syncwrite_log_time: proc~identify_crack4:->proc~syncwrite_log_time: proc~write_unsteady_d0s write_unsteady_D0S proc~write_unsteady_d0s:->proc~syncwrite_log_time: proc~identify_crack2 identify_crack2 proc~identify_crack2:->proc~syncwrite_log_time: proc~initialise_darcy:->proc~syncwrite_log_time: proc~get_lhs_darcy2:->proc~syncwrite_log_time: proc~extract_sol_ksp2:->proc~syncwrite_log_time: proc~identify_crack3:->proc~syncwrite_log_time: proc~get_rhs_darcy2:->proc~syncwrite_log_time: proc~start_mpi:->proc~end_mpi: proc~start_petsc:->proc~end_mpi: proc~read_bndry_cond read_bndry_cond proc~read_bndry_cond:->proc~end_mpi: proc~set_defaults_solver set_defaults_solver proc~set_defaults_solver:->proc~end_mpi: proc~int_merge_rows int_merge_rows proc~int_merge_rows:->proc~end_mpi: proc~read_glb_indx2 read_glb_indx2 proc~read_glb_indx2:->proc~end_mpi: proc~t_start_mpi t_start_mpi proc~t_start_mpi:->proc~end_mpi: proc~read_glb_indx read_glb_indx proc~read_glb_indx:->proc~end_mpi: proc~get_glb_indx:->proc~read_glb_indx2: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents None","tags":"","loc":"proc/rootwrite_log.html","title":"rootwrite_log – ParaGEMS"},{"text":"public subroutine start_mpi() Start MPI execution environment & get details Arguments None Calls proc~~start_mpi~~CallsGraph proc~start_mpi start_mpi proc~end_mpi end_mpi proc~start_mpi:->proc~end_mpi: mpi_comm_size mpi_comm_size proc~start_mpi:->mpi_comm_size: proc~syncwrite_log_mpidata syncwrite_log_mpidata proc~start_mpi:->proc~syncwrite_log_mpidata: proc~open_log open_log proc~start_mpi:->proc~open_log: mpi_init mpi_init proc~start_mpi:->mpi_init: mpi_barrier mpi_barrier proc~start_mpi:->mpi_barrier: mpi_wtime mpi_wtime proc~start_mpi:->mpi_wtime: mpi_comm_rank mpi_comm_rank proc~start_mpi:->mpi_comm_rank: proc~syncwrite_log syncwrite_log proc~end_mpi:->proc~syncwrite_log: mpi_finalize mpi_finalize proc~end_mpi:->mpi_finalize: proc~syncwrite_log_time syncwrite_log_time proc~end_mpi:->proc~syncwrite_log_time: proc~close_log close_log proc~end_mpi:->proc~close_log: proc~syncwrite_log_mpidata:->mpi_barrier: mpi_bcast mpi_bcast proc~open_log:->mpi_bcast: proc~syncwrite_log:->mpi_barrier: proc~syncwrite_log_time:->mpi_barrier: proc~syncwrite_log_time:->mpi_wtime: proc~rootwrite_log rootwrite_log proc~syncwrite_log_time:->proc~rootwrite_log: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~start_mpi~~CalledByGraph proc~start_mpi start_mpi program~darcy_1f darcy_1f program~darcy_1f:->proc~start_mpi: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~start_mpi: program~darcy darcy program~darcy:->proc~start_mpi: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->proc~start_mpi: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~start_mpi: program~darcy_2f darcy_2f program~darcy_2f:->proc~start_mpi: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents None","tags":"","loc":"proc/start_mpi.html","title":"start_mpi – ParaGEMS"},{"text":"public subroutine syncwrite_log(msg) Write uynchronised message from root process to log file Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: msg message string Calls proc~~syncwrite_log~~CallsGraph proc~syncwrite_log syncwrite_log mpi_barrier mpi_barrier proc~syncwrite_log:->mpi_barrier: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~syncwrite_log~~CalledByGraph proc~syncwrite_log syncwrite_log proc~end_mpi end_mpi proc~end_mpi:->proc~syncwrite_log: proc~initialise_darcy initialise_darcy proc~initialise_darcy:->proc~syncwrite_log: program~darcy darcy program~darcy:->proc~syncwrite_log: program~darcy:->proc~end_mpi: program~darcy:->proc~initialise_darcy: proc~parallel_setup parallel_setup program~darcy:->proc~parallel_setup: proc~get_rhs_darcy get_RHS_darcy program~darcy:->proc~get_rhs_darcy: proc~solve_ksp solve_KSP program~darcy:->proc~solve_ksp: proc~read_input_darcy read_input_darcy program~darcy:->proc~read_input_darcy: proc~start_petsc start_petsc program~darcy:->proc~start_petsc: proc~get_lhs_darcy get_LHS_darcy program~darcy:->proc~get_lhs_darcy: proc~start_mpi start_mpi program~darcy:->proc~start_mpi: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~syncwrite_log: program~darcy_crkp_2f~2:->proc~end_mpi: program~darcy_crkp_2f~2:->proc~initialise_darcy: proc~identify_crack identify_crack program~darcy_crkp_2f~2:->proc~identify_crack: program~darcy_crkp_2f~2:->proc~parallel_setup: program~darcy_crkp_2f~2:->proc~get_rhs_darcy: program~darcy_crkp_2f~2:->proc~solve_ksp: program~darcy_crkp_2f~2:->proc~read_input_darcy: program~darcy_crkp_2f~2:->proc~start_petsc: program~darcy_crkp_2f~2:->proc~get_lhs_darcy: proc~identify_crack3 identify_crack3 program~darcy_crkp_2f~2:->proc~identify_crack3: program~darcy_crkp_2f~2:->proc~start_mpi: proc~identify_crack:->proc~syncwrite_log: proc~write_unsteady_d0s2 write_unsteady_D0S2 proc~write_unsteady_d0s2:->proc~syncwrite_log: program~darcy_1f darcy_1f program~darcy_1f:->proc~syncwrite_log: program~darcy_1f:->proc~end_mpi: program~darcy_1f:->proc~initialise_darcy: program~darcy_1f:->proc~parallel_setup: program~darcy_1f:->proc~get_rhs_darcy: program~darcy_1f:->proc~solve_ksp: program~darcy_1f:->proc~read_input_darcy: program~darcy_1f:->proc~start_petsc: program~darcy_1f:->proc~get_lhs_darcy: program~darcy_1f:->proc~start_mpi: proc~write_solution_d0s2 write_solution_D0S2 proc~write_solution_d0s2:->proc~syncwrite_log: proc~initialise_darcy2 initialise_darcy2 proc~initialise_darcy2:->proc~syncwrite_log: proc~get_rhs_darcy2 get_RHS_darcy2 proc~get_rhs_darcy2:->proc~syncwrite_log: proc~calc_bndry_cobndry calc_bndry_cobndry proc~calc_bndry_cobndry:->proc~syncwrite_log: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->proc~syncwrite_log: program~darcy_crkp_1f:->proc~end_mpi: program~darcy_crkp_1f:->proc~initialise_darcy: program~darcy_crkp_1f:->proc~parallel_setup: program~darcy_crkp_1f:->proc~get_rhs_darcy: program~darcy_crkp_1f:->proc~solve_ksp: program~darcy_crkp_1f:->proc~read_input_darcy: program~darcy_crkp_1f:->proc~start_petsc: program~darcy_crkp_1f:->proc~get_lhs_darcy: program~darcy_crkp_1f:->proc~start_mpi: proc~solve_ksp2 solve_KSP2 proc~solve_ksp2:->proc~syncwrite_log: proc~parallel_setup:->proc~syncwrite_log: proc~parallel_setup:->proc~calc_bndry_cobndry: proc~get_glb_indx get_glb_indx proc~parallel_setup:->proc~get_glb_indx: proc~get_rhs_darcy:->proc~syncwrite_log: proc~get_lhs_darcy2 get_LHS_darcy2 proc~get_lhs_darcy2:->proc~syncwrite_log: proc~solve_ksp:->proc~syncwrite_log: proc~read_input_darcy:->proc~syncwrite_log: proc~read_input_darcy:->proc~end_mpi: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~syncwrite_log: program~darcy_crkp_2f:->proc~end_mpi: program~darcy_crkp_2f:->proc~initialise_darcy2: program~darcy_crkp_2f:->proc~get_rhs_darcy2: program~darcy_crkp_2f:->proc~solve_ksp2: program~darcy_crkp_2f:->proc~parallel_setup: program~darcy_crkp_2f:->proc~get_lhs_darcy2: program~darcy_crkp_2f:->proc~read_input_darcy: program~darcy_crkp_2f:->proc~start_petsc: proc~identify_crack7 identify_crack7 program~darcy_crkp_2f:->proc~identify_crack7: proc~identify_crack6 identify_crack6 program~darcy_crkp_2f:->proc~identify_crack6: proc~identify_crack5 identify_crack5 program~darcy_crkp_2f:->proc~identify_crack5: proc~identify_crack4 identify_crack4 program~darcy_crkp_2f:->proc~identify_crack4: program~darcy_crkp_2f:->proc~start_mpi: proc~write_unsteady_d0s write_unsteady_D0S proc~write_unsteady_d0s:->proc~syncwrite_log: proc~initialise_geo initialise_geo proc~initialise_geo:->proc~syncwrite_log: proc~start_petsc:->proc~syncwrite_log: proc~start_petsc:->proc~end_mpi: proc~get_lhs_darcy:->proc~syncwrite_log: proc~identify_crack7:->proc~syncwrite_log: proc~identify_crack6:->proc~syncwrite_log: proc~identify_crack5:->proc~syncwrite_log: proc~identify_crack4:->proc~syncwrite_log: proc~identify_crack3:->proc~syncwrite_log: proc~identify_crack2 identify_crack2 proc~identify_crack2:->proc~syncwrite_log: program~darcy_2f darcy_2f program~darcy_2f:->proc~syncwrite_log: program~darcy_2f:->proc~end_mpi: program~darcy_2f:->proc~initialise_darcy2: program~darcy_2f:->proc~get_rhs_darcy2: program~darcy_2f:->proc~solve_ksp2: program~darcy_2f:->proc~parallel_setup: program~darcy_2f:->proc~get_lhs_darcy2: program~darcy_2f:->proc~read_input_darcy: program~darcy_2f:->proc~start_petsc: program~darcy_2f:->proc~start_mpi: proc~start_mpi:->proc~end_mpi: proc~read_bndry_cond read_bndry_cond proc~read_bndry_cond:->proc~end_mpi: proc~set_defaults_solver set_defaults_solver proc~set_defaults_solver:->proc~end_mpi: proc~int_merge_rows int_merge_rows proc~int_merge_rows:->proc~end_mpi: proc~read_glb_indx2 read_glb_indx2 proc~read_glb_indx2:->proc~end_mpi: proc~t_start_mpi t_start_mpi proc~t_start_mpi:->proc~end_mpi: proc~read_glb_indx read_glb_indx proc~read_glb_indx:->proc~end_mpi: proc~get_glb_indx:->proc~read_glb_indx2: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents None","tags":"","loc":"proc/syncwrite_log.html","title":"syncwrite_log – ParaGEMS"},{"text":"public subroutine syncwrite_log_mpidata() Write a synchronised message with number of mpi ranks to log file Arguments None Calls proc~~syncwrite_log_mpidata~~CallsGraph proc~syncwrite_log_mpidata syncwrite_log_mpidata mpi_barrier mpi_barrier proc~syncwrite_log_mpidata:->mpi_barrier: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~syncwrite_log_mpidata~~CalledByGraph proc~syncwrite_log_mpidata syncwrite_log_mpidata proc~start_mpi start_mpi proc~start_mpi:->proc~syncwrite_log_mpidata: program~darcy_1f darcy_1f program~darcy_1f:->proc~start_mpi: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~start_mpi: program~darcy darcy program~darcy:->proc~start_mpi: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->proc~start_mpi: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~start_mpi: program~darcy_2f darcy_2f program~darcy_2f:->proc~start_mpi: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents None","tags":"","loc":"proc/syncwrite_log_mpidata.html","title":"syncwrite_log_mpidata – ParaGEMS"},{"text":"public subroutine syncwrite_log_time() Write a synchronised message to log file with timings Arguments None Calls proc~~syncwrite_log_time~~CallsGraph proc~syncwrite_log_time syncwrite_log_time mpi_wtime mpi_wtime proc~syncwrite_log_time:->mpi_wtime: proc~rootwrite_log rootwrite_log proc~syncwrite_log_time:->proc~rootwrite_log: mpi_barrier mpi_barrier proc~syncwrite_log_time:->mpi_barrier: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~syncwrite_log_time~~CalledByGraph proc~syncwrite_log_time syncwrite_log_time proc~end_mpi end_mpi proc~end_mpi:->proc~syncwrite_log_time: proc~extract_sol_ksp extract_sol_KSP proc~extract_sol_ksp:->proc~syncwrite_log_time: proc~initialise_geo initialise_geo proc~initialise_geo:->proc~syncwrite_log_time: proc~identify_crack identify_crack proc~identify_crack:->proc~syncwrite_log_time: proc~write_unsteady_d0s2 write_unsteady_D0S2 proc~write_unsteady_d0s2:->proc~syncwrite_log_time: proc~write_solution_d0s2 write_solution_D0S2 proc~write_solution_d0s2:->proc~syncwrite_log_time: proc~initialise_darcy2 initialise_darcy2 proc~initialise_darcy2:->proc~syncwrite_log_time: proc~get_rhs_darcy2 get_RHS_darcy2 proc~get_rhs_darcy2:->proc~syncwrite_log_time: proc~calc_bndry_cobndry calc_bndry_cobndry proc~calc_bndry_cobndry:->proc~syncwrite_log_time: proc~parallel_setup parallel_setup proc~parallel_setup:->proc~syncwrite_log_time: proc~parallel_setup:->proc~calc_bndry_cobndry: proc~get_glb_indx get_glb_indx proc~parallel_setup:->proc~get_glb_indx: proc~get_rhs_darcy get_RHS_darcy proc~get_rhs_darcy:->proc~syncwrite_log_time: proc~get_lhs_darcy2 get_LHS_darcy2 proc~get_lhs_darcy2:->proc~syncwrite_log_time: proc~write_unsteady_d0s write_unsteady_D0S proc~write_unsteady_d0s:->proc~syncwrite_log_time: proc~initialise_darcy initialise_darcy proc~initialise_darcy:->proc~syncwrite_log_time: proc~extract_sol_ksp2 extract_sol_KSP2 proc~extract_sol_ksp2:->proc~syncwrite_log_time: proc~get_lhs_darcy get_LHS_darcy proc~get_lhs_darcy:->proc~syncwrite_log_time: proc~identify_crack7 identify_crack7 proc~identify_crack7:->proc~syncwrite_log_time: proc~identify_crack6 identify_crack6 proc~identify_crack6:->proc~syncwrite_log_time: proc~identify_crack5 identify_crack5 proc~identify_crack5:->proc~syncwrite_log_time: proc~identify_crack4 identify_crack4 proc~identify_crack4:->proc~syncwrite_log_time: proc~identify_crack3 identify_crack3 proc~identify_crack3:->proc~syncwrite_log_time: proc~identify_crack2 identify_crack2 proc~identify_crack2:->proc~syncwrite_log_time: program~darcy_1f darcy_1f program~darcy_1f:->proc~end_mpi: program~darcy_1f:->proc~parallel_setup: program~darcy_1f:->proc~get_rhs_darcy: program~darcy_1f:->proc~initialise_darcy: program~darcy_1f:->proc~get_lhs_darcy: proc~start_mpi start_mpi program~darcy_1f:->proc~start_mpi: proc~start_petsc start_petsc program~darcy_1f:->proc~start_petsc: proc~read_input_darcy read_input_darcy program~darcy_1f:->proc~read_input_darcy: proc~start_mpi:->proc~end_mpi: proc~start_petsc:->proc~end_mpi: proc~read_bndry_cond read_bndry_cond proc~read_bndry_cond:->proc~end_mpi: proc~set_defaults_solver set_defaults_solver proc~set_defaults_solver:->proc~end_mpi: proc~read_input_darcy:->proc~end_mpi: proc~int_merge_rows int_merge_rows proc~int_merge_rows:->proc~end_mpi: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~end_mpi: program~darcy_crkp_2f:->proc~initialise_darcy2: program~darcy_crkp_2f:->proc~get_rhs_darcy2: program~darcy_crkp_2f:->proc~parallel_setup: program~darcy_crkp_2f:->proc~get_lhs_darcy2: program~darcy_crkp_2f:->proc~extract_sol_ksp2: program~darcy_crkp_2f:->proc~identify_crack7: program~darcy_crkp_2f:->proc~identify_crack6: program~darcy_crkp_2f:->proc~identify_crack5: program~darcy_crkp_2f:->proc~identify_crack4: program~darcy_crkp_2f:->proc~start_mpi: program~darcy_crkp_2f:->proc~start_petsc: program~darcy_crkp_2f:->proc~read_input_darcy: program~darcy darcy program~darcy:->proc~end_mpi: program~darcy:->proc~parallel_setup: program~darcy:->proc~get_rhs_darcy: program~darcy:->proc~initialise_darcy: program~darcy:->proc~get_lhs_darcy: program~darcy:->proc~start_mpi: program~darcy:->proc~start_petsc: program~darcy:->proc~read_input_darcy: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->proc~end_mpi: program~darcy_crkp_1f:->proc~parallel_setup: program~darcy_crkp_1f:->proc~get_rhs_darcy: program~darcy_crkp_1f:->proc~initialise_darcy: program~darcy_crkp_1f:->proc~get_lhs_darcy: program~darcy_crkp_1f:->proc~start_mpi: program~darcy_crkp_1f:->proc~start_petsc: program~darcy_crkp_1f:->proc~read_input_darcy: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~end_mpi: program~darcy_crkp_2f~2:->proc~extract_sol_ksp: program~darcy_crkp_2f~2:->proc~identify_crack: program~darcy_crkp_2f~2:->proc~parallel_setup: program~darcy_crkp_2f~2:->proc~get_rhs_darcy: program~darcy_crkp_2f~2:->proc~initialise_darcy: program~darcy_crkp_2f~2:->proc~get_lhs_darcy: program~darcy_crkp_2f~2:->proc~identify_crack3: program~darcy_crkp_2f~2:->proc~start_mpi: program~darcy_crkp_2f~2:->proc~start_petsc: program~darcy_crkp_2f~2:->proc~read_input_darcy: proc~read_glb_indx2 read_glb_indx2 proc~read_glb_indx2:->proc~end_mpi: proc~t_start_mpi t_start_mpi proc~t_start_mpi:->proc~end_mpi: proc~read_glb_indx read_glb_indx proc~read_glb_indx:->proc~end_mpi: program~darcy_2f darcy_2f program~darcy_2f:->proc~end_mpi: program~darcy_2f:->proc~initialise_darcy2: program~darcy_2f:->proc~get_rhs_darcy2: program~darcy_2f:->proc~parallel_setup: program~darcy_2f:->proc~get_lhs_darcy2: program~darcy_2f:->proc~extract_sol_ksp2: program~darcy_2f:->proc~start_mpi: program~darcy_2f:->proc~start_petsc: program~darcy_2f:->proc~read_input_darcy: proc~get_glb_indx:->proc~read_glb_indx2: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables log_msg time Variables Type Visibility Attributes Name Initial character(len=slen), public :: log_msg message string real(kind=iwp), public :: time time variable","tags":"","loc":"proc/syncwrite_log_time.html","title":"syncwrite_log_time – ParaGEMS"},{"text":"public subroutine write_log(msg) Write unsynchronised messages collated from processes to log file Arguments Type Intent Optional Attributes Name character(len=slen), intent(in) :: msg message string Calls proc~~write_log~~CallsGraph proc~write_log write_log mpi_send mpi_send proc~write_log:->mpi_send: mpi_recv mpi_recv proc~write_log:->mpi_recv: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables i status Variables Type Visibility Attributes Name Initial integer, public :: i counter integer, public :: status (MPI_STATUS_SIZE) mpi status variable","tags":"","loc":"proc/write_log.html","title":"write_log – ParaGEMS"},{"text":"public subroutine test_vars(cnt) Arguments Type Intent Optional Attributes Name integer, intent(inout) :: cnt Contents None","tags":"","loc":"proc/test_vars~2.html","title":"test_vars – ParaGEMS"},{"text":"public subroutine clean_PETSc_output() Clean up PETSc output format Arguments None Calls proc~~clean_petsc_output~~CallsGraph proc~clean_petsc_output clean_PETSc_output petscviewerpopformat petscviewerpopformat proc~clean_petsc_output:->petscviewerpopformat: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents None","tags":"","loc":"proc/clean_petsc_output.html","title":"clean_PETSc_output – ParaGEMS"},{"text":"public subroutine init_write_MATLAB() Setup PETSc for MATLAB output Arguments None Calls proc~~init_write_matlab~~CallsGraph proc~init_write_matlab init_write_MATLAB petscviewerpushformat petscviewerpushformat proc~init_write_matlab:->petscviewerpushformat: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents None","tags":"","loc":"proc/init_write_matlab.html","title":"init_write_MATLAB – ParaGEMS"},{"text":"public subroutine read_bndry_cond() Read boundary conditions from file Arguments None Calls proc~~read_bndry_cond~~CallsGraph proc~read_bndry_cond read_bndry_cond proc~end_mpi end_mpi proc~read_bndry_cond:->proc~end_mpi: lcl_complex lcl_complex proc~read_bndry_cond:->lcl_complex: proc~root_open_file_read root_open_file_read proc~read_bndry_cond:->proc~root_open_file_read: indx_offset indx_offset proc~read_bndry_cond:->indx_offset: mpi_request_free mpi_request_free proc~read_bndry_cond:->mpi_request_free: int_insertion_sort int_insertion_sort proc~read_bndry_cond:->int_insertion_sort: mpi_barrier mpi_barrier proc~read_bndry_cond:->mpi_barrier: proc~elm2proc elm2proc proc~read_bndry_cond:->proc~elm2proc: num_elm num_elm proc~read_bndry_cond:->num_elm: glb_num_elm glb_num_elm proc~read_bndry_cond:->glb_num_elm: proc~syncwrite_log syncwrite_log proc~end_mpi:->proc~syncwrite_log: proc~syncwrite_log_time syncwrite_log_time proc~end_mpi:->proc~syncwrite_log_time: mpi_finalize mpi_finalize proc~end_mpi:->mpi_finalize: proc~close_log close_log proc~end_mpi:->proc~close_log: mpi_bcast mpi_bcast proc~root_open_file_read:->mpi_bcast: num_pelm_pp num_pelm_pp proc~elm2proc:->num_pelm_pp: proc~syncwrite_log:->mpi_barrier: proc~syncwrite_log_time:->mpi_barrier: mpi_wtime mpi_wtime proc~syncwrite_log_time:->mpi_wtime: proc~rootwrite_log rootwrite_log proc~syncwrite_log_time:->proc~rootwrite_log: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables fname buffer_size i ie in int_buffer j junk node_indices prml_elm proc proc_old ptr req status Variables Type Visibility Attributes Name Initial character(len=slen), public :: fname file name integer, public :: buffer_size size of MPI comm buffer integer, public :: i loop indices (procs,elems) integer, public :: ie loop indices (procs,elems) integer, public :: in loop indices (procs,elems) integer, public, ALLOCATABLE :: int_buffer (:,:) integer buffer for MPI comms integer, public :: j loop indices (procs,elems) integer, public :: junk junk IO variable integer, public, ALLOCATABLE :: node_indices (:) junk IO variable for node indicies integer, public, ALLOCATABLE :: prml_elm (:,:) primal element list integer, public :: proc processor rank (current, past) integer, public :: proc_old processor rank (current, past) integer, public :: ptr pointer integer, public, ALLOCATABLE :: req (:) request variable for non-blocking comms integer, public, ALLOCATABLE :: status (:) size of MPI comm buffer","tags":"","loc":"proc/read_bndry_cond.html","title":"read_bndry_cond – ParaGEMS"},{"text":"public subroutine read_bndry_cond2() Read boundary conditions from file Arguments None Calls proc~~read_bndry_cond2~~CallsGraph proc~read_bndry_cond2 read_bndry_cond2 proc~root_open_file_read root_open_file_read proc~read_bndry_cond2:->proc~root_open_file_read: mpi_bcast mpi_bcast proc~read_bndry_cond2:->mpi_bcast: lcl_complex lcl_complex proc~read_bndry_cond2:->lcl_complex: mpi_barrier mpi_barrier proc~read_bndry_cond2:->mpi_barrier: num_elm num_elm proc~read_bndry_cond2:->num_elm: glb_num_elm glb_num_elm proc~read_bndry_cond2:->glb_num_elm: proc~root_open_file_read:->mpi_bcast: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~read_bndry_cond2~~CalledByGraph proc~read_bndry_cond2 read_bndry_cond2 proc~get_rhs_darcy get_RHS_darcy proc~get_rhs_darcy:->proc~read_bndry_cond2: proc~get_rhs_darcy2 get_RHS_darcy2 proc~get_rhs_darcy2:->proc~read_bndry_cond2: program~darcy_1f darcy_1f program~darcy_1f:->proc~get_rhs_darcy: program~darcy_2f darcy_2f program~darcy_2f:->proc~get_rhs_darcy2: program~darcy darcy program~darcy:->proc~get_rhs_darcy: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->proc~get_rhs_darcy: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~get_rhs_darcy: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~get_rhs_darcy2: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables fname buffer_size fib i ie iib in int_buffer j junk k l max_read_size node_indices num_read_iters orient prml_elm proc proc_old ptr read_size req resid_read_size status stride strt_indx Variables Type Visibility Attributes Name Initial character(len=slen), public :: fname file name integer, public :: buffer_size size of MPI comm buffer integer, public :: fib integer, public :: i loop indices (procs,elems) integer, public :: ie loop indices (procs,elems) integer, public :: iib integer, public :: in loop indices (procs,elems) integer, public, ALLOCATABLE :: int_buffer (:) integer buffer for MPI comms integer, public :: j loop indices (procs,elems) integer, public :: junk junk IO variable integer, public :: k integer, public :: l integer, public :: max_read_size integer, public, ALLOCATABLE :: node_indices (:) junk IO variable for node indicies integer, public :: num_read_iters integer, public :: orient integer, public, ALLOCATABLE :: prml_elm (:,:) primal element list integer, public :: proc processor rank (current, past) integer, public :: proc_old processor rank (current, past) integer, public :: ptr pointer integer, public :: read_size integer, public, ALLOCATABLE :: req (:) request variable for non-blocking comms integer, public :: resid_read_size integer, public, ALLOCATABLE :: status (:) size of MPI comm buffer integer, public :: stride integer, public :: strt_indx","tags":"","loc":"proc/read_bndry_cond2.html","title":"read_bndry_cond2 – ParaGEMS"},{"text":"public subroutine read_crack_faces() read crack faces from file Arguments None Calls proc~~read_crack_faces~~CallsGraph proc~read_crack_faces read_crack_faces lcl_complex lcl_complex proc~read_crack_faces:->lcl_complex: proc~root_open_file_read root_open_file_read proc~read_crack_faces:->proc~root_open_file_read: indx_offset indx_offset proc~read_crack_faces:->indx_offset: mpi_bcast mpi_bcast proc~read_crack_faces:->mpi_bcast: mpi_request_free mpi_request_free proc~read_crack_faces:->mpi_request_free: mpi_barrier mpi_barrier proc~read_crack_faces:->mpi_barrier: mpi_recv mpi_recv proc~read_crack_faces:->mpi_recv: proc~elm2proc elm2proc proc~read_crack_faces:->proc~elm2proc: num_elm num_elm proc~read_crack_faces:->num_elm: proc~root_open_file_read:->mpi_bcast: num_pelm_pp num_pelm_pp proc~elm2proc:->num_pelm_pp: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables fname buffer_size glb_num_cracks i ie in int_buffer j junk prml_elm proc proc_old ptr req status Variables Type Visibility Attributes Name Initial character(len=slen), public :: fname file name integer, public :: buffer_size size of MPI comm buffer integer, public :: glb_num_cracks global number of cracks integer, public :: i loop indices (procs,elems) integer, public :: ie loop indices (procs,elems) integer, public :: in loop indices (procs,elems) integer, public, ALLOCATABLE :: int_buffer (:,:) integer buffer for MPI comms integer, public :: j loop indices (procs,elems) integer, public :: junk junk IO variable integer, public, ALLOCATABLE :: prml_elm (:,:) primal element list integer, public :: proc processor rank (current, past) integer, public :: proc_old processor rank (current, past) integer, public :: ptr pointer integer, public, ALLOCATABLE :: req (:) request variable for non-blocking comms integer, public, ALLOCATABLE :: status (:) size of MPI comm buffer","tags":"","loc":"proc/read_crack_faces.html","title":"read_crack_faces – ParaGEMS"},{"text":"public subroutine read_glb_indx(k) Read glbal indices of elements of given geometric order Arguments Type Intent Optional Attributes Name integer, intent(in) :: k simplicial order Calls proc~~read_glb_indx~~CallsGraph proc~read_glb_indx read_glb_indx proc~end_mpi end_mpi proc~read_glb_indx:->proc~end_mpi: mpi_request_free mpi_request_free proc~read_glb_indx:->mpi_request_free: proc~root_open_file_read root_open_file_read proc~read_glb_indx:->proc~root_open_file_read: indx_offset indx_offset proc~read_glb_indx:->indx_offset: lcl_complex lcl_complex proc~read_glb_indx:->lcl_complex: mpi_barrier mpi_barrier proc~read_glb_indx:->mpi_barrier: proc~elm2proc elm2proc proc~read_glb_indx:->proc~elm2proc: num_elm num_elm proc~read_glb_indx:->num_elm: glb_num_elm glb_num_elm proc~read_glb_indx:->glb_num_elm: proc~syncwrite_log syncwrite_log proc~end_mpi:->proc~syncwrite_log: proc~syncwrite_log_time syncwrite_log_time proc~end_mpi:->proc~syncwrite_log_time: mpi_finalize mpi_finalize proc~end_mpi:->mpi_finalize: proc~close_log close_log proc~end_mpi:->proc~close_log: mpi_bcast mpi_bcast proc~root_open_file_read:->mpi_bcast: num_pelm_pp num_pelm_pp proc~elm2proc:->num_pelm_pp: proc~syncwrite_log:->mpi_barrier: proc~syncwrite_log_time:->mpi_barrier: mpi_wtime mpi_wtime proc~syncwrite_log_time:->mpi_wtime: proc~rootwrite_log rootwrite_log proc~syncwrite_log_time:->proc~rootwrite_log: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables fname buffer_size i ie in int_buffer j junk prml_elm proc proc_old ptr req status Variables Type Visibility Attributes Name Initial character(len=slen), public :: fname file name integer, public :: buffer_size size of MPI comm buffer integer, public :: i loop indices (procs,elems) integer, public :: ie loop indices (procs,elems) integer, public :: in loop indices (procs,elems) integer, public, ALLOCATABLE :: int_buffer (:,:) integer buffer for MPI comms integer, public :: j loop indices (procs,elems) integer, public :: junk junk IO variable integer, public, ALLOCATABLE :: prml_elm (:,:) primal element list integer, public :: proc processor rank (current, past) integer, public :: proc_old processor rank (current, past) integer, public :: ptr pointer integer, public, ALLOCATABLE :: req (:) request variable for non-blocking comms integer, public, ALLOCATABLE :: status (:) size of MPI comm buffer","tags":"","loc":"proc/read_glb_indx.html","title":"read_glb_indx – ParaGEMS"},{"text":"public subroutine read_glb_indx2(k) Read local node indices of highest order primal elements Arguments Type Intent Optional Attributes Name integer, intent(in) :: k simplicial order Calls proc~~read_glb_indx2~~CallsGraph proc~read_glb_indx2 read_glb_indx2 proc~end_mpi end_mpi proc~read_glb_indx2:->proc~end_mpi: proc~root_open_file_read root_open_file_read proc~read_glb_indx2:->proc~root_open_file_read: indx_offset indx_offset proc~read_glb_indx2:->indx_offset: mpi_bcast mpi_bcast proc~read_glb_indx2:->mpi_bcast: lcl_complex lcl_complex proc~read_glb_indx2:->lcl_complex: mpi_barrier mpi_barrier proc~read_glb_indx2:->mpi_barrier: proc~elm2proc elm2proc proc~read_glb_indx2:->proc~elm2proc: num_elm num_elm proc~read_glb_indx2:->num_elm: glb_num_elm glb_num_elm proc~read_glb_indx2:->glb_num_elm: proc~syncwrite_log syncwrite_log proc~end_mpi:->proc~syncwrite_log: mpi_finalize mpi_finalize proc~end_mpi:->mpi_finalize: proc~syncwrite_log_time syncwrite_log_time proc~end_mpi:->proc~syncwrite_log_time: proc~close_log close_log proc~end_mpi:->proc~close_log: proc~root_open_file_read:->mpi_bcast: num_pelm_pp num_pelm_pp proc~elm2proc:->num_pelm_pp: proc~syncwrite_log:->mpi_barrier: proc~syncwrite_log_time:->mpi_barrier: mpi_wtime mpi_wtime proc~syncwrite_log_time:->mpi_wtime: proc~rootwrite_log rootwrite_log proc~syncwrite_log_time:->proc~rootwrite_log: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~read_glb_indx2~~CalledByGraph proc~read_glb_indx2 read_glb_indx2 proc~get_glb_indx get_glb_indx proc~get_glb_indx:->proc~read_glb_indx2: proc~parallel_setup parallel_setup proc~parallel_setup:->proc~get_glb_indx: program~darcy_1f darcy_1f program~darcy_1f:->proc~parallel_setup: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~parallel_setup: program~darcy darcy program~darcy:->proc~parallel_setup: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->proc~parallel_setup: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~parallel_setup: program~darcy_2f darcy_2f program~darcy_2f:->proc~parallel_setup: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables fname buffer_size fib i ie iib in int_buffer j junk l max_read_size n num_read_iters orient prml_elm proc proc_old ptr read_size req resid_read_size status stride strt_indx Variables Type Visibility Attributes Name Initial character(len=slen), public :: fname file name integer, public :: buffer_size size of MPI comm buffer integer, public :: fib integer, public :: i loop indices (procs,elems) integer, public :: ie loop indices (procs,elems) integer, public :: iib integer, public :: in loop indices (procs,elems) integer, public, ALLOCATABLE :: int_buffer (:) integer buffer for MPI comms integer, public :: j loop indices (procs,elems) integer, public :: junk junk IO variable integer, public :: l integer, public :: max_read_size integer, public :: n integer, public :: num_read_iters integer, public :: orient integer, public, ALLOCATABLE :: prml_elm (:,:) primal element list integer, public :: proc processor rank (current, past) integer, public :: proc_old processor rank (current, past) integer, public :: ptr pointer integer, public :: read_size integer, public, ALLOCATABLE :: req (:) request variable for non-blocking comms integer, public :: resid_read_size integer, public, ALLOCATABLE :: status (:) size of MPI comm buffer integer, public :: stride integer, public :: strt_indx","tags":"","loc":"proc/read_glb_indx2.html","title":"read_glb_indx2 – ParaGEMS"},{"text":"public subroutine read_nodes_prml() Root process reads the location of primal nodes Arguments None Calls proc~~read_nodes_prml~~CallsGraph proc~read_nodes_prml read_nodes_prml mpi_recv mpi_recv proc~read_nodes_prml:->mpi_recv: proc~root_open_file_read root_open_file_read proc~read_nodes_prml:->proc~root_open_file_read: indx_offset indx_offset proc~read_nodes_prml:->indx_offset: mpi_bcast mpi_bcast proc~read_nodes_prml:->mpi_bcast: idnint idnint proc~read_nodes_prml:->idnint: lcl_complex lcl_complex proc~read_nodes_prml:->lcl_complex: num_pelm_pp num_pelm_pp proc~read_nodes_prml:->num_pelm_pp: num_elm num_elm proc~read_nodes_prml:->num_elm: mpi_send mpi_send proc~read_nodes_prml:->mpi_send: glb_num_elm glb_num_elm proc~read_nodes_prml:->glb_num_elm: proc~root_open_file_read:->mpi_bcast: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~read_nodes_prml~~CalledByGraph proc~read_nodes_prml read_nodes_prml proc~parallel_setup parallel_setup proc~parallel_setup:->proc~read_nodes_prml: program~darcy_1f darcy_1f program~darcy_1f:->proc~parallel_setup: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~parallel_setup: program~darcy darcy program~darcy:->proc~parallel_setup: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->proc~parallel_setup: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~parallel_setup: program~darcy_2f darcy_2f program~darcy_2f:->proc~parallel_setup: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables fname buffer_size fib ie iib ip junk k ptr ptrp status err real_buffer Variables Type Visibility Attributes Name Initial character(len=slen), public :: fname file name integer, public :: buffer_size size of MPI comm buffer integer, public :: fib initial/final index for MPI comm buffer integer, public :: ie loop indices (procs,elems) integer, public :: iib initial/final index for MPI comm buffer integer, public :: ip loop indices (procs,elems) integer, public :: junk junk IO variable integer, public :: k simplicial order integer, public :: ptr integer, public :: ptrp integer, public, ALLOCATABLE :: status (:) size of MPI comm buffer logical, public :: err = .FALSE. error variable real(kind=iwp), public, ALLOCATABLE :: real_buffer (:) buffer for MPI comms","tags":"","loc":"proc/read_nodes_prml.html","title":"read_nodes_prml – ParaGEMS"},{"text":"public subroutine read_prml_elms() Read local node indices of highest order primal elements Arguments None Calls proc~~read_prml_elms~~CallsGraph proc~read_prml_elms read_prml_elms lcl_complex lcl_complex proc~read_prml_elms:->lcl_complex: proc~root_open_file_read root_open_file_read proc~read_prml_elms:->proc~root_open_file_read: indx_offset indx_offset proc~read_prml_elms:->indx_offset: mpi_bcast mpi_bcast proc~read_prml_elms:->mpi_bcast: mpi_request_free mpi_request_free proc~read_prml_elms:->mpi_request_free: mpi_barrier mpi_barrier proc~read_prml_elms:->mpi_barrier: proc~elm2proc elm2proc proc~read_prml_elms:->proc~elm2proc: num_elm num_elm proc~read_prml_elms:->num_elm: glb_num_elm glb_num_elm proc~read_prml_elms:->glb_num_elm: proc~root_open_file_read:->mpi_bcast: num_pelm_pp num_pelm_pp proc~elm2proc:->num_pelm_pp: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables fname buffer_size dual_elm_in ie in int_buffer junk k num_dual_elm prml_elm proc proc_old ptr req status err Variables Type Visibility Attributes Name Initial character(len=slen), public :: fname file name integer, public :: buffer_size size of MPI comm buffer integer, public, ALLOCATABLE :: dual_elm_in (:) node indices of dual element integer, public :: ie loop indices (procs,elems) integer, public :: in loop indices (procs,elems) integer, public, ALLOCATABLE :: int_buffer (:,:) integer buffer for MPI comms integer, public :: junk junk IO variable integer, public :: k simplicial order integer, public :: num_dual_elm number of local dual element integer, public, ALLOCATABLE :: prml_elm (:,:) primal element list integer, public :: proc processor rank (current, past) integer, public :: proc_old processor rank (current, past) integer, public :: ptr pointer integer, public, ALLOCATABLE :: req (:) request variable for non-blocking comms integer, public, ALLOCATABLE :: status (:) size of MPI comm buffer logical, public :: err = .FALSE. error variable","tags":"","loc":"proc/read_prml_elms.html","title":"read_prml_elms – ParaGEMS"},{"text":"public subroutine read_prml_elms2() Read local node indices of highest order primal elements Arguments None Calls proc~~read_prml_elms2~~CallsGraph proc~read_prml_elms2 read_prml_elms2 proc~root_open_file_read root_open_file_read proc~read_prml_elms2:->proc~root_open_file_read: indx_offset indx_offset proc~read_prml_elms2:->indx_offset: mpi_bcast mpi_bcast proc~read_prml_elms2:->mpi_bcast: lcl_complex lcl_complex proc~read_prml_elms2:->lcl_complex: num_pelm_pp num_pelm_pp proc~read_prml_elms2:->num_pelm_pp: mpi_barrier mpi_barrier proc~read_prml_elms2:->mpi_barrier: proc~elm2proc elm2proc proc~read_prml_elms2:->proc~elm2proc: num_elm num_elm proc~read_prml_elms2:->num_elm: glb_num_elm glb_num_elm proc~read_prml_elms2:->glb_num_elm: proc~root_open_file_read:->mpi_bcast: proc~elm2proc:->num_pelm_pp: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~read_prml_elms2~~CalledByGraph proc~read_prml_elms2 read_prml_elms2 proc~parallel_setup parallel_setup proc~parallel_setup:->proc~read_prml_elms2: program~darcy_1f darcy_1f program~darcy_1f:->proc~parallel_setup: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~parallel_setup: program~darcy darcy program~darcy:->proc~parallel_setup: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->proc~parallel_setup: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~parallel_setup: program~darcy_2f darcy_2f program~darcy_2f:->proc~parallel_setup: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables fname buffer_size dual_elm_in fib i ie iib in int_buffer j junk k l max_read_size num_dual_elm num_read_iters orient proc proc_old ptr read_size req resid_read_size status stride strt_indx err Variables Type Visibility Attributes Name Initial character(len=slen), public :: fname file name integer, public :: buffer_size size of MPI comm buffer integer, public, ALLOCATABLE :: dual_elm_in (:) node indices of dual element integer, public :: fib integer, public :: i integer, public :: ie loop indices (procs,elems) integer, public :: iib integer, public :: in loop indices (procs,elems) integer, public, ALLOCATABLE :: int_buffer (:) integer buffer for MPI comms integer, public :: j integer, public :: junk junk IO variable integer, public :: k simplicial order integer, public :: l integer, public :: max_read_size integer, public :: num_dual_elm number of local dual element integer, public :: num_read_iters integer, public :: orient integer, public :: proc processor rank (current, past) integer, public :: proc_old processor rank (current, past) integer, public :: ptr pointer integer, public :: read_size integer, public, ALLOCATABLE :: req (:) request variable for non-blocking comms integer, public :: resid_read_size integer, public, ALLOCATABLE :: status (:) size of MPI comm buffer integer, public :: stride integer, public :: strt_indx logical, public :: err = .FALSE. error variable","tags":"","loc":"proc/read_prml_elms2.html","title":"read_prml_elms2 – ParaGEMS"},{"text":"public subroutine root_open_file_read(fname, unit) Open given file on root process for reading Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: fname file name integer, intent(in) :: unit file unit Calls proc~~root_open_file_read~~CallsGraph proc~root_open_file_read root_open_file_read mpi_bcast mpi_bcast proc~root_open_file_read:->mpi_bcast: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~root_open_file_read~~CalledByGraph proc~root_open_file_read root_open_file_read proc~read_bndry_cond read_bndry_cond proc~read_bndry_cond:->proc~root_open_file_read: proc~read_prml_elms2 read_prml_elms2 proc~read_prml_elms2:->proc~root_open_file_read: proc~read_crack_faces read_crack_faces proc~read_crack_faces:->proc~root_open_file_read: proc~read_nodes_prml read_nodes_prml proc~read_nodes_prml:->proc~root_open_file_read: proc~write_unsteady_d0s write_unsteady_D0S proc~write_unsteady_d0s:->proc~root_open_file_read: proc~write_solution_d0s write_solution_D0S proc~write_solution_d0s:->proc~root_open_file_read: proc~read_prml_elms read_prml_elms proc~read_prml_elms:->proc~root_open_file_read: proc~read_glb_indx2 read_glb_indx2 proc~read_glb_indx2:->proc~root_open_file_read: proc~read_glb_indx read_glb_indx proc~read_glb_indx:->proc~root_open_file_read: proc~read_bndry_cond2 read_bndry_cond2 proc~read_bndry_cond2:->proc~root_open_file_read: proc~get_rhs_darcy get_RHS_darcy proc~get_rhs_darcy:->proc~read_bndry_cond2: proc~parallel_setup parallel_setup proc~parallel_setup:->proc~read_prml_elms2: proc~parallel_setup:->proc~read_nodes_prml: proc~get_glb_indx get_glb_indx proc~parallel_setup:->proc~get_glb_indx: proc~get_glb_indx:->proc~read_glb_indx2: proc~get_rhs_darcy2 get_RHS_darcy2 proc~get_rhs_darcy2:->proc~read_bndry_cond2: program~darcy_1f darcy_1f program~darcy_1f:->proc~get_rhs_darcy: program~darcy_1f:->proc~parallel_setup: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~parallel_setup: program~darcy_crkp_2f:->proc~get_rhs_darcy2: program~darcy darcy program~darcy:->proc~get_rhs_darcy: program~darcy:->proc~parallel_setup: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->proc~get_rhs_darcy: program~darcy_crkp_1f:->proc~parallel_setup: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~get_rhs_darcy: program~darcy_crkp_2f~2:->proc~parallel_setup: program~darcy_2f darcy_2f program~darcy_2f:->proc~parallel_setup: program~darcy_2f:->proc~get_rhs_darcy2: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables err Variables Type Visibility Attributes Name Initial logical, public :: err = .FALSE. error variable","tags":"","loc":"proc/root_open_file_read.html","title":"root_open_file_read – ParaGEMS"},{"text":"public subroutine root_open_file_write(fname, unit) Open given file on root process for writing Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: fname file name integer, intent(in) :: unit file unit Calls proc~~root_open_file_write~~CallsGraph proc~root_open_file_write root_open_file_write mpi_bcast mpi_bcast proc~root_open_file_write:->mpi_bcast: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~root_open_file_write~~CalledByGraph proc~root_open_file_write root_open_file_write proc~write_solution_d0s2 write_solution_D0S2 proc~write_solution_d0s2:->proc~root_open_file_write: proc~write_unsteady_d0s2 write_unsteady_D0S2 proc~write_unsteady_d0s2:->proc~root_open_file_write: proc~write_unsteady_d0s write_unsteady_D0S proc~write_unsteady_d0s:->proc~root_open_file_write: proc~write_solution_d0s write_solution_D0S proc~write_solution_d0s:->proc~root_open_file_write: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables err Variables Type Visibility Attributes Name Initial logical, public :: err = .FALSE. error variable","tags":"","loc":"proc/root_open_file_write.html","title":"root_open_file_write – ParaGEMS"},{"text":"public subroutine write_centers_MATLAB() write barycenters to file in MATLAB format Arguments None Calls proc~~write_centers_matlab~~CallsGraph proc~write_centers_matlab write_centers_MATLAB petscviewerpushformat petscviewerpushformat proc~write_centers_matlab:->petscviewerpushformat: vecassemblyend vecassemblyend proc~write_centers_matlab:->vecassemblyend: petscviewerdestroy petscviewerdestroy proc~write_centers_matlab:->petscviewerdestroy: vecdestroy vecdestroy proc~write_centers_matlab:->vecdestroy: vecduplicate vecduplicate proc~write_centers_matlab:->vecduplicate: petscviewerasciiopen petscviewerasciiopen proc~write_centers_matlab:->petscviewerasciiopen: vecview vecview proc~write_centers_matlab:->vecview: num_elm num_elm proc~write_centers_matlab:->num_elm: lcl_complex lcl_complex proc~write_centers_matlab:->lcl_complex: isg isg proc~write_centers_matlab:->isg: vecassemblybegin vecassemblybegin proc~write_centers_matlab:->vecassemblybegin: petscobjectsetname petscobjectsetname proc~write_centers_matlab:->petscobjectsetname: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables fname i id work wrk1 wrk2 Variables Type Visibility Attributes Name Initial character(len=slen), public :: fname file name integer, public :: i loop and temporary indicies integer, public :: id loop and temporary indicies vec, public :: work PETSc work array vec, public :: wrk1 sub arrays vec, public :: wrk2 sub arrays","tags":"","loc":"proc/write_centers_matlab.html","title":"write_centers_MATLAB – ParaGEMS"},{"text":"public subroutine write_centers_MATLAB2() write barycenters to file in MATLAB format Arguments None Calls proc~~write_centers_matlab2~~CallsGraph proc~write_centers_matlab2 write_centers_MATLAB2 petscviewerpushformat petscviewerpushformat proc~write_centers_matlab2:->petscviewerpushformat: veccreatempi veccreatempi proc~write_centers_matlab2:->veccreatempi: vecassemblyend vecassemblyend proc~write_centers_matlab2:->vecassemblyend: glb_num_elm glb_num_elm proc~write_centers_matlab2:->glb_num_elm: petscviewerdestroy petscviewerdestroy proc~write_centers_matlab2:->petscviewerdestroy: vecdestroy vecdestroy proc~write_centers_matlab2:->vecdestroy: petscviewerasciiopen petscviewerasciiopen proc~write_centers_matlab2:->petscviewerasciiopen: petscobjectsetname petscobjectsetname proc~write_centers_matlab2:->petscobjectsetname: num_elm num_elm proc~write_centers_matlab2:->num_elm: lcl_complex lcl_complex proc~write_centers_matlab2:->lcl_complex: vecassemblybegin vecassemblybegin proc~write_centers_matlab2:->vecassemblybegin: vecview vecview proc~write_centers_matlab2:->vecview: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables fname i id m work Variables Type Visibility Attributes Name Initial character(len=slen), public :: fname file name integer, public :: i loop and temporary indicies integer, public :: id loop and temporary indicies integer, public :: m number of rows in global solution variables vec, public :: work PETSc work array","tags":"","loc":"proc/write_centers_matlab2.html","title":"write_centers_MATLAB2 – ParaGEMS"},{"text":"public subroutine write_pressure_MATLAB(iter) write scaled pressure solution to file in MATLAB format Arguments Type Intent Optional Attributes Name integer, intent(in), optional :: iter iteration for file name Calls proc~~write_pressure_matlab~~CallsGraph proc~write_pressure_matlab write_pressure_MATLAB petscviewerpushformat petscviewerpushformat proc~write_pressure_matlab:->petscviewerpushformat: vecassemblyend vecassemblyend proc~write_pressure_matlab:->vecassemblyend: lcl_complex lcl_complex proc~write_pressure_matlab:->lcl_complex: vecduplicate vecduplicate proc~write_pressure_matlab:->vecduplicate: vecdestroy vecdestroy proc~write_pressure_matlab:->vecdestroy: petscviewerdestroy petscviewerdestroy proc~write_pressure_matlab:->petscviewerdestroy: petscviewerasciiopen petscviewerasciiopen proc~write_pressure_matlab:->petscviewerasciiopen: num_elm num_elm proc~write_pressure_matlab:->num_elm: petscobjectsetname petscobjectsetname proc~write_pressure_matlab:->petscobjectsetname: isg isg proc~write_pressure_matlab:->isg: vecassemblybegin vecassemblybegin proc~write_pressure_matlab:->vecassemblybegin: vecview vecview proc~write_pressure_matlab:->vecview: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables fname i indx j prelen cnt dxyz nan sol_edge work wrk1 wrk2 Variables Type Visibility Attributes Name Initial character(len=slen), public :: fname file name integer, public :: i loop index integer, public :: indx face index integer, public :: j loop index integer, public :: prelen string pointer real(kind=iwp), public, ALLOCATABLE :: cnt (:) counter real(kind=iwp), public :: dxyz extrapolation variable real(kind=iwp), public :: nan nan variable real(kind=iwp), public, ALLOCATABLE :: sol_edge (:) solution variable for MATLAB output vec, public :: work PETSc work array vec, public :: wrk1 sub arrays vec, public :: wrk2 sub arrays","tags":"","loc":"proc/write_pressure_matlab.html","title":"write_pressure_MATLAB – ParaGEMS"},{"text":"public subroutine write_pressure_MATLAB2(iter) write scaled pressure solution to file in MATLAB format Arguments Type Intent Optional Attributes Name integer, intent(in), optional :: iter iteration for file name Calls proc~~write_pressure_matlab2~~CallsGraph proc~write_pressure_matlab2 write_pressure_MATLAB2 petscviewerpushformat petscviewerpushformat proc~write_pressure_matlab2:->petscviewerpushformat: vecassemblyend vecassemblyend proc~write_pressure_matlab2:->vecassemblyend: lcl_complex lcl_complex proc~write_pressure_matlab2:->lcl_complex: vecduplicate vecduplicate proc~write_pressure_matlab2:->vecduplicate: vecdestroy vecdestroy proc~write_pressure_matlab2:->vecdestroy: petscviewerdestroy petscviewerdestroy proc~write_pressure_matlab2:->petscviewerdestroy: petscviewerasciiopen petscviewerasciiopen proc~write_pressure_matlab2:->petscviewerasciiopen: glb_num_elm glb_num_elm proc~write_pressure_matlab2:->glb_num_elm: num_elm num_elm proc~write_pressure_matlab2:->num_elm: petscobjectsetname petscobjectsetname proc~write_pressure_matlab2:->petscobjectsetname: vecassemblybegin vecassemblybegin proc~write_pressure_matlab2:->vecassemblybegin: vecview vecview proc~write_pressure_matlab2:->vecview: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables fname i indx j prelen cnt dxyz nan sol_edge work Variables Type Visibility Attributes Name Initial character(len=slen), public :: fname file name integer, public :: i loop index integer, public :: indx face index integer, public :: j loop index integer, public :: prelen string pointer real(kind=iwp), public, ALLOCATABLE :: cnt (:) counter real(kind=iwp), public :: dxyz extrapolation variable real(kind=iwp), public :: nan nan variable real(kind=iwp), public, ALLOCATABLE :: sol_edge (:) solution variable for MATLAB output vec, public :: work PETSc work array","tags":"","loc":"proc/write_pressure_matlab2.html","title":"write_pressure_MATLAB2 – ParaGEMS"},{"text":"public subroutine write_prml_volumes_MATLAB() write primal volumes to file in MATLAB format Arguments None Calls proc~~write_prml_volumes_matlab~~CallsGraph proc~write_prml_volumes_matlab write_prml_volumes_MATLAB petscviewerpushformat petscviewerpushformat proc~write_prml_volumes_matlab:->petscviewerpushformat: vecassemblyend vecassemblyend proc~write_prml_volumes_matlab:->vecassemblyend: petscviewerdestroy petscviewerdestroy proc~write_prml_volumes_matlab:->petscviewerdestroy: vecdestroy vecdestroy proc~write_prml_volumes_matlab:->vecdestroy: vecduplicate vecduplicate proc~write_prml_volumes_matlab:->vecduplicate: petscviewerasciiopen petscviewerasciiopen proc~write_prml_volumes_matlab:->petscviewerasciiopen: vecview vecview proc~write_prml_volumes_matlab:->vecview: num_elm num_elm proc~write_prml_volumes_matlab:->num_elm: lcl_complex lcl_complex proc~write_prml_volumes_matlab:->lcl_complex: isg isg proc~write_prml_volumes_matlab:->isg: vecassemblybegin vecassemblybegin proc~write_prml_volumes_matlab:->vecassemblybegin: petscobjectsetname petscobjectsetname proc~write_prml_volumes_matlab:->petscobjectsetname: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables fname i id work wrk1 wrk2 Variables Type Visibility Attributes Name Initial character(len=slen), public :: fname file name integer, public :: i loop and temporary indicies integer, public :: id loop and temporary indicies vec, public :: work PETSc work array vec, public :: wrk1 sub arrays vec, public :: wrk2 sub arrays","tags":"","loc":"proc/write_prml_volumes_matlab.html","title":"write_prml_volumes_MATLAB – ParaGEMS"},{"text":"public subroutine write_prml_volumes_MATLAB2() write primary volumes to file in MATLAB format Arguments None Calls proc~~write_prml_volumes_matlab2~~CallsGraph proc~write_prml_volumes_matlab2 write_prml_volumes_MATLAB2 petscviewerpushformat petscviewerpushformat proc~write_prml_volumes_matlab2:->petscviewerpushformat: veccreatempi veccreatempi proc~write_prml_volumes_matlab2:->veccreatempi: vecassemblyend vecassemblyend proc~write_prml_volumes_matlab2:->vecassemblyend: glb_num_elm glb_num_elm proc~write_prml_volumes_matlab2:->glb_num_elm: petscviewerdestroy petscviewerdestroy proc~write_prml_volumes_matlab2:->petscviewerdestroy: vecdestroy vecdestroy proc~write_prml_volumes_matlab2:->vecdestroy: petscviewerasciiopen petscviewerasciiopen proc~write_prml_volumes_matlab2:->petscviewerasciiopen: petscobjectsetname petscobjectsetname proc~write_prml_volumes_matlab2:->petscobjectsetname: num_elm num_elm proc~write_prml_volumes_matlab2:->num_elm: lcl_complex lcl_complex proc~write_prml_volumes_matlab2:->lcl_complex: vecassemblybegin vecassemblybegin proc~write_prml_volumes_matlab2:->vecassemblybegin: vecview vecview proc~write_prml_volumes_matlab2:->vecview: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables fname i id m work Variables Type Visibility Attributes Name Initial character(len=slen), public :: fname file name integer, public :: i loop and temporary indicies integer, public :: id loop and temporary indicies integer, public :: m number of rows in global solution variables vec, public :: work PETSc work array","tags":"","loc":"proc/write_prml_volumes_matlab2.html","title":"write_prml_volumes_MATLAB2 – ParaGEMS"},{"text":"public subroutine write_solution_D0S(sclr_name, vctr_name) write vtk solution at dual 0-cells Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: sclr_name scalar variable name (label) character(len=*), intent(in) :: vctr_name vector variable name (label) Calls proc~~write_solution_d0s~~CallsGraph proc~write_solution_d0s write_solution_D0S lcl_complex lcl_complex proc~write_solution_d0s:->lcl_complex: proc~root_open_file_read root_open_file_read proc~write_solution_d0s:->proc~root_open_file_read: indx_offset indx_offset proc~write_solution_d0s:->indx_offset: num_elm num_elm proc~write_solution_d0s:->num_elm: mpi_recv mpi_recv proc~write_solution_d0s:->mpi_recv: num_pelm_pp num_pelm_pp proc~write_solution_d0s:->num_pelm_pp: mpi_barrier mpi_barrier proc~write_solution_d0s:->mpi_barrier: mpi_request_free mpi_request_free proc~write_solution_d0s:->mpi_request_free: mpi_waitall mpi_waitall proc~write_solution_d0s:->mpi_waitall: proc~root_open_file_write root_open_file_write proc~write_solution_d0s:->proc~root_open_file_write: mpi_isend mpi_isend proc~write_solution_d0s:->mpi_isend: glb_num_elm glb_num_elm proc~write_solution_d0s:->glb_num_elm: mpi_bcast mpi_bcast proc~root_open_file_read:->mpi_bcast: proc~root_open_file_write:->mpi_bcast: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables fname buffer_size extra_comms fib i iib int_buffer istatus j junk junk_int_buffer kmax ptr req status junk_real_buffer real_buffer Variables Type Visibility Attributes Name Initial character(len=slen), public :: fname file name integer, public :: buffer_size integer, public :: extra_comms integer, public :: fib integer, public :: i integer, public :: iib integer, public, ALLOCATABLE :: int_buffer (:) integer, public, ALLOCATABLE :: istatus (:,:) size of MPI comm buffer integer, public :: j integer, public :: junk integer, public, ALLOCATABLE :: junk_int_buffer (:) integer, public :: kmax integer, public :: ptr integer, public, ALLOCATABLE :: req (:) request variable for non-blocking comms integer, public, ALLOCATABLE :: status (:) size of MPI comm buffer real(kind=iwp), public, ALLOCATABLE :: junk_real_buffer (:) real(kind=iwp), public, ALLOCATABLE :: real_buffer (:)","tags":"","loc":"proc/write_solution_d0s.html","title":"write_solution_D0S – ParaGEMS"},{"text":"public subroutine write_solution_D0S2(sclr_name, vctr_name) write vtk solution at dual 0-cells Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: sclr_name scalar variable name (label) character(len=*), intent(in) :: vctr_name vector variable name (label) Calls proc~~write_solution_d0s2~~CallsGraph proc~write_solution_d0s2 write_solution_D0S2 proc~syncwrite_log syncwrite_log proc~write_solution_d0s2:->proc~syncwrite_log: lcl_complex lcl_complex proc~write_solution_d0s2:->lcl_complex: proc~syncwrite_log_time syncwrite_log_time proc~write_solution_d0s2:->proc~syncwrite_log_time: mpi_recv mpi_recv proc~write_solution_d0s2:->mpi_recv: num_pelm_pp num_pelm_pp proc~write_solution_d0s2:->num_pelm_pp: mpi_barrier mpi_barrier proc~write_solution_d0s2:->mpi_barrier: num_elm num_elm proc~write_solution_d0s2:->num_elm: proc~root_open_file_write root_open_file_write proc~write_solution_d0s2:->proc~root_open_file_write: mpi_send mpi_send proc~write_solution_d0s2:->mpi_send: mpi_reduce mpi_reduce proc~write_solution_d0s2:->mpi_reduce: glb_num_elm glb_num_elm proc~write_solution_d0s2:->glb_num_elm: proc~syncwrite_log:->mpi_barrier: proc~syncwrite_log_time:->mpi_barrier: mpi_wtime mpi_wtime proc~syncwrite_log_time:->mpi_wtime: proc~rootwrite_log rootwrite_log proc~syncwrite_log_time:->proc~rootwrite_log: mpi_bcast mpi_bcast proc~root_open_file_write:->mpi_bcast: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables fname buffer_size comm_size extra_comms fib gindx i iib int_buffer istatus j junk junk_int_buffer k kmax max_comm_size num_comm_iters ptr req resid_comm_size status strt_indx tmp_time tmp_time_int junk_real_buffer real_buffer Variables Type Visibility Attributes Name Initial character(len=slen), public :: fname file name integer, public :: buffer_size integer, public :: comm_size integer, public :: extra_comms integer, public :: fib integer, public :: gindx integer, public :: i integer, public :: iib integer, public, ALLOCATABLE :: int_buffer (:) integer, public, ALLOCATABLE :: istatus (:,:) size of MPI comm buffer integer, public :: j integer, public :: junk integer, public, ALLOCATABLE :: junk_int_buffer (:) integer, public :: k integer, public :: kmax integer, public :: max_comm_size integer, public :: num_comm_iters integer, public :: ptr integer, public, ALLOCATABLE :: req (:) request variable for non-blocking comms integer, public :: resid_comm_size integer, public, ALLOCATABLE :: status (:) size of MPI comm buffer integer, public :: strt_indx real(kind=IWP), public :: tmp_time temporary timing variable real(kind=IWP), public :: tmp_time_int temporary timing variable real(kind=iwp), public, ALLOCATABLE :: junk_real_buffer (:) real(kind=iwp), public, ALLOCATABLE :: real_buffer (:)","tags":"","loc":"proc/write_solution_d0s2.html","title":"write_solution_D0S2 – ParaGEMS"},{"text":"public subroutine write_solution_MATLAB(iter) write solution to file in MATLAB format Arguments Type Intent Optional Attributes Name integer, intent(in), optional :: iter iteration for file name Calls proc~~write_solution_matlab~~CallsGraph proc~write_solution_matlab write_solution_MATLAB petscviewerpushformat petscviewerpushformat proc~write_solution_matlab:->petscviewerpushformat: petscviewerasciiopen petscviewerasciiopen proc~write_solution_matlab:->petscviewerasciiopen: petscviewerdestroy petscviewerdestroy proc~write_solution_matlab:->petscviewerdestroy: vecview vecview proc~write_solution_matlab:->vecview: petscobjectsetname petscobjectsetname proc~write_solution_matlab:->petscobjectsetname: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables fname prelen Variables Type Visibility Attributes Name Initial character(len=slen), public :: fname file name integer, public :: prelen string pointer","tags":"","loc":"proc/write_solution_matlab.html","title":"write_solution_MATLAB – ParaGEMS"},{"text":"public subroutine write_solution_MATLAB2(iter) write solution to file in MATLAB format Arguments Type Intent Optional Attributes Name integer, intent(in), optional :: iter iteration for file name Calls proc~~write_solution_matlab2~~CallsGraph proc~write_solution_matlab2 write_solution_MATLAB2 petscviewerpushformat petscviewerpushformat proc~write_solution_matlab2:->petscviewerpushformat: petscviewerasciiopen petscviewerasciiopen proc~write_solution_matlab2:->petscviewerasciiopen: petscviewerdestroy petscviewerdestroy proc~write_solution_matlab2:->petscviewerdestroy: vecview vecview proc~write_solution_matlab2:->vecview: petscobjectsetname petscobjectsetname proc~write_solution_matlab2:->petscobjectsetname: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables fname prelen Variables Type Visibility Attributes Name Initial character(len=slen), public :: fname file name integer, public :: prelen string pointer","tags":"","loc":"proc/write_solution_matlab2.html","title":"write_solution_MATLAB2 – ParaGEMS"},{"text":"public subroutine write_unsteady_D0S(sclr_name, vctr_name, iter) write vtk solution at dual 0-cells for evolving simulations Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: sclr_name scalar variable name (label) character(len=*), intent(in) :: vctr_name vector variable name (label) integer, intent(in) :: iter integer iteration number Calls proc~~write_unsteady_d0s~~CallsGraph proc~write_unsteady_d0s write_unsteady_D0S proc~syncwrite_log syncwrite_log proc~write_unsteady_d0s:->proc~syncwrite_log: lcl_complex lcl_complex proc~write_unsteady_d0s:->lcl_complex: proc~root_open_file_read root_open_file_read proc~write_unsteady_d0s:->proc~root_open_file_read: indx_offset indx_offset proc~write_unsteady_d0s:->indx_offset: num_elm num_elm proc~write_unsteady_d0s:->num_elm: mpi_recv mpi_recv proc~write_unsteady_d0s:->mpi_recv: num_pelm_pp num_pelm_pp proc~write_unsteady_d0s:->num_pelm_pp: proc~syncwrite_log_time syncwrite_log_time proc~write_unsteady_d0s:->proc~syncwrite_log_time: mpi_barrier mpi_barrier proc~write_unsteady_d0s:->mpi_barrier: mpi_request_free mpi_request_free proc~write_unsteady_d0s:->mpi_request_free: mpi_waitall mpi_waitall proc~write_unsteady_d0s:->mpi_waitall: proc~root_open_file_write root_open_file_write proc~write_unsteady_d0s:->proc~root_open_file_write: mpi_wtime mpi_wtime proc~write_unsteady_d0s:->mpi_wtime: mpi_isend mpi_isend proc~write_unsteady_d0s:->mpi_isend: glb_num_elm glb_num_elm proc~write_unsteady_d0s:->glb_num_elm: proc~syncwrite_log:->mpi_barrier: mpi_bcast mpi_bcast proc~root_open_file_read:->mpi_bcast: proc~syncwrite_log_time:->mpi_barrier: proc~syncwrite_log_time:->mpi_wtime: proc~rootwrite_log rootwrite_log proc~syncwrite_log_time:->proc~rootwrite_log: proc~root_open_file_write:->mpi_bcast: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables fname buffer_size extra_comms fib i iib int_buffer istatus j junk junk_int_buffer kmax prelen ptr req status tmp_time tmp_time_int junk_real_buffer real_buffer Variables Type Visibility Attributes Name Initial character(len=slen), public :: fname file name integer, public :: buffer_size integer, public :: extra_comms integer, public :: fib integer, public :: i integer, public :: iib integer, public, ALLOCATABLE :: int_buffer (:) integer, public, ALLOCATABLE :: istatus (:,:) size of MPI comm buffer integer, public :: j integer, public :: junk integer, public, ALLOCATABLE :: junk_int_buffer (:) integer, public :: kmax integer, public :: prelen integer, public :: ptr integer, public, ALLOCATABLE :: req (:) request variable for non-blocking comms integer, public, ALLOCATABLE :: status (:) size of MPI comm buffer real(kind=IWP), public :: tmp_time temporary timing variable real(kind=IWP), public :: tmp_time_int temporary timing variable real(kind=iwp), public, ALLOCATABLE :: junk_real_buffer (:) real(kind=iwp), public, ALLOCATABLE :: real_buffer (:)","tags":"","loc":"proc/write_unsteady_d0s.html","title":"write_unsteady_D0S – ParaGEMS"},{"text":"public subroutine write_unsteady_D0S2(sclr_name, vctr_name, iter) write vtk solution at dual 0-cells for evolving simulations Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: sclr_name scalar variable name (label) character(len=*), intent(in) :: vctr_name vector variable name (label) integer, intent(in) :: iter integer iteration number Calls proc~~write_unsteady_d0s2~~CallsGraph proc~write_unsteady_d0s2 write_unsteady_D0S2 proc~syncwrite_log syncwrite_log proc~write_unsteady_d0s2:->proc~syncwrite_log: lcl_complex lcl_complex proc~write_unsteady_d0s2:->lcl_complex: proc~syncwrite_log_time syncwrite_log_time proc~write_unsteady_d0s2:->proc~syncwrite_log_time: mpi_recv mpi_recv proc~write_unsteady_d0s2:->mpi_recv: num_pelm_pp num_pelm_pp proc~write_unsteady_d0s2:->num_pelm_pp: mpi_barrier mpi_barrier proc~write_unsteady_d0s2:->mpi_barrier: num_elm num_elm proc~write_unsteady_d0s2:->num_elm: proc~root_open_file_write root_open_file_write proc~write_unsteady_d0s2:->proc~root_open_file_write: mpi_wtime mpi_wtime proc~write_unsteady_d0s2:->mpi_wtime: mpi_send mpi_send proc~write_unsteady_d0s2:->mpi_send: mpi_reduce mpi_reduce proc~write_unsteady_d0s2:->mpi_reduce: glb_num_elm glb_num_elm proc~write_unsteady_d0s2:->glb_num_elm: proc~syncwrite_log:->mpi_barrier: proc~syncwrite_log_time:->mpi_barrier: proc~syncwrite_log_time:->mpi_wtime: proc~rootwrite_log rootwrite_log proc~syncwrite_log_time:->proc~rootwrite_log: mpi_bcast mpi_bcast proc~root_open_file_write:->mpi_bcast: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables fname buffer_size comm_size extra_comms fib gindx i iib int_buffer istatus j junk junk_int_buffer k kmax max_comm_size num_comm_iters prelen ptr req resid_comm_size status strt_indx tmp_time tmp_time_int junk_real_buffer real_buffer Variables Type Visibility Attributes Name Initial character(len=slen), public :: fname file name integer, public :: buffer_size integer, public :: comm_size integer, public :: extra_comms integer, public :: fib integer, public :: gindx integer, public :: i integer, public :: iib integer, public, ALLOCATABLE :: int_buffer (:) integer, public, ALLOCATABLE :: istatus (:,:) size of MPI comm buffer integer, public :: j integer, public :: junk integer, public, ALLOCATABLE :: junk_int_buffer (:) integer, public :: k integer, public :: kmax integer, public :: max_comm_size integer, public :: num_comm_iters integer, public :: prelen integer, public :: ptr integer, public, ALLOCATABLE :: req (:) request variable for non-blocking comms integer, public :: resid_comm_size integer, public, ALLOCATABLE :: status (:) size of MPI comm buffer integer, public :: strt_indx real(kind=IWP), public :: tmp_time temporary timing variable real(kind=IWP), public :: tmp_time_int temporary timing variable real(kind=iwp), public, ALLOCATABLE :: junk_real_buffer (:) real(kind=iwp), public, ALLOCATABLE :: real_buffer (:)","tags":"","loc":"proc/write_unsteady_d0s2.html","title":"write_unsteady_D0S2 – ParaGEMS"},{"text":"public subroutine test_vars(cnt) Arguments Type Intent Optional Attributes Name integer, intent(inout) :: cnt Contents None","tags":"","loc":"proc/test_vars~3.html","title":"test_vars – ParaGEMS"},{"text":"public subroutine check_param_solver() Check values set for solver Arguments None Contents Variables error Variables Type Visibility Attributes Name Initial logical, public :: error = .FALSE.","tags":"","loc":"proc/check_param_solver.html","title":"check_param_solver – ParaGEMS"},{"text":"public subroutine end_petsc() Finalise PETSc library Arguments None Calls proc~~end_petsc~~CallsGraph proc~end_petsc end_petsc petscfinalize petscfinalize proc~end_petsc:->petscfinalize: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~end_petsc~~CalledByGraph proc~end_petsc end_petsc program~darcy_1f darcy_1f program~darcy_1f:->proc~end_petsc: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~end_petsc: program~darcy darcy program~darcy:->proc~end_petsc: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->proc~end_petsc: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~end_petsc: program~darcy_2f darcy_2f program~darcy_2f:->proc~end_petsc: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents None","tags":"","loc":"proc/end_petsc.html","title":"end_petsc – ParaGEMS"},{"text":"public subroutine extract_sol_KSP() extract local solution values from distributed PETSc vector scatter variable Arguments None Calls proc~~extract_sol_ksp~~CallsGraph proc~extract_sol_ksp extract_sol_KSP proc~syncwrite_log_time syncwrite_log_time proc~extract_sol_ksp:->proc~syncwrite_log_time: vecscatterbegin vecscatterbegin proc~extract_sol_ksp:->vecscatterbegin: vecscatterend vecscatterend proc~extract_sol_ksp:->vecscatterend: lcl_complex lcl_complex proc~extract_sol_ksp:->lcl_complex: vecdestroy vecdestroy proc~extract_sol_ksp:->vecdestroy: vecscatterdestroy vecscatterdestroy proc~extract_sol_ksp:->vecscatterdestroy: num_elm num_elm proc~extract_sol_ksp:->num_elm: vecscattercreate vecscattercreate proc~extract_sol_ksp:->vecscattercreate: glb_num_elm glb_num_elm proc~extract_sol_ksp:->glb_num_elm: isdestroy isdestroy proc~extract_sol_ksp:->isdestroy: mpi_wtime mpi_wtime proc~syncwrite_log_time:->mpi_wtime: proc~rootwrite_log rootwrite_log proc~syncwrite_log_time:->proc~rootwrite_log: mpi_barrier mpi_barrier proc~syncwrite_log_time:->mpi_barrier: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~extract_sol_ksp~~CalledByGraph proc~extract_sol_ksp extract_sol_KSP program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~extract_sol_ksp: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables i from to lcl_sol Variables Type Visibility Attributes Name Initial integer, public :: i loop index is, public :: from global and local vecor indices is, public :: to global and local vecor indices vec, public :: lcl_sol local solution vector","tags":"","loc":"proc/extract_sol_ksp.html","title":"extract_sol_KSP – ParaGEMS"},{"text":"public subroutine extract_sol_KSP2() extract local solution values from distributed PETSc vector scatter variable Arguments None Calls proc~~extract_sol_ksp2~~CallsGraph proc~extract_sol_ksp2 extract_sol_KSP2 proc~syncwrite_log_time syncwrite_log_time proc~extract_sol_ksp2:->proc~syncwrite_log_time: vecscatterbegin vecscatterbegin proc~extract_sol_ksp2:->vecscatterbegin: vecscattercreate vecscattercreate proc~extract_sol_ksp2:->vecscattercreate: vecscatterend vecscatterend proc~extract_sol_ksp2:->vecscatterend: lcl_complex lcl_complex proc~extract_sol_ksp2:->lcl_complex: vecdestroy vecdestroy proc~extract_sol_ksp2:->vecdestroy: vecscatterdestroy vecscatterdestroy proc~extract_sol_ksp2:->vecscatterdestroy: num_elm num_elm proc~extract_sol_ksp2:->num_elm: isg isg proc~extract_sol_ksp2:->isg: isdestroy isdestroy proc~extract_sol_ksp2:->isdestroy: mpi_wtime mpi_wtime proc~syncwrite_log_time:->mpi_wtime: proc~rootwrite_log rootwrite_log proc~syncwrite_log_time:->proc~rootwrite_log: mpi_barrier mpi_barrier proc~syncwrite_log_time:->mpi_barrier: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~extract_sol_ksp2~~CalledByGraph proc~extract_sol_ksp2 extract_sol_KSP2 program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~extract_sol_ksp2: program~darcy_2f darcy_2f program~darcy_2f:->proc~extract_sol_ksp2: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables fname i from to lcl_sol sol1 sol2 Variables Type Visibility Attributes Name Initial character(len=slen), public :: fname file name integer, public :: i loop index is, public :: from global and local vecor indices is, public :: to global and local vecor indices vec, public :: lcl_sol local solution vector vec, public :: sol1 vec, public :: sol2","tags":"","loc":"proc/extract_sol_ksp2.html","title":"extract_sol_KSP2 – ParaGEMS"},{"text":"public subroutine set_defaults_solver(solver) Set default values for solver Arguments Type Intent Optional Attributes Name character(len=*) :: solver Calls proc~~set_defaults_solver~~CallsGraph proc~set_defaults_solver set_defaults_solver proc~end_mpi end_mpi proc~set_defaults_solver:->proc~end_mpi: proc~syncwrite_log syncwrite_log proc~end_mpi:->proc~syncwrite_log: mpi_finalize mpi_finalize proc~end_mpi:->mpi_finalize: proc~syncwrite_log_time syncwrite_log_time proc~end_mpi:->proc~syncwrite_log_time: proc~close_log close_log proc~end_mpi:->proc~close_log: mpi_barrier mpi_barrier proc~syncwrite_log:->mpi_barrier: mpi_wtime mpi_wtime proc~syncwrite_log_time:->mpi_wtime: proc~rootwrite_log rootwrite_log proc~syncwrite_log_time:->proc~rootwrite_log: proc~syncwrite_log_time:->mpi_barrier: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents None","tags":"","loc":"proc/set_defaults_solver.html","title":"set_defaults_solver – ParaGEMS"},{"text":"public subroutine solve_KSP() Solve linear system using PETSc KSP Arguments None Calls proc~~solve_ksp~~CallsGraph proc~solve_ksp solve_KSP proc~syncwrite_log syncwrite_log proc~solve_ksp:->proc~syncwrite_log: pcfactorsetmatorderingtype pcfactorsetmatorderingtype proc~solve_ksp:->pcfactorsetmatorderingtype: kspsetfromoptions kspsetfromoptions proc~solve_ksp:->kspsetfromoptions: kspsolve kspsolve proc~solve_ksp:->kspsolve: kspcreate kspcreate proc~solve_ksp:->kspcreate: petscviewerandformatcreate petscviewerandformatcreate proc~solve_ksp:->petscviewerandformatcreate: kspsettolerances kspsettolerances proc~solve_ksp:->kspsettolerances: kspsetup kspsetup proc~solve_ksp:->kspsetup: kspgetpc kspgetpc proc~solve_ksp:->kspgetpc: kspgetresidualnorm kspgetresidualnorm proc~solve_ksp:->kspgetresidualnorm: kspview kspview proc~solve_ksp:->kspview: kspmonitorset kspmonitorset proc~solve_ksp:->kspmonitorset: mpi_wtime mpi_wtime proc~solve_ksp:->mpi_wtime: pcsettype pcsettype proc~solve_ksp:->pcsettype: kspsettype kspsettype proc~solve_ksp:->kspsettype: kspgetiterationnumber kspgetiterationnumber proc~solve_ksp:->kspgetiterationnumber: kspsetoperators kspsetoperators proc~solve_ksp:->kspsetoperators: mpi_barrier mpi_barrier proc~syncwrite_log:->mpi_barrier: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~solve_ksp~~CalledByGraph proc~solve_ksp solve_KSP program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~solve_ksp: program~darcy_1f darcy_1f program~darcy_1f:->proc~solve_ksp: program~darcy darcy program~darcy:->proc~solve_ksp: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->proc~solve_ksp: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables msg iters rnorm Variables Type Visibility Attributes Name Initial character(len=slen), public :: msg message sting for log file integer, public :: iters number of iterations real(kind=iwp), public :: rnorm final residual norm","tags":"","loc":"proc/solve_ksp.html","title":"solve_KSP – ParaGEMS"},{"text":"public subroutine solve_KSP2() Solve linear system using PETSc KSP Arguments None Calls proc~~solve_ksp2~~CallsGraph proc~solve_ksp2 solve_KSP2 kspsolve kspsolve proc~solve_ksp2:->kspsolve: kspcreate kspcreate proc~solve_ksp2:->kspcreate: kspsetup kspsetup proc~solve_ksp2:->kspsetup: kspgetconvergedreason kspgetconvergedreason proc~solve_ksp2:->kspgetconvergedreason: sub2pc_id sub2pc_id proc~solve_ksp2:->sub2pc_id: sub2ksp_id sub2ksp_id proc~solve_ksp2:->sub2ksp_id: subksp_id subksp_id proc~solve_ksp2:->subksp_id: kspgetresidualnorm kspgetresidualnorm proc~solve_ksp2:->kspgetresidualnorm: pcsettype pcsettype proc~solve_ksp2:->pcsettype: kspmonitorset kspmonitorset proc~solve_ksp2:->kspmonitorset: kspsetoperators kspsetoperators proc~solve_ksp2:->kspsetoperators: subpc_id subpc_id proc~solve_ksp2:->subpc_id: petscviewerandformatcreate petscviewerandformatcreate proc~solve_ksp2:->petscviewerandformatcreate: kspsettolerances kspsettolerances proc~solve_ksp2:->kspsettolerances: pcsetup pcsetup proc~solve_ksp2:->pcsetup: mpi_wtime mpi_wtime proc~solve_ksp2:->mpi_wtime: kspsetfromoptions kspsetfromoptions proc~solve_ksp2:->kspsetfromoptions: kspsettype kspsettype proc~solve_ksp2:->kspsettype: proc~syncwrite_log syncwrite_log proc~solve_ksp2:->proc~syncwrite_log: pcfieldsplitsetschurfacttype pcfieldsplitsetschurfacttype proc~solve_ksp2:->pcfieldsplitsetschurfacttype: pcfieldsplitgetsubksp pcfieldsplitgetsubksp proc~solve_ksp2:->pcfieldsplitgetsubksp: kspview kspview proc~solve_ksp2:->kspview: pcfieldsplitsetschurpre pcfieldsplitsetschurpre proc~solve_ksp2:->pcfieldsplitsetschurpre: kspgetpc kspgetpc proc~solve_ksp2:->kspgetpc: pcfieldsplitsettype pcfieldsplitsettype proc~solve_ksp2:->pcfieldsplitsettype: isg isg proc~solve_ksp2:->isg: kspgetiterationnumber kspgetiterationnumber proc~solve_ksp2:->kspgetiterationnumber: kspdestroy kspdestroy proc~solve_ksp2:->kspdestroy: mpi_barrier mpi_barrier proc~syncwrite_log:->mpi_barrier: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~solve_ksp2~~CalledByGraph proc~solve_ksp2 solve_KSP2 program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~solve_ksp2: program~darcy_2f darcy_2f program~darcy_2f:->proc~solve_ksp2: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables fname msg iters m n rnorm Variables Type Visibility Attributes Name Initial character(len=slen), public :: fname file name character(len=slen), public :: msg message sting for log file integer, public :: iters number of iterations integer, public :: m number of iterations integer, public :: n number of iterations real(kind=iwp), public :: rnorm final residual norm","tags":"","loc":"proc/solve_ksp2.html","title":"solve_KSP2 – ParaGEMS"},{"text":"public subroutine start_petsc() Initialise PETSc library Arguments None Calls proc~~start_petsc~~CallsGraph proc~start_petsc start_petsc petscinitialize petscinitialize proc~start_petsc:->petscinitialize: proc~syncwrite_log syncwrite_log proc~start_petsc:->proc~syncwrite_log: proc~end_mpi end_mpi proc~start_petsc:->proc~end_mpi: mpi_barrier mpi_barrier proc~syncwrite_log:->mpi_barrier: proc~end_mpi:->proc~syncwrite_log: proc~close_log close_log proc~end_mpi:->proc~close_log: mpi_finalize mpi_finalize proc~end_mpi:->mpi_finalize: proc~syncwrite_log_time syncwrite_log_time proc~end_mpi:->proc~syncwrite_log_time: proc~syncwrite_log_time:->mpi_barrier: mpi_wtime mpi_wtime proc~syncwrite_log_time:->mpi_wtime: proc~rootwrite_log rootwrite_log proc~syncwrite_log_time:->proc~rootwrite_log: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~start_petsc~~CalledByGraph proc~start_petsc start_petsc program~darcy_1f darcy_1f program~darcy_1f:->proc~start_petsc: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~start_petsc: program~darcy darcy program~darcy:->proc~start_petsc: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->proc~start_petsc: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~start_petsc: program~darcy_2f darcy_2f program~darcy_2f:->proc~start_petsc: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents None","tags":"","loc":"proc/start_petsc.html","title":"start_petsc – ParaGEMS"},{"text":"public subroutine test_vars(cnt) Arguments Type Intent Optional Attributes Name integer, intent(inout) :: cnt Contents None","tags":"","loc":"proc/test_vars~4.html","title":"test_vars – ParaGEMS"},{"text":"public function index_in_list(elm, list, init_guess) Test if any element of one array is in another Arguments Type Intent Optional Attributes Name integer, intent(in) :: elm integer, intent(in) :: list (:) integer, intent(in) :: init_guess Return Value integer Called by proc~~index_in_list~~CalledByGraph proc~index_in_list index_in_list proc~get_connectivity get_connectivity proc~get_connectivity:->proc~index_in_list: proc~parallel_setup parallel_setup proc~parallel_setup:->proc~get_connectivity: program~darcy_1f darcy_1f program~darcy_1f:->proc~parallel_setup: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~parallel_setup: program~darcy darcy program~darcy:->proc~parallel_setup: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->proc~parallel_setup: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~parallel_setup: program~darcy_2f darcy_2f program~darcy_2f:->proc~parallel_setup: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables in Variables Type Visibility Attributes Name Initial integer, public :: in","tags":"","loc":"proc/index_in_list.html","title":"index_in_list – ParaGEMS"},{"text":"public function num_element_in_list(elm, node_list) Test if any element of one array is in another Arguments Type Intent Optional Attributes Name integer, intent(in) :: elm (:) integer, intent(in) :: node_list (:) Return Value integer Contents Variables in Variables Type Visibility Attributes Name Initial integer, public :: in","tags":"","loc":"proc/num_element_in_list.html","title":"num_element_in_list – ParaGEMS"},{"text":"public function any_element_in_list(elm, node_list) Test if any element of one array is in another Arguments Type Intent Optional Attributes Name integer, intent(in) :: elm (:) integer, intent(in) :: node_list (:) Return Value logical Called by proc~~any_element_in_list~~CalledByGraph proc~any_element_in_list any_element_in_list proc~count_bndry_cobndry count_bndry_cobndry proc~count_bndry_cobndry:->proc~any_element_in_list: proc~calc_bndry_cobndry calc_bndry_cobndry proc~calc_bndry_cobndry:->proc~count_bndry_cobndry: proc~parallel_setup parallel_setup proc~parallel_setup:->proc~calc_bndry_cobndry: program~darcy_1f darcy_1f program~darcy_1f:->proc~parallel_setup: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~parallel_setup: program~darcy darcy program~darcy:->proc~parallel_setup: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->proc~parallel_setup: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~parallel_setup: program~darcy_2f darcy_2f program~darcy_2f:->proc~parallel_setup: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables end_array in jn Variables Type Visibility Attributes Name Initial integer, public :: end_array integer, public :: in integer, public :: jn","tags":"","loc":"proc/any_element_in_list.html","title":"any_element_in_list – ParaGEMS"},{"text":"public function cross_product(a, b) Performs a direct calculation of vector cross product Arguments Type Intent Optional Attributes Name real(kind=iwp), intent(in) :: a (3) input vectors real(kind=iwp), intent(in) :: b (3) input vectors Return Value real(kind=iwp)\n  (3) resulting vector Called by proc~~cross_product~~CalledByGraph proc~cross_product cross_product proc~calc_whitney_c2_bc calc_whitney_C2_BC proc~calc_whitney_c2_bc:->proc~cross_product: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents None","tags":"","loc":"proc/cross_product.html","title":"cross_product – ParaGEMS"},{"text":"public recursive function determinant(A, n) result(accumulation) Compute determinant Arguments Type Intent Optional Attributes Name real(kind=iwp), intent(in) :: A (n,n) integer, intent(in) :: n Return Value real(kind=iwp) Called by proc~~determinant~~CalledByGraph proc~determinant determinant proc~determinant:->proc~determinant: proc~calc_unsgnd_vlm calc_unsgnd_vlm proc~calc_unsgnd_vlm:->proc~determinant: proc~calc_prml_sgnd_vlm calc_prml_sgnd_vlm proc~calc_prml_sgnd_vlm:->proc~determinant: proc~calc_prml_unsgnd_vlm calc_prml_unsgnd_vlm proc~calc_prml_unsgnd_vlm:->proc~determinant: proc~calc_dual_vlm_i calc_dual_vlm_i proc~calc_dual_vlm_i:->proc~calc_unsgnd_vlm: proc~initialise_geo initialise_geo proc~initialise_geo:->proc~calc_prml_sgnd_vlm: proc~initialise_geo:->proc~calc_prml_unsgnd_vlm: proc~calc_dual_vlm calc_dual_vlm proc~initialise_geo:->proc~calc_dual_vlm: proc~calc_dual_vlm:->proc~calc_dual_vlm_i: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables i sgn B Variables Type Visibility Attributes Name Initial integer, public :: i integer, public :: sgn real(kind=iwp), public :: B (n-1,n-1)","tags":"","loc":"proc/determinant.html","title":"determinant – ParaGEMS"},{"text":"public function matinv2(A) result(B) Performs a direct calculation of the inverse of a 2x2 matrix. Arguments Type Intent Optional Attributes Name real(kind=iwp), intent(in) :: A (2,2) Matrix Return Value real(kind=iwp)\n  (2,2) Inverse matrix Called by proc~~matinv2~~CalledByGraph proc~matinv2 matinv2 proc~calc_barycentric_grad calc_barycentric_grad proc~calc_barycentric_grad:->proc~matinv2: proc~calc_whitney_c2_bc calc_whitney_C2_BC proc~calc_whitney_c2_bc:->proc~calc_barycentric_grad: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables detinv Variables Type Visibility Attributes Name Initial real(kind=iwp), public :: detinv","tags":"","loc":"proc/matinv2.html","title":"matinv2 – ParaGEMS"},{"text":"public function matinv3(A) result(B) Performs a direct calculation of the inverse of a 3×3 matrix. Arguments Type Intent Optional Attributes Name real(kind=iwp), intent(in) :: A (3,3) Matrix Return Value real(kind=iwp)\n  (3,3) Inverse matrix Called by proc~~matinv3~~CalledByGraph proc~matinv3 matinv3 proc~calc_barycentric_grad calc_barycentric_grad proc~calc_barycentric_grad:->proc~matinv3: proc~calc_whitney_c2_bc calc_whitney_C2_BC proc~calc_whitney_c2_bc:->proc~calc_barycentric_grad: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables detinv Variables Type Visibility Attributes Name Initial real(kind=iwp), public :: detinv","tags":"","loc":"proc/matinv3.html","title":"matinv3 – ParaGEMS"},{"text":"public subroutine int_insertion_sort(A) Insertion sort of integer array Arguments Type Intent Optional Attributes Name integer, intent(inout) :: A (:) Contents Variables i j work Variables Type Visibility Attributes Name Initial integer, public :: i integer, public :: j integer, public :: work","tags":"","loc":"proc/int_insertion_sort.html","title":"int_insertion_sort – ParaGEMS"},{"text":"public subroutine int_merge_rows(A, B, C, irow, frow) Merge rows of an array in ascending order\n Assumption: rows are already sorted in ascending order Arguments Type Intent Optional Attributes Name integer, intent(in) :: A (:,:) integer, intent(in) :: B (:,:) integer, intent(inout) :: C (:,:) integer, intent(in) :: irow integer, intent(in) :: frow Calls proc~~int_merge_rows~~CallsGraph proc~int_merge_rows int_merge_rows proc~end_mpi end_mpi proc~int_merge_rows:->proc~end_mpi: proc~syncwrite_log syncwrite_log proc~end_mpi:->proc~syncwrite_log: mpi_finalize mpi_finalize proc~end_mpi:->mpi_finalize: proc~syncwrite_log_time syncwrite_log_time proc~end_mpi:->proc~syncwrite_log_time: proc~close_log close_log proc~end_mpi:->proc~close_log: mpi_barrier mpi_barrier proc~syncwrite_log:->mpi_barrier: mpi_wtime mpi_wtime proc~syncwrite_log_time:->mpi_wtime: proc~rootwrite_log rootwrite_log proc~syncwrite_log_time:->proc~rootwrite_log: proc~syncwrite_log_time:->mpi_barrier: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables i ir j k is_le Variables Type Visibility Attributes Name Initial integer, public :: i integer, public :: ir integer, public :: j integer, public :: k logical, public :: is_le","tags":"","loc":"proc/int_merge_rows.html","title":"int_merge_rows – ParaGEMS"},{"text":"public subroutine int_merge_sort_rows(A, length, irow, frow, work) Merge sort rows of integer array in ascending order\n Assumption: rows are already sorted in ascending order Arguments Type Intent Optional Attributes Name integer, intent(inout) :: A (:,:) integer :: length integer, intent(in) :: irow integer, intent(in) :: frow integer, intent(inout) :: work (:,:) Contents Variables half i ir Variables Type Visibility Attributes Name Initial integer, public :: half integer, public :: i integer, public :: ir","tags":"","loc":"proc/int_merge_sort_rows.html","title":"int_merge_sort_rows – ParaGEMS"},{"text":"public subroutine check_param_darcy() Check values set for solving Darcy flow simulation Arguments None Calls proc~~check_param_darcy~~CallsGraph proc~check_param_darcy check_param_darcy proc~rootwrite_log rootwrite_log proc~check_param_darcy:->proc~rootwrite_log: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~check_param_darcy~~CalledByGraph proc~check_param_darcy check_param_darcy proc~read_input_darcy read_input_darcy proc~read_input_darcy:->proc~check_param_darcy: program~darcy_1f darcy_1f program~darcy_1f:->proc~read_input_darcy: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~read_input_darcy: program~darcy darcy program~darcy:->proc~read_input_darcy: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->proc~read_input_darcy: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~read_input_darcy: program~darcy_2f darcy_2f program~darcy_2f:->proc~read_input_darcy: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables msg err Variables Type Visibility Attributes Name Initial character(len=SLEN), public :: msg error msg string logical, public :: err = .FALSE. error flag","tags":"","loc":"proc/check_param_darcy.html","title":"check_param_darcy – ParaGEMS"},{"text":"public subroutine exchange_bndry_cond(k) exchange boundary conditions Arguments Type Intent Optional Attributes Name integer, intent(in) :: k simplicial order Calls proc~~exchange_bndry_cond~~CallsGraph proc~exchange_bndry_cond exchange_bndry_cond lcl_complex lcl_complex proc~exchange_bndry_cond:->lcl_complex: mpi_waitall mpi_waitall proc~exchange_bndry_cond:->mpi_waitall: mpi_request_free mpi_request_free proc~exchange_bndry_cond:->mpi_request_free: mpi_barrier mpi_barrier proc~exchange_bndry_cond:->mpi_barrier: num_send num_send proc~exchange_bndry_cond:->num_send: adj_proc adj_proc proc~exchange_bndry_cond:->adj_proc: num_recv num_recv proc~exchange_bndry_cond:->num_recv: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables buffer_size i j junk kp ptr ptrp rbuffer req sbuffer status Variables Type Visibility Attributes Name Initial integer, public :: buffer_size size of comm buffer integer, public :: i loop index integer, public :: j loop index integer, public :: junk junk comm variable integer, public :: kp simplicial order integer, public :: ptr pointer integer, public :: ptrp pointer integer, public, ALLOCATABLE :: rbuffer (:) receive buffer integer, public, ALLOCATABLE :: req (:) request variable for non-blocking comms integer, public, ALLOCATABLE :: sbuffer (:) send buffer integer, public, ALLOCATABLE :: status (:,:) size of MPI comm buffer","tags":"","loc":"proc/exchange_bndry_cond.html","title":"exchange_bndry_cond – ParaGEMS"},{"text":"public subroutine finalise_darcy() clean up matrices and vectors for Darcy simulations Arguments None Calls proc~~finalise_darcy~~CallsGraph proc~finalise_darcy finalise_darcy vecdestroy vecdestroy proc~finalise_darcy:->vecdestroy: matdestroy matdestroy proc~finalise_darcy:->matdestroy: kspdestroy kspdestroy proc~finalise_darcy:->kspdestroy: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~finalise_darcy~~CalledByGraph proc~finalise_darcy finalise_darcy program~darcy_1f darcy_1f program~darcy_1f:->proc~finalise_darcy: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~finalise_darcy: program~darcy darcy program~darcy:->proc~finalise_darcy: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->proc~finalise_darcy: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~finalise_darcy: program~darcy_2f darcy_2f program~darcy_2f:->proc~finalise_darcy: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents None","tags":"","loc":"proc/finalise_darcy.html","title":"finalise_darcy – ParaGEMS"},{"text":"public subroutine finalise_darcy2() clean up matrices and vectors for Darcy simulations Arguments None Calls proc~~finalise_darcy2~~CallsGraph proc~finalise_darcy2 finalise_darcy2 asub asub proc~finalise_darcy2:->asub: isg isg proc~finalise_darcy2:->isg: matdestroy matdestroy proc~finalise_darcy2:->matdestroy: kspdestroy kspdestroy proc~finalise_darcy2:->kspdestroy: vecdestroy vecdestroy proc~finalise_darcy2:->vecdestroy: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents None","tags":"","loc":"proc/finalise_darcy2.html","title":"finalise_darcy2 – ParaGEMS"},{"text":"public subroutine get_LHS_darcy() setup left hand side matrix for darcy simulations Arguments None Calls proc~~get_lhs_darcy~~CallsGraph proc~get_lhs_darcy get_LHS_darcy proc~syncwrite_log syncwrite_log proc~get_lhs_darcy:->proc~syncwrite_log: proc~syncwrite_log_time syncwrite_log_time proc~get_lhs_darcy:->proc~syncwrite_log_time: lcl_complex lcl_complex proc~get_lhs_darcy:->lcl_complex: matassemblybegin matassemblybegin proc~get_lhs_darcy:->matassemblybegin: matsetoption matsetoption proc~get_lhs_darcy:->matsetoption: num_elm num_elm proc~get_lhs_darcy:->num_elm: mpi_wtime mpi_wtime proc~get_lhs_darcy:->mpi_wtime: matassemblyend matassemblyend proc~get_lhs_darcy:->matassemblyend: glb_num_elm glb_num_elm proc~get_lhs_darcy:->glb_num_elm: mpi_barrier mpi_barrier proc~syncwrite_log:->mpi_barrier: proc~syncwrite_log_time:->mpi_wtime: proc~rootwrite_log rootwrite_log proc~syncwrite_log_time:->proc~rootwrite_log: proc~syncwrite_log_time:->mpi_barrier: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~get_lhs_darcy~~CalledByGraph proc~get_lhs_darcy get_LHS_darcy program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~get_lhs_darcy: program~darcy_1f darcy_1f program~darcy_1f:->proc~get_lhs_darcy: program~darcy darcy program~darcy:->proc~get_lhs_darcy: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->proc~get_lhs_darcy: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables i j offset Variables Type Visibility Attributes Name Initial integer, public :: i integer, public :: j integer, public :: offset","tags":"","loc":"proc/get_lhs_darcy.html","title":"get_LHS_darcy – ParaGEMS"},{"text":"public subroutine get_LHS_darcy2() setup left hand side matrix for darcy simulations Arguments None Calls proc~~get_lhs_darcy2~~CallsGraph proc~get_lhs_darcy2 get_LHS_darcy2 proc~syncwrite_log syncwrite_log proc~get_lhs_darcy2:->proc~syncwrite_log: proc~syncwrite_log_time syncwrite_log_time proc~get_lhs_darcy2:->proc~syncwrite_log_time: asub asub proc~get_lhs_darcy2:->asub: lcl_complex lcl_complex proc~get_lhs_darcy2:->lcl_complex: glb_num_elm glb_num_elm proc~get_lhs_darcy2:->glb_num_elm: matsetoption matsetoption proc~get_lhs_darcy2:->matsetoption: matcreatenest matcreatenest proc~get_lhs_darcy2:->matcreatenest: num_elm num_elm proc~get_lhs_darcy2:->num_elm: mpi_wtime mpi_wtime proc~get_lhs_darcy2:->mpi_wtime: matnestgetiss matnestgetiss proc~get_lhs_darcy2:->matnestgetiss: mpi_barrier mpi_barrier proc~syncwrite_log:->mpi_barrier: proc~syncwrite_log_time:->mpi_wtime: proc~rootwrite_log rootwrite_log proc~syncwrite_log_time:->proc~rootwrite_log: proc~syncwrite_log_time:->mpi_barrier: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~get_lhs_darcy2~~CalledByGraph proc~get_lhs_darcy2 get_LHS_darcy2 program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~get_lhs_darcy2: program~darcy_2f darcy_2f program~darcy_2f:->proc~get_lhs_darcy2: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables i j m offset rnd Variables Type Visibility Attributes Name Initial integer, public :: i integer, public :: j integer, public :: m number of rows/columns in global solution variables integer, public :: offset real(kind=iwp), public :: rnd","tags":"","loc":"proc/get_lhs_darcy2.html","title":"get_LHS_darcy2 – ParaGEMS"},{"text":"public subroutine get_RHS_darcy() setup right hand side vector and boundary conditions for darcy simulations Arguments None Calls proc~~get_rhs_darcy~~CallsGraph proc~get_rhs_darcy get_RHS_darcy proc~syncwrite_log syncwrite_log proc~get_rhs_darcy:->proc~syncwrite_log: lcl_complex lcl_complex proc~get_rhs_darcy:->lcl_complex: bc_press bc_press proc~get_rhs_darcy:->bc_press: proc~syncwrite_log_time syncwrite_log_time proc~get_rhs_darcy:->proc~syncwrite_log_time: vecassemblyend vecassemblyend proc~get_rhs_darcy:->vecassemblyend: proc~read_bndry_cond2 read_bndry_cond2 proc~get_rhs_darcy:->proc~read_bndry_cond2: num_elm num_elm proc~get_rhs_darcy:->num_elm: mpi_wtime mpi_wtime proc~get_rhs_darcy:->mpi_wtime: mpi_reduce mpi_reduce proc~get_rhs_darcy:->mpi_reduce: vecassemblybegin vecassemblybegin proc~get_rhs_darcy:->vecassemblybegin: glb_num_elm glb_num_elm proc~get_rhs_darcy:->glb_num_elm: mpi_barrier mpi_barrier proc~syncwrite_log:->mpi_barrier: proc~syncwrite_log_time:->mpi_wtime: proc~rootwrite_log rootwrite_log proc~syncwrite_log_time:->proc~rootwrite_log: proc~syncwrite_log_time:->mpi_barrier: proc~read_bndry_cond2:->lcl_complex: proc~read_bndry_cond2:->num_elm: proc~read_bndry_cond2:->glb_num_elm: mpi_bcast mpi_bcast proc~read_bndry_cond2:->mpi_bcast: proc~root_open_file_read root_open_file_read proc~read_bndry_cond2:->proc~root_open_file_read: proc~read_bndry_cond2:->mpi_barrier: proc~root_open_file_read:->mpi_bcast: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~get_rhs_darcy~~CalledByGraph proc~get_rhs_darcy get_RHS_darcy program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~get_rhs_darcy: program~darcy_1f darcy_1f program~darcy_1f:->proc~get_rhs_darcy: program~darcy darcy program~darcy:->proc~get_rhs_darcy: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->proc~get_rhs_darcy: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables fname flx_indx i indx num_flx offset type_bc l_use_pref use_pref Variables Type Visibility Attributes Name Initial character(len=slen), public :: fname file name integer, public, ALLOCATABLE :: flx_indx (:) flx boundary condition indices integer, public :: i loop and temporary indicies integer, public :: indx loop and temporary indicies integer, public :: num_flx number of flux boundary faces integer, public :: offset vector offset for volumes (after faces) integer, public :: type_bc type of boundary condition logical, public :: l_use_pref use reference pressure (if no pressure bc used) logical, public :: use_pref use reference pressure (if no pressure bc used)","tags":"","loc":"proc/get_rhs_darcy.html","title":"get_RHS_darcy – ParaGEMS"},{"text":"public subroutine get_RHS_darcy2() setup right hand side vector and boundary conditions for darcy simulations Arguments None Calls proc~~get_rhs_darcy2~~CallsGraph proc~get_rhs_darcy2 get_RHS_darcy2 proc~syncwrite_log syncwrite_log proc~get_rhs_darcy2:->proc~syncwrite_log: lcl_complex lcl_complex proc~get_rhs_darcy2:->lcl_complex: matgetownershiprange matgetownershiprange proc~get_rhs_darcy2:->matgetownershiprange: bc_press bc_press proc~get_rhs_darcy2:->bc_press: proc~syncwrite_log_time syncwrite_log_time proc~get_rhs_darcy2:->proc~syncwrite_log_time: vecsetfromoptions vecsetfromoptions proc~get_rhs_darcy2:->vecsetfromoptions: vecassemblyend vecassemblyend proc~get_rhs_darcy2:->vecassemblyend: vecduplicate vecduplicate proc~get_rhs_darcy2:->vecduplicate: vecdestroy vecdestroy proc~get_rhs_darcy2:->vecdestroy: glb_num_elm glb_num_elm proc~get_rhs_darcy2:->glb_num_elm: num_elm num_elm proc~get_rhs_darcy2:->num_elm: asub asub proc~get_rhs_darcy2:->asub: mpi_wtime mpi_wtime proc~get_rhs_darcy2:->mpi_wtime: isg isg proc~get_rhs_darcy2:->isg: vecassemblybegin vecassemblybegin proc~get_rhs_darcy2:->vecassemblybegin: proc~read_bndry_cond2 read_bndry_cond2 proc~get_rhs_darcy2:->proc~read_bndry_cond2: matcreatenest matcreatenest proc~get_rhs_darcy2:->matcreatenest: mpi_barrier mpi_barrier proc~syncwrite_log:->mpi_barrier: proc~syncwrite_log_time:->mpi_wtime: proc~rootwrite_log rootwrite_log proc~syncwrite_log_time:->proc~rootwrite_log: proc~syncwrite_log_time:->mpi_barrier: proc~read_bndry_cond2:->lcl_complex: proc~read_bndry_cond2:->glb_num_elm: proc~read_bndry_cond2:->num_elm: mpi_bcast mpi_bcast proc~read_bndry_cond2:->mpi_bcast: proc~root_open_file_read root_open_file_read proc~read_bndry_cond2:->proc~root_open_file_read: proc~read_bndry_cond2:->mpi_barrier: proc~root_open_file_read:->mpi_bcast: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~get_rhs_darcy2~~CalledByGraph proc~get_rhs_darcy2 get_RHS_darcy2 program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~get_rhs_darcy2: program~darcy_2f darcy_2f program~darcy_2f:->proc~get_rhs_darcy2: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables fname flx_indx i indx m n num_flx offset type_bc l_use_pref use_pref flx b1 b2 sol1 sol2 wrk Variables Type Visibility Attributes Name Initial character(len=slen), public :: fname file name integer, public, ALLOCATABLE :: flx_indx (:) flx boundary condition indices integer, public :: i loop and temporary indicies integer, public :: indx loop and temporary indicies integer, public :: m integer, public :: n integer, public :: num_flx number of flux boundary faces integer, public :: offset vector offset for volumes (after faces) integer, public :: type_bc type of boundary condition logical, public :: l_use_pref use reference pressure (if no pressure bc used) logical, public :: use_pref use reference pressure (if no pressure bc used) real(kind=iwp), public :: flx vec, public :: b1 vec, public :: b2 vec, public :: sol1 vec, public :: sol2 vec, public :: wrk","tags":"","loc":"proc/get_rhs_darcy2.html","title":"get_RHS_darcy2 – ParaGEMS"},{"text":"public subroutine identify_crack(exit_cond) identify faces to crack based on threshold value Arguments Type Intent Optional Attributes Name logical, intent(out) :: exit_cond Calls proc~~identify_crack~~CallsGraph proc~identify_crack identify_crack proc~syncwrite_log syncwrite_log proc~identify_crack:->proc~syncwrite_log: lcl_complex lcl_complex proc~identify_crack:->lcl_complex: proc~syncwrite_log_time syncwrite_log_time proc~identify_crack:->proc~syncwrite_log_time: vecassemblyend vecassemblyend proc~identify_crack:->vecassemblyend: mpi_allreduce mpi_allreduce proc~identify_crack:->mpi_allreduce: num_elm num_elm proc~identify_crack:->num_elm: mpi_wtime mpi_wtime proc~identify_crack:->mpi_wtime: vecassemblybegin vecassemblybegin proc~identify_crack:->vecassemblybegin: mpi_barrier mpi_barrier proc~syncwrite_log:->mpi_barrier: proc~syncwrite_log_time:->mpi_wtime: proc~rootwrite_log rootwrite_log proc~syncwrite_log_time:->proc~rootwrite_log: proc~syncwrite_log_time:->mpi_barrier: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~identify_crack~~CalledByGraph proc~identify_crack identify_crack program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~identify_crack: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables flx_indx i num_flx type_bc lcl_exit_cond Variables Type Visibility Attributes Name Initial integer, public, ALLOCATABLE :: flx_indx (:) flx boundary condition indices integer, public :: i loop and temporary indicies integer, public :: num_flx number of flux boundary faces integer, public :: type_bc type of boundary condition logical, public :: lcl_exit_cond = .FALSE.","tags":"","loc":"proc/identify_crack.html","title":"identify_crack – ParaGEMS"},{"text":"public subroutine identify_crack2(exit_cond) identify faces to crack based on threshold value Arguments Type Intent Optional Attributes Name logical, intent(out) :: exit_cond Calls proc~~identify_crack2~~CallsGraph proc~identify_crack2 identify_crack2 proc~syncwrite_log syncwrite_log proc~identify_crack2:->proc~syncwrite_log: lcl_complex lcl_complex proc~identify_crack2:->lcl_complex: proc~syncwrite_log_time syncwrite_log_time proc~identify_crack2:->proc~syncwrite_log_time: vecassemblyend vecassemblyend proc~identify_crack2:->vecassemblyend: mpi_allreduce mpi_allreduce proc~identify_crack2:->mpi_allreduce: num_elm num_elm proc~identify_crack2:->num_elm: mpi_wtime mpi_wtime proc~identify_crack2:->mpi_wtime: vecassemblybegin vecassemblybegin proc~identify_crack2:->vecassemblybegin: mpi_barrier mpi_barrier proc~syncwrite_log:->mpi_barrier: proc~syncwrite_log_time:->mpi_wtime: proc~rootwrite_log rootwrite_log proc~syncwrite_log_time:->proc~rootwrite_log: proc~syncwrite_log_time:->mpi_barrier: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables msg flx_indx glb_num_flx i indx j lcl_num_flx num_flx type_bc lcl_exit_cond flx flxs tmp_crck_thrshld Variables Type Visibility Attributes Name Initial character(len=slen), public :: msg integer, public, ALLOCATABLE :: flx_indx (:) flx boundary condition indices integer, public :: glb_num_flx number of flux boundary faces integer, public :: i loop and temporary indicies integer, public :: indx loop and temporary indicies integer, public :: j loop and temporary indicies integer, public :: lcl_num_flx number of flux boundary faces integer, public :: num_flx number of flux boundary faces integer, public :: type_bc type of boundary condition logical, public :: lcl_exit_cond = .FALSE. real(kind=iwp), public :: flx flx boundary condition indices real(kind=iwp), public, ALLOCATABLE :: flxs (:) flx boundary condition indices real(kind=iwp), public :: tmp_crck_thrshld flx boundary condition indices","tags":"","loc":"proc/identify_crack2.html","title":"identify_crack2 – ParaGEMS"},{"text":"public subroutine identify_crack3(iter, exit_cond) identify faces to crack based on max value Arguments Type Intent Optional Attributes Name integer, intent(in) :: iter logical, intent(out) :: exit_cond Calls proc~~identify_crack3~~CallsGraph proc~identify_crack3 identify_crack3 proc~syncwrite_log syncwrite_log proc~identify_crack3:->proc~syncwrite_log: lcl_complex lcl_complex proc~identify_crack3:->lcl_complex: matzerorowscolumns matzerorowscolumns proc~identify_crack3:->matzerorowscolumns: proc~syncwrite_log_time syncwrite_log_time proc~identify_crack3:->proc~syncwrite_log_time: vecassemblyend vecassemblyend proc~identify_crack3:->vecassemblyend: num_elm num_elm proc~identify_crack3:->num_elm: mpi_wtime mpi_wtime proc~identify_crack3:->mpi_wtime: mpi_reduce mpi_reduce proc~identify_crack3:->mpi_reduce: vecassemblybegin vecassemblybegin proc~identify_crack3:->vecassemblybegin: mpi_barrier mpi_barrier proc~syncwrite_log:->mpi_barrier: proc~syncwrite_log_time:->mpi_wtime: proc~rootwrite_log rootwrite_log proc~syncwrite_log_time:->proc~rootwrite_log: proc~syncwrite_log_time:->mpi_barrier: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~identify_crack3~~CalledByGraph proc~identify_crack3 identify_crack3 program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~identify_crack3: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables msg i junk max_indx status area flx glb_max_flx max_flx Variables Type Visibility Attributes Name Initial character(len=slen), public :: msg integer, public :: i loop and temporary indicies integer, public :: junk loop and temporary indicies integer, public :: max_indx (2) loop and temporary indicies integer, public :: status (MPI_STATUS_SIZE) size of MPI comm buffer real(kind=iwp), public :: area real(kind=iwp), public :: flx real(kind=iwp), public :: glb_max_flx (3) real(kind=iwp), public :: max_flx (3)","tags":"","loc":"proc/identify_crack3.html","title":"identify_crack3 – ParaGEMS"},{"text":"public subroutine identify_crack4(iter, exit_cond) identify faces to crack based on threshold value Arguments Type Intent Optional Attributes Name integer, intent(in) :: iter logical, intent(out) :: exit_cond Calls proc~~identify_crack4~~CallsGraph proc~identify_crack4 identify_crack4 proc~syncwrite_log syncwrite_log proc~identify_crack4:->proc~syncwrite_log: vecassemblyend vecassemblyend proc~identify_crack4:->vecassemblyend: proc~syncwrite_log_time syncwrite_log_time proc~identify_crack4:->proc~syncwrite_log_time: asub asub proc~identify_crack4:->asub: lcl_complex lcl_complex proc~identify_crack4:->lcl_complex: mpi_allreduce mpi_allreduce proc~identify_crack4:->mpi_allreduce: num_elm num_elm proc~identify_crack4:->num_elm: mpi_wtime mpi_wtime proc~identify_crack4:->mpi_wtime: vecassemblybegin vecassemblybegin proc~identify_crack4:->vecassemblybegin: mpi_barrier mpi_barrier proc~syncwrite_log:->mpi_barrier: proc~syncwrite_log_time:->mpi_wtime: proc~rootwrite_log rootwrite_log proc~syncwrite_log_time:->proc~rootwrite_log: proc~syncwrite_log_time:->mpi_barrier: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~identify_crack4~~CalledByGraph proc~identify_crack4 identify_crack4 program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~identify_crack4: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables flx_indx i j num_flx type_bc lcl_exit_cond Variables Type Visibility Attributes Name Initial integer, public, ALLOCATABLE :: flx_indx (:) flx boundary condition indices integer, public :: i loop and temporary indicies integer, public :: j loop and temporary indicies integer, public :: num_flx number of flux boundary faces integer, public :: type_bc type of boundary condition logical, public :: lcl_exit_cond = .FALSE.","tags":"","loc":"proc/identify_crack4.html","title":"identify_crack4 – ParaGEMS"},{"text":"public subroutine identify_crack5(iter, exit_cond) identify faces to crack based on max value Arguments Type Intent Optional Attributes Name integer, intent(in) :: iter logical, intent(out) :: exit_cond Calls proc~~identify_crack5~~CallsGraph proc~identify_crack5 identify_crack5 proc~syncwrite_log syncwrite_log proc~identify_crack5:->proc~syncwrite_log: vecassemblyend vecassemblyend proc~identify_crack5:->vecassemblyend: proc~syncwrite_log_time syncwrite_log_time proc~identify_crack5:->proc~syncwrite_log_time: asub asub proc~identify_crack5:->asub: lcl_complex lcl_complex proc~identify_crack5:->lcl_complex: num_elm num_elm proc~identify_crack5:->num_elm: mpi_wtime mpi_wtime proc~identify_crack5:->mpi_wtime: mpi_reduce mpi_reduce proc~identify_crack5:->mpi_reduce: vecassemblybegin vecassemblybegin proc~identify_crack5:->vecassemblybegin: mpi_barrier mpi_barrier proc~syncwrite_log:->mpi_barrier: proc~syncwrite_log_time:->mpi_wtime: proc~rootwrite_log rootwrite_log proc~syncwrite_log_time:->proc~rootwrite_log: proc~syncwrite_log_time:->mpi_barrier: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~identify_crack5~~CalledByGraph proc~identify_crack5 identify_crack5 program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~identify_crack5: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables msg i junk max_indx status area flx glb_max_flx max_flx Variables Type Visibility Attributes Name Initial character(len=slen), public :: msg integer, public :: i loop and temporary indicies integer, public :: junk loop and temporary indicies integer, public :: max_indx (2) loop and temporary indicies integer, public :: status (MPI_STATUS_SIZE) size of MPI comm buffer real(kind=iwp), public :: area real(kind=iwp), public :: flx real(kind=iwp), public :: glb_max_flx (3) real(kind=iwp), public :: max_flx (3)","tags":"","loc":"proc/identify_crack5.html","title":"identify_crack5 – ParaGEMS"},{"text":"public subroutine identify_crack6(iter, exit_cond) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iter logical, intent(out) :: exit_cond Calls proc~~identify_crack6~~CallsGraph proc~identify_crack6 identify_crack6 proc~syncwrite_log syncwrite_log proc~identify_crack6:->proc~syncwrite_log: lcl_complex lcl_complex proc~identify_crack6:->lcl_complex: vecsetvalues vecsetvalues proc~identify_crack6:->vecsetvalues: proc~syncwrite_log_time syncwrite_log_time proc~identify_crack6:->proc~syncwrite_log_time: mpi_bcast mpi_bcast proc~identify_crack6:->mpi_bcast: asub asub proc~identify_crack6:->asub: vecassemblyend vecassemblyend proc~identify_crack6:->vecassemblyend: mpi_allreduce mpi_allreduce proc~identify_crack6:->mpi_allreduce: num_elm num_elm proc~identify_crack6:->num_elm: mpi_wtime mpi_wtime proc~identify_crack6:->mpi_wtime: mpi_reduce mpi_reduce proc~identify_crack6:->mpi_reduce: vecassemblybegin vecassemblybegin proc~identify_crack6:->vecassemblybegin: glb_num_elm glb_num_elm proc~identify_crack6:->glb_num_elm: mpi_barrier mpi_barrier proc~syncwrite_log:->mpi_barrier: proc~syncwrite_log_time:->mpi_wtime: proc~rootwrite_log rootwrite_log proc~syncwrite_log_time:->proc~rootwrite_log: proc~syncwrite_log_time:->mpi_barrier: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~identify_crack6~~CalledByGraph proc~identify_crack6 identify_crack6 program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~identify_crack6: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables msg i indx j junk status search area rnd Variables Type Visibility Attributes Name Initial character(len=slen), public :: msg integer, public :: i loop and temporary indicies integer, public :: indx loop and temporary indicies integer, public :: j loop and temporary indicies integer, public :: junk loop and temporary indicies integer, public :: status (MPI_STATUS_SIZE) size of MPI comm buffer logical, public :: search real(kind=iwp), public :: area real(kind=iwp), public :: rnd","tags":"","loc":"proc/identify_crack6.html","title":"identify_crack6 – ParaGEMS"},{"text":"public subroutine identify_crack7(iter, exit_cond) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iter logical, intent(out) :: exit_cond Calls proc~~identify_crack7~~CallsGraph proc~identify_crack7 identify_crack7 proc~syncwrite_log syncwrite_log proc~identify_crack7:->proc~syncwrite_log: vecassemblyend vecassemblyend proc~identify_crack7:->vecassemblyend: proc~syncwrite_log_time syncwrite_log_time proc~identify_crack7:->proc~syncwrite_log_time: asub asub proc~identify_crack7:->asub: lcl_complex lcl_complex proc~identify_crack7:->lcl_complex: num_elm num_elm proc~identify_crack7:->num_elm: mpi_wtime mpi_wtime proc~identify_crack7:->mpi_wtime: mpi_reduce mpi_reduce proc~identify_crack7:->mpi_reduce: vecassemblybegin vecassemblybegin proc~identify_crack7:->vecassemblybegin: mpi_barrier mpi_barrier proc~syncwrite_log:->mpi_barrier: proc~syncwrite_log_time:->mpi_wtime: proc~rootwrite_log rootwrite_log proc~syncwrite_log_time:->proc~rootwrite_log: proc~syncwrite_log_time:->mpi_barrier: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~identify_crack7~~CalledByGraph proc~identify_crack7 identify_crack7 program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~identify_crack7: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables msg i junk max_indx status area flx glb_max_flx max_flx Variables Type Visibility Attributes Name Initial character(len=slen), public :: msg integer, public :: i loop and temporary indicies integer, public :: junk loop and temporary indicies integer, public :: max_indx (2) loop and temporary indicies integer, public :: status (MPI_STATUS_SIZE) size of MPI comm buffer real(kind=iwp), public :: area real(kind=iwp), public :: flx real(kind=iwp), public :: glb_max_flx (3) real(kind=iwp), public :: max_flx (3)","tags":"","loc":"proc/identify_crack7.html","title":"identify_crack7 – ParaGEMS"},{"text":"public subroutine initialise_darcy() initialise matrices and vectors for Darcy simulations Arguments None Calls proc~~initialise_darcy~~CallsGraph proc~initialise_darcy initialise_darcy proc~syncwrite_log syncwrite_log proc~initialise_darcy:->proc~syncwrite_log: matcreate matcreate proc~initialise_darcy:->matcreate: proc~syncwrite_log_time syncwrite_log_time proc~initialise_darcy:->proc~syncwrite_log_time: veccreatempi veccreatempi proc~initialise_darcy:->veccreatempi: vecsetfromoptions vecsetfromoptions proc~initialise_darcy:->vecsetfromoptions: lcl_complex lcl_complex proc~initialise_darcy:->lcl_complex: vecduplicate vecduplicate proc~initialise_darcy:->vecduplicate: vecdestroy vecdestroy proc~initialise_darcy:->vecdestroy: vecassemblyend vecassemblyend proc~initialise_darcy:->vecassemblyend: vecgetvalues vecgetvalues proc~initialise_darcy:->vecgetvalues: matsetsizes matsetsizes proc~initialise_darcy:->matsetsizes: vecgetownershiprange vecgetownershiprange proc~initialise_darcy:->vecgetownershiprange: num_elm num_elm proc~initialise_darcy:->num_elm: mpi_wtime mpi_wtime proc~initialise_darcy:->mpi_wtime: matsetfromoptions matsetfromoptions proc~initialise_darcy:->matsetfromoptions: vecassemblybegin vecassemblybegin proc~initialise_darcy:->vecassemblybegin: glb_num_elm glb_num_elm proc~initialise_darcy:->glb_num_elm: mpi_barrier mpi_barrier proc~syncwrite_log:->mpi_barrier: proc~syncwrite_log_time:->mpi_wtime: proc~rootwrite_log rootwrite_log proc~syncwrite_log_time:->proc~rootwrite_log: proc~syncwrite_log_time:->mpi_barrier: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~initialise_darcy~~CalledByGraph proc~initialise_darcy initialise_darcy program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~initialise_darcy: program~darcy_1f darcy_1f program~darcy_1f:->proc~initialise_darcy: program~darcy darcy program~darcy:->proc~initialise_darcy: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->proc~initialise_darcy: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables fname fib i id iib m offset o_nnz work Variables Type Visibility Attributes Name Initial character(len=slen), public :: fname file name integer, public :: fib loop index integer, public :: i loop index integer, public :: id index integer, public :: iib loop index integer, public :: m number of rows/columns in global solution variables integer, public :: offset loop index real(kind=iwp), public, ALLOCATABLE :: o_nnz (:) vec, public :: work PETSc work array","tags":"","loc":"proc/initialise_darcy.html","title":"initialise_darcy – ParaGEMS"},{"text":"public subroutine initialise_darcy2() initialise matrices and vectors for Darcy simulations Arguments None Calls proc~~initialise_darcy2~~CallsGraph proc~initialise_darcy2 initialise_darcy2 mpi_wtime mpi_wtime proc~initialise_darcy2:->mpi_wtime: proc~syncwrite_log syncwrite_log proc~initialise_darcy2:->proc~syncwrite_log: lcl_complex lcl_complex proc~initialise_darcy2:->lcl_complex: num_elm num_elm proc~initialise_darcy2:->num_elm: proc~syncwrite_log_time syncwrite_log_time proc~initialise_darcy2:->proc~syncwrite_log_time: mpi_barrier mpi_barrier proc~syncwrite_log:->mpi_barrier: proc~syncwrite_log_time:->mpi_wtime: proc~rootwrite_log rootwrite_log proc~syncwrite_log_time:->proc~rootwrite_log: proc~syncwrite_log_time:->mpi_barrier: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~initialise_darcy2~~CalledByGraph proc~initialise_darcy2 initialise_darcy2 program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~initialise_darcy2: program~darcy_2f darcy_2f program~darcy_2f:->proc~initialise_darcy2: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables m offset Variables Type Visibility Attributes Name Initial integer, public :: m number of rows/columns in global solution variables integer, public :: offset loop index","tags":"","loc":"proc/initialise_darcy2.html","title":"initialise_darcy2 – ParaGEMS"},{"text":"public subroutine read_input_darcy() Read input filefor user defined parameters, and perform checks to ensure\n reasonable values are set Arguments None Calls proc~~read_input_darcy~~CallsGraph proc~read_input_darcy read_input_darcy proc~end_mpi end_mpi proc~read_input_darcy:->proc~end_mpi: proc~syncwrite_log syncwrite_log proc~read_input_darcy:->proc~syncwrite_log: proc~rootwrite_log rootwrite_log proc~read_input_darcy:->proc~rootwrite_log: proc~check_param_darcy check_param_darcy proc~read_input_darcy:->proc~check_param_darcy: proc~end_mpi:->proc~syncwrite_log: mpi_finalize mpi_finalize proc~end_mpi:->mpi_finalize: proc~syncwrite_log_time syncwrite_log_time proc~end_mpi:->proc~syncwrite_log_time: proc~close_log close_log proc~end_mpi:->proc~close_log: mpi_barrier mpi_barrier proc~syncwrite_log:->mpi_barrier: proc~check_param_darcy:->proc~rootwrite_log: proc~syncwrite_log_time:->proc~rootwrite_log: proc~syncwrite_log_time:->mpi_barrier: mpi_wtime mpi_wtime proc~syncwrite_log_time:->mpi_wtime: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~read_input_darcy~~CalledByGraph proc~read_input_darcy read_input_darcy program~darcy_1f darcy_1f program~darcy_1f:->proc~read_input_darcy: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~read_input_darcy: program~darcy darcy program~darcy:->proc~read_input_darcy: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->proc~read_input_darcy: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~read_input_darcy: program~darcy_2f darcy_2f program~darcy_2f:->proc~read_input_darcy: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables msg buffer fname Variables Type Visibility Attributes Name Initial character(len=SLEN), public :: msg error msg string character(len=slen), public :: buffer buffer for command line arguments character(len=slen), public :: fname file name","tags":"","loc":"proc/read_input_darcy.html","title":"read_input_darcy – ParaGEMS"},{"text":"public subroutine test_vars(cnt) Arguments Type Intent Optional Attributes Name integer, intent(inout) :: cnt Contents None","tags":"","loc":"proc/test_vars~5.html","title":"test_vars – ParaGEMS"},{"text":"public subroutine clean_up() Deallocate global structures/variables Arguments None Called by proc~~clean_up~~CalledByGraph proc~clean_up clean_up program~darcy_1f darcy_1f program~darcy_1f:->proc~clean_up: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~clean_up: program~darcy darcy program~darcy:->proc~clean_up: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->proc~clean_up: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~clean_up: program~darcy_2f darcy_2f program~darcy_2f:->proc~clean_up: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents None","tags":"","loc":"proc/clean_up.html","title":"clean_up – ParaGEMS"},{"text":"public subroutine test_vars(cnt) deallocate global structures/variables Arguments Type Intent Optional Attributes Name integer, intent(inout) :: cnt test counter Called by proc~~test_vars~6~~CalledByGraph proc~test_vars~6 test_vars program~paragems_unit paragems_unit program~paragems_unit:->proc~test_vars~6: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents None","tags":"","loc":"proc/test_vars~6.html","title":"test_vars – ParaGEMS"},{"text":"public function calc_unsgnd_vlm(pts, n) Compute unsigned volume for given set of points Arguments Type Intent Optional Attributes Name real(kind=iwp), intent(in) :: pts (:,:) bounding points integer, intent(in) :: n # points to use Return Value real(kind=iwp) function value Calls proc~~calc_unsgnd_vlm~~CallsGraph proc~calc_unsgnd_vlm calc_unsgnd_vlm proc~determinant determinant proc~calc_unsgnd_vlm:->proc~determinant: proc~determinant:->proc~determinant: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~calc_unsgnd_vlm~~CalledByGraph proc~calc_unsgnd_vlm calc_unsgnd_vlm proc~calc_dual_vlm_i calc_dual_vlm_i proc~calc_dual_vlm_i:->proc~calc_unsgnd_vlm: proc~calc_dual_vlm calc_dual_vlm proc~calc_dual_vlm:->proc~calc_dual_vlm_i: proc~initialise_geo initialise_geo proc~initialise_geo:->proc~calc_dual_vlm: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables fac i A Variables Type Visibility Attributes Name Initial integer, public :: fac factorial integer, public :: i loop counters real(kind=iwp), public, ALLOCATABLE :: A (:,:) solution variable","tags":"","loc":"proc/calc_unsgnd_vlm.html","title":"calc_unsgnd_vlm – ParaGEMS"},{"text":"public subroutine add_points_i(pts, indx, k) Recursively adds points for dual volume calculation Arguments Type Intent Optional Attributes Name real(kind=iwp), intent(inout) :: pts (:,:) bounding points integer, intent(in) :: indx element index integer, intent(in) :: k simplicial order Calls proc~~add_points_i~~CallsGraph proc~add_points_i add_points_i lcl_complex lcl_complex proc~add_points_i:->lcl_complex: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~add_points_i~~CalledByGraph proc~add_points_i add_points_i proc~calc_dual_vlm calc_dual_vlm proc~calc_dual_vlm:->proc~add_points_i: proc~initialise_geo initialise_geo proc~initialise_geo:->proc~calc_dual_vlm: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables i n Variables Type Visibility Attributes Name Initial integer, public :: i loop counters integer, public :: n number of points","tags":"","loc":"proc/add_points_i.html","title":"add_points_i – ParaGEMS"},{"text":"public subroutine allocate_bndry_cobndry(k, cnt, bndry_cnt, cobndry_cnt, ext_cnt, surf_cnt) Allocate [co-]boundaries structures/variables Arguments Type Intent Optional Attributes Name integer, intent(in) :: k simplicial order integer, intent(in) :: cnt counter (k-1) element integer, intent(in) :: bndry_cnt (:) temp # local boundaries integer, intent(in) :: cobndry_cnt (:) temp # co-boundaries integer, intent(in) :: ext_cnt (2) counter external boundaries integer, intent(in) :: surf_cnt counter surface boundaries Calls proc~~allocate_bndry_cobndry~~CallsGraph proc~allocate_bndry_cobndry allocate_bndry_cobndry lcl_complex lcl_complex proc~allocate_bndry_cobndry:->lcl_complex: num_elm num_elm proc~allocate_bndry_cobndry:->num_elm: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~allocate_bndry_cobndry~~CalledByGraph proc~allocate_bndry_cobndry allocate_bndry_cobndry proc~calc_bndry_cobndry calc_bndry_cobndry proc~calc_bndry_cobndry:->proc~allocate_bndry_cobndry: proc~parallel_setup parallel_setup proc~parallel_setup:->proc~calc_bndry_cobndry: program~darcy_1f darcy_1f program~darcy_1f:->proc~parallel_setup: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~parallel_setup: program~darcy darcy program~darcy:->proc~parallel_setup: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->proc~parallel_setup: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~parallel_setup: program~darcy_2f darcy_2f program~darcy_2f:->proc~parallel_setup: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables i Variables Type Visibility Attributes Name Initial integer, public :: i counter","tags":"","loc":"proc/allocate_bndry_cobndry.html","title":"allocate_bndry_cobndry – ParaGEMS"},{"text":"public subroutine build_bndry_work_array(bndry, k) Build boundary data working array Arguments Type Intent Optional Attributes Name integer, intent(inout) :: bndry (:,:) boundary work array integer, intent(in) :: k simplicial order Calls proc~~build_bndry_work_array~~CallsGraph proc~build_bndry_work_array build_bndry_work_array lcl_complex lcl_complex proc~build_bndry_work_array:->lcl_complex: num_elm num_elm proc~build_bndry_work_array:->num_elm: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~build_bndry_work_array~~CalledByGraph proc~build_bndry_work_array build_bndry_work_array proc~calc_bndry_cobndry calc_bndry_cobndry proc~calc_bndry_cobndry:->proc~build_bndry_work_array: proc~parallel_setup parallel_setup proc~parallel_setup:->proc~calc_bndry_cobndry: program~darcy_1f darcy_1f program~darcy_1f:->proc~parallel_setup: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~parallel_setup: program~darcy darcy program~darcy:->proc~parallel_setup: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->proc~parallel_setup: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~parallel_setup: program~darcy_2f darcy_2f program~darcy_2f:->proc~parallel_setup: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables fib i iib j Variables Type Visibility Attributes Name Initial integer, public :: fib array initial/final index integer, public :: i loop index integer, public :: iib array initial/final index integer, public :: j loop index","tags":"","loc":"proc/build_bndry_work_array.html","title":"build_bndry_work_array – ParaGEMS"},{"text":"public subroutine calc_barycentric_grad(pts, n) Compute barycentric gradients Arguments Type Intent Optional Attributes Name real(kind=iwp), intent(inout) :: pts (:,:) vertices of simplex integer, intent(in) :: n number of vertices Calls proc~~calc_barycentric_grad~~CallsGraph proc~calc_barycentric_grad calc_barycentric_grad proc~matinv3 matinv3 proc~calc_barycentric_grad:->proc~matinv3: proc~matinv2 matinv2 proc~calc_barycentric_grad:->proc~matinv2: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~calc_barycentric_grad~~CalledByGraph proc~calc_barycentric_grad calc_barycentric_grad proc~calc_whitney_c2_bc calc_whitney_C2_BC proc~calc_whitney_c2_bc:->proc~calc_barycentric_grad: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables i j Variables Type Visibility Attributes Name Initial integer, public :: i loop counters integer, public :: j loop counters","tags":"","loc":"proc/calc_barycentric_grad.html","title":"calc_barycentric_grad – ParaGEMS"},{"text":"public subroutine calc_bndry_cobndry() Recursively compute element (co-)boundaries from highest to lowest geometric order Arguments None Calls proc~~calc_bndry_cobndry~~CallsGraph proc~calc_bndry_cobndry calc_bndry_cobndry proc~syncwrite_log syncwrite_log proc~calc_bndry_cobndry:->proc~syncwrite_log: num_elm num_elm proc~calc_bndry_cobndry:->num_elm: proc~syncwrite_log_time syncwrite_log_time proc~calc_bndry_cobndry:->proc~syncwrite_log_time: proc~count_bndry_cobndry count_bndry_cobndry proc~calc_bndry_cobndry:->proc~count_bndry_cobndry: proc~allocate_bndry_cobndry allocate_bndry_cobndry proc~calc_bndry_cobndry:->proc~allocate_bndry_cobndry: proc~set_bndry_cobndry set_bndry_cobndry proc~calc_bndry_cobndry:->proc~set_bndry_cobndry: proc~build_bndry_work_array build_bndry_work_array proc~calc_bndry_cobndry:->proc~build_bndry_work_array: mpi_barrier mpi_barrier proc~syncwrite_log:->mpi_barrier: proc~rootwrite_log rootwrite_log proc~syncwrite_log_time:->proc~rootwrite_log: proc~syncwrite_log_time:->mpi_barrier: mpi_wtime mpi_wtime proc~syncwrite_log_time:->mpi_wtime: proc~count_bndry_cobndry:->num_elm: lcl_complex lcl_complex proc~count_bndry_cobndry:->lcl_complex: proc~any_element_in_list any_element_in_list proc~count_bndry_cobndry:->proc~any_element_in_list: proc~allocate_bndry_cobndry:->num_elm: proc~allocate_bndry_cobndry:->lcl_complex: proc~set_bndry_cobndry:->num_elm: proc~set_bndry_cobndry:->lcl_complex: proc~elm2proc elm2proc proc~set_bndry_cobndry:->proc~elm2proc: proc~build_bndry_work_array:->num_elm: proc~build_bndry_work_array:->lcl_complex: num_pelm_pp num_pelm_pp proc~elm2proc:->num_pelm_pp: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~calc_bndry_cobndry~~CalledByGraph proc~calc_bndry_cobndry calc_bndry_cobndry proc~parallel_setup parallel_setup proc~parallel_setup:->proc~calc_bndry_cobndry: program~darcy_1f darcy_1f program~darcy_1f:->proc~parallel_setup: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~parallel_setup: program~darcy darcy program~darcy:->proc~parallel_setup: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->proc~parallel_setup: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~parallel_setup: program~darcy_2f darcy_2f program~darcy_2f:->proc~parallel_setup: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables bndry bndry_cnt cnt cobndry_cnt ext_cnt k surf_cnt work lwork Variables Type Visibility Attributes Name Initial integer, public, ALLOCATABLE :: bndry (:,:) temp. boundary work array integer, public, ALLOCATABLE :: bndry_cnt (:) counter local boundaries integer, public :: cnt counter boundaries integer, public, ALLOCATABLE :: cobndry_cnt (:) counter co-boundaries integer, public :: ext_cnt (2) counter external boundaries integer, public :: k simplicial order integer, public :: surf_cnt counter surface boundaries integer, public, ALLOCATABLE :: work (:,:) work array for merge sort logical, public, ALLOCATABLE :: lwork (:) work array to determine locality","tags":"","loc":"proc/calc_bndry_cobndry.html","title":"calc_bndry_cobndry – ParaGEMS"},{"text":"public subroutine calc_circumcenters(k) Compute circumcenter of given elements Arguments Type Intent Optional Attributes Name integer :: k simplicial order Calls proc~~calc_circumcenters~~CallsGraph proc~calc_circumcenters calc_circumcenters lcl_complex lcl_complex proc~calc_circumcenters:->lcl_complex: num_elm num_elm proc~calc_circumcenters:->num_elm: dsysv dsysv proc~calc_circumcenters:->dsysv: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~calc_circumcenters~~CalledByGraph proc~calc_circumcenters calc_circumcenters proc~initialise_geo initialise_geo proc~initialise_geo:->proc~calc_circumcenters: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables i j kp A b ipiv pts work Variables Type Visibility Attributes Name Initial integer, public :: i loop indices integer, public :: j loop indices integer, public :: kp simplicial order plus one real(kind=iwp), public, ALLOCATABLE :: A (:,:) solution variables real(kind=iwp), public, ALLOCATABLE :: b (:) solution variables real(kind=iwp), public, ALLOCATABLE :: ipiv (:) work arrays real(kind=iwp), public, ALLOCATABLE :: pts (:,:) bounding points real(kind=iwp), public, ALLOCATABLE :: work (:) work arrays","tags":"","loc":"proc/calc_circumcenters.html","title":"calc_circumcenters – ParaGEMS"},{"text":"public subroutine calc_dual_dir() Compute the unit direction of dual edges Arguments None Calls proc~~calc_dual_dir~~CallsGraph proc~calc_dual_dir calc_dual_dir lcl_complex lcl_complex proc~calc_dual_dir:->lcl_complex: proc~exchange_dual_dir exchange_dual_dir proc~calc_dual_dir:->proc~exchange_dual_dir: num_elm num_elm proc~calc_dual_dir:->num_elm: proc~exchange_dual_dir:->lcl_complex: mpi_waitall mpi_waitall proc~exchange_dual_dir:->mpi_waitall: mpi_request_free mpi_request_free proc~exchange_dual_dir:->mpi_request_free: mpi_barrier mpi_barrier proc~exchange_dual_dir:->mpi_barrier: num_send num_send proc~exchange_dual_dir:->num_send: adj_proc adj_proc proc~exchange_dual_dir:->adj_proc: num_recv num_recv proc~exchange_dual_dir:->num_recv: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~calc_dual_dir~~CalledByGraph proc~calc_dual_dir calc_dual_dir proc~initialise_geo initialise_geo proc~initialise_geo:->proc~calc_dual_dir: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables i j Variables Type Visibility Attributes Name Initial integer, public :: i loop counters integer, public :: j loop counters","tags":"","loc":"proc/calc_dual_dir.html","title":"calc_dual_dir – ParaGEMS"},{"text":"public subroutine calc_dual_vlm() Compute volume for dual elements of all geometric order Arguments None Calls proc~~calc_dual_vlm~~CallsGraph proc~calc_dual_vlm calc_dual_vlm lcl_complex lcl_complex proc~calc_dual_vlm:->lcl_complex: proc~calc_dual_vlm_i calc_dual_vlm_i proc~calc_dual_vlm:->proc~calc_dual_vlm_i: num_elm num_elm proc~calc_dual_vlm:->num_elm: proc~add_points_i add_points_i proc~calc_dual_vlm:->proc~add_points_i: proc~calc_dual_vlm_i:->lcl_complex: proc~calc_unsgnd_vlm calc_unsgnd_vlm proc~calc_dual_vlm_i:->proc~calc_unsgnd_vlm: proc~add_points_i:->lcl_complex: proc~determinant determinant proc~calc_unsgnd_vlm:->proc~determinant: proc~determinant:->proc~determinant: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~calc_dual_vlm~~CalledByGraph proc~calc_dual_vlm calc_dual_vlm proc~initialise_geo initialise_geo proc~initialise_geo:->proc~calc_dual_vlm: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables i indx k pts sgn Variables Type Visibility Attributes Name Initial integer, public :: i loop counters integer, public :: indx loop counters integer, public :: k simplicial order real(kind=iwp), public, ALLOCATABLE :: pts (:,:) bounding points real(kind=iwp), public, ALLOCATABLE :: sgn (:) sign of volume elements","tags":"","loc":"proc/calc_dual_vlm.html","title":"calc_dual_vlm – ParaGEMS"},{"text":"public subroutine calc_dual_vlm_i(pts, sgn, indx, p_indx, k) Recursively add to dual volume calculation Arguments Type Intent Optional Attributes Name real(kind=iwp), intent(inout) :: pts (:,:) bounding points real(kind=iwp), intent(inout) :: sgn (:) sign of volume elements integer, intent(in) :: indx element and parent index integer, intent(in) :: p_indx element and parent index integer, intent(in) :: k simplicial order Calls proc~~calc_dual_vlm_i~~CallsGraph proc~calc_dual_vlm_i calc_dual_vlm_i lcl_complex lcl_complex proc~calc_dual_vlm_i:->lcl_complex: proc~calc_unsgnd_vlm calc_unsgnd_vlm proc~calc_dual_vlm_i:->proc~calc_unsgnd_vlm: proc~determinant determinant proc~calc_unsgnd_vlm:->proc~determinant: proc~determinant:->proc~determinant: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~calc_dual_vlm_i~~CalledByGraph proc~calc_dual_vlm_i calc_dual_vlm_i proc~calc_dual_vlm calc_dual_vlm proc~calc_dual_vlm:->proc~calc_dual_vlm_i: proc~initialise_geo initialise_geo proc~initialise_geo:->proc~calc_dual_vlm: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables i n t_indx Variables Type Visibility Attributes Name Initial integer, public :: i loop counters integer, public :: n number of points integer, public :: t_indx tmp index","tags":"","loc":"proc/calc_dual_vlm_i.html","title":"calc_dual_vlm_i – ParaGEMS"},{"text":"public subroutine calc_hodge_star(k) Compute hodge star and it's inverse from primal and dual volumes Arguments Type Intent Optional Attributes Name integer, intent(in) :: k geometric order Calls proc~~calc_hodge_star~~CallsGraph proc~calc_hodge_star calc_hodge_star lcl_complex lcl_complex proc~calc_hodge_star:->lcl_complex: num_elm num_elm proc~calc_hodge_star:->num_elm: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~calc_hodge_star~~CalledByGraph proc~calc_hodge_star calc_hodge_star proc~initialise_geo initialise_geo proc~initialise_geo:->proc~calc_hodge_star: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents None","tags":"","loc":"proc/calc_hodge_star.html","title":"calc_hodge_star – ParaGEMS"},{"text":"public subroutine calc_orientation(elm, orientation) Sort nodal indices and compute ± orientation of an element Arguments Type Intent Optional Attributes Name integer, intent(inout) :: elm (:) nodal indices of elements integer, intent(inout) :: orientation orientations of the elements Contents Variables i j length work Variables Type Visibility Attributes Name Initial integer, public :: i loop counters integer, public :: j loop counters integer, public :: length size of elm array integer, public :: work work variable","tags":"","loc":"proc/calc_orientation.html","title":"calc_orientation – ParaGEMS"},{"text":"public subroutine calc_prml_dir() Compute the unit direction of primal edges Arguments None Calls proc~~calc_prml_dir~~CallsGraph proc~calc_prml_dir calc_prml_dir lcl_complex lcl_complex proc~calc_prml_dir:->lcl_complex: num_elm num_elm proc~calc_prml_dir:->num_elm: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~calc_prml_dir~~CalledByGraph proc~calc_prml_dir calc_prml_dir proc~initialise_geo initialise_geo proc~initialise_geo:->proc~calc_prml_dir: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables i Variables Type Visibility Attributes Name Initial integer, public :: i loop counter","tags":"","loc":"proc/calc_prml_dir.html","title":"calc_prml_dir – ParaGEMS"},{"text":"public subroutine calc_prml_sgnd_vlm(k) Compute signed volume of primal elements Arguments Type Intent Optional Attributes Name integer :: k simplicial order Calls proc~~calc_prml_sgnd_vlm~~CallsGraph proc~calc_prml_sgnd_vlm calc_prml_sgnd_vlm lcl_complex lcl_complex proc~calc_prml_sgnd_vlm:->lcl_complex: num_elm num_elm proc~calc_prml_sgnd_vlm:->num_elm: proc~determinant determinant proc~calc_prml_sgnd_vlm:->proc~determinant: proc~determinant:->proc~determinant: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~calc_prml_sgnd_vlm~~CalledByGraph proc~calc_prml_sgnd_vlm calc_prml_sgnd_vlm proc~initialise_geo initialise_geo proc~initialise_geo:->proc~calc_prml_sgnd_vlm: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables fac i j A pts Variables Type Visibility Attributes Name Initial integer, public :: fac factorial integer, public :: i loop counters integer, public :: j loop counters real(kind=iwp), public, ALLOCATABLE :: A (:,:) solution variable real(kind=iwp), public, ALLOCATABLE :: pts (:) bounding points","tags":"","loc":"proc/calc_prml_sgnd_vlm.html","title":"calc_prml_sgnd_vlm – ParaGEMS"},{"text":"public subroutine calc_prml_unsgnd_vlm(k) Compute unsigned volume of primal elements Arguments Type Intent Optional Attributes Name integer :: k loop counters Calls proc~~calc_prml_unsgnd_vlm~~CallsGraph proc~calc_prml_unsgnd_vlm calc_prml_unsgnd_vlm lcl_complex lcl_complex proc~calc_prml_unsgnd_vlm:->lcl_complex: num_elm num_elm proc~calc_prml_unsgnd_vlm:->num_elm: proc~determinant determinant proc~calc_prml_unsgnd_vlm:->proc~determinant: proc~determinant:->proc~determinant: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~calc_prml_unsgnd_vlm~~CalledByGraph proc~calc_prml_unsgnd_vlm calc_prml_unsgnd_vlm proc~initialise_geo initialise_geo proc~initialise_geo:->proc~calc_prml_unsgnd_vlm: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables fac i j A pts Variables Type Visibility Attributes Name Initial integer, public :: fac factorial integer, public :: i loop counters integer, public :: j loop counters real(kind=iwp), public, ALLOCATABLE :: A (:,:) solution variable real(kind=iwp), public, ALLOCATABLE :: pts (:) bounding points","tags":"","loc":"proc/calc_prml_unsgnd_vlm.html","title":"calc_prml_unsgnd_vlm – ParaGEMS"},{"text":"public subroutine calc_whitney_C2_BC() Compute the Whitney interpolation of primal faces to primal volume barycentric centers Arguments None Calls proc~~calc_whitney_c2_bc~~CallsGraph proc~calc_whitney_c2_bc calc_whitney_C2_BC proc~cross_product cross_product proc~calc_whitney_c2_bc:->proc~cross_product: proc~calc_barycentric_grad calc_barycentric_grad proc~calc_whitney_c2_bc:->proc~calc_barycentric_grad: lcl_complex lcl_complex proc~calc_whitney_c2_bc:->lcl_complex: num_elm num_elm proc~calc_whitney_c2_bc:->num_elm: proc~matinv3 matinv3 proc~calc_barycentric_grad:->proc~matinv3: proc~matinv2 matinv2 proc~calc_barycentric_grad:->proc~matinv2: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables f_vrts i indx j k vrts not_found grads Variables Type Visibility Attributes Name Initial integer, public :: f_vrts (3) face vertex indices integer, public :: i loop indices integer, public :: indx face index integer, public :: j loop indices integer, public :: k loop indices integer, public :: vrts (4) volume vertex indices logical, public :: not_found is boundary face index found real(kind=iwp), public :: grads (4,3) barycentric gradients","tags":"","loc":"proc/calc_whitney_c2_bc.html","title":"calc_whitney_C2_BC – ParaGEMS"},{"text":"public subroutine count_bndry_cobndry(bndry, k, cnt, bndry_cnt, cobndry_cnt, ext_cnt, surf_cnt, local) Count [co-]boundaries: internal, external, surface Arguments Type Intent Optional Attributes Name integer, intent(in) :: bndry (:,:) boundary work array integer, intent(in) :: k simplicial order integer, intent(inout) :: cnt counter (k-1) element integer, intent(inout) :: bndry_cnt (:) counter local boundaries integer, intent(inout) :: cobndry_cnt (:) counter co-boundaries integer, intent(inout) :: ext_cnt (2) counter external boundaries integer, intent(inout) :: surf_cnt counter surface boundaries logical, intent(inout) :: local (:) locality work array Calls proc~~count_bndry_cobndry~~CallsGraph proc~count_bndry_cobndry count_bndry_cobndry lcl_complex lcl_complex proc~count_bndry_cobndry:->lcl_complex: proc~any_element_in_list any_element_in_list proc~count_bndry_cobndry:->proc~any_element_in_list: num_elm num_elm proc~count_bndry_cobndry:->num_elm: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~count_bndry_cobndry~~CalledByGraph proc~count_bndry_cobndry count_bndry_cobndry proc~calc_bndry_cobndry calc_bndry_cobndry proc~calc_bndry_cobndry:->proc~count_bndry_cobndry: proc~parallel_setup parallel_setup proc~parallel_setup:->proc~calc_bndry_cobndry: program~darcy_1f darcy_1f program~darcy_1f:->proc~parallel_setup: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~parallel_setup: program~darcy darcy program~darcy:->proc~parallel_setup: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->proc~parallel_setup: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~parallel_setup: program~darcy_2f darcy_2f program~darcy_2f:->proc~parallel_setup: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables i Variables Type Visibility Attributes Name Initial integer, public :: i counter","tags":"","loc":"proc/count_bndry_cobndry.html","title":"count_bndry_cobndry – ParaGEMS"},{"text":"public subroutine exchange_dual_dir(k) Exchange external dual edge directions between adjacent processes Arguments Type Intent Optional Attributes Name integer, intent(in) :: k simplicial order Calls proc~~exchange_dual_dir~~CallsGraph proc~exchange_dual_dir exchange_dual_dir lcl_complex lcl_complex proc~exchange_dual_dir:->lcl_complex: mpi_waitall mpi_waitall proc~exchange_dual_dir:->mpi_waitall: mpi_request_free mpi_request_free proc~exchange_dual_dir:->mpi_request_free: mpi_barrier mpi_barrier proc~exchange_dual_dir:->mpi_barrier: num_send num_send proc~exchange_dual_dir:->num_send: adj_proc adj_proc proc~exchange_dual_dir:->adj_proc: num_recv num_recv proc~exchange_dual_dir:->num_recv: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~exchange_dual_dir~~CalledByGraph proc~exchange_dual_dir exchange_dual_dir proc~calc_dual_dir calc_dual_dir proc~calc_dual_dir:->proc~exchange_dual_dir: proc~initialise_geo initialise_geo proc~initialise_geo:->proc~calc_dual_dir: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables buffer_size cnt i junk kp req status rbuffer sbuffer Variables Type Visibility Attributes Name Initial integer, public :: buffer_size size of comm buffer integer, public :: cnt counter integer, public :: i loop index integer, public :: junk junk comm variable integer, public :: kp simplicial order integer, public, ALLOCATABLE :: req (:) request variable for non-blocking comms integer, public, ALLOCATABLE :: status (:,:) size of MPI comm buffer real(kind=iwp), public, ALLOCATABLE :: rbuffer (:) receive buffer real(kind=iwp), public, ALLOCATABLE :: sbuffer (:) send buffer","tags":"","loc":"proc/exchange_dual_dir.html","title":"exchange_dual_dir – ParaGEMS"},{"text":"public subroutine exchange_dual_vlm(k) Exchange external dual volumes between adjacent processes Arguments Type Intent Optional Attributes Name integer, intent(in) :: k simplicial order Calls proc~~exchange_dual_vlm~~CallsGraph proc~exchange_dual_vlm exchange_dual_vlm lcl_complex lcl_complex proc~exchange_dual_vlm:->lcl_complex: mpi_waitall mpi_waitall proc~exchange_dual_vlm:->mpi_waitall: mpi_request_free mpi_request_free proc~exchange_dual_vlm:->mpi_request_free: mpi_barrier mpi_barrier proc~exchange_dual_vlm:->mpi_barrier: num_send num_send proc~exchange_dual_vlm:->num_send: adj_proc adj_proc proc~exchange_dual_vlm:->adj_proc: num_recv num_recv proc~exchange_dual_vlm:->num_recv: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~exchange_dual_vlm~~CalledByGraph proc~exchange_dual_vlm exchange_dual_vlm proc~initialise_geo initialise_geo proc~initialise_geo:->proc~exchange_dual_vlm: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables buffer_size cnt i junk kp req status rbuffer sbuffer Variables Type Visibility Attributes Name Initial integer, public :: buffer_size size of comm buffer integer, public :: cnt counter integer, public :: i loop index integer, public :: junk junk comm variable integer, public :: kp simplicial order plus one integer, public, ALLOCATABLE :: req (:) request variable for non-blocking comms integer, public, ALLOCATABLE :: status (:,:) size of MPI comm buffer real(kind=iwp), public, ALLOCATABLE :: rbuffer (:) receive buffer real(kind=iwp), public, ALLOCATABLE :: sbuffer (:) send buffer","tags":"","loc":"proc/exchange_dual_vlm.html","title":"exchange_dual_vlm – ParaGEMS"},{"text":"public subroutine get_lcl_node_indx() Create map between global and local node indices Arguments None Calls proc~~get_lcl_node_indx~~CallsGraph proc~get_lcl_node_indx get_lcl_node_indx lcl_complex lcl_complex proc~get_lcl_node_indx:->lcl_complex: num_elm num_elm proc~get_lcl_node_indx:->num_elm: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~get_lcl_node_indx~~CalledByGraph proc~get_lcl_node_indx get_lcl_node_indx proc~initialise_geo initialise_geo proc~initialise_geo:->proc~get_lcl_node_indx: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables i k m n Variables Type Visibility Attributes Name Initial integer, public :: i loop counters integer, public :: k loop counters integer, public :: m loop counters integer, public :: n loop counters","tags":"","loc":"proc/get_lcl_node_indx.html","title":"get_lcl_node_indx – ParaGEMS"},{"text":"public subroutine initialise_geo() Initialise geometric quantities for the given mesh Arguments None Calls proc~~initialise_geo~~CallsGraph proc~initialise_geo initialise_geo proc~calc_hodge_star calc_hodge_star proc~initialise_geo:->proc~calc_hodge_star: proc~syncwrite_log syncwrite_log proc~initialise_geo:->proc~syncwrite_log: proc~rootwrite_log rootwrite_log proc~initialise_geo:->proc~rootwrite_log: proc~calc_prml_sgnd_vlm calc_prml_sgnd_vlm proc~initialise_geo:->proc~calc_prml_sgnd_vlm: proc~calc_dual_vlm calc_dual_vlm proc~initialise_geo:->proc~calc_dual_vlm: proc~get_lcl_node_indx get_lcl_node_indx proc~initialise_geo:->proc~get_lcl_node_indx: num_elm num_elm proc~initialise_geo:->num_elm: lcl_complex lcl_complex proc~initialise_geo:->lcl_complex: proc~syncwrite_log_time syncwrite_log_time proc~initialise_geo:->proc~syncwrite_log_time: proc~calc_prml_dir calc_prml_dir proc~initialise_geo:->proc~calc_prml_dir: proc~calc_circumcenters calc_circumcenters proc~initialise_geo:->proc~calc_circumcenters: glb_num_elm glb_num_elm proc~initialise_geo:->glb_num_elm: proc~exchange_dual_vlm exchange_dual_vlm proc~initialise_geo:->proc~exchange_dual_vlm: mpi_wtime mpi_wtime proc~initialise_geo:->mpi_wtime: proc~calc_dual_dir calc_dual_dir proc~initialise_geo:->proc~calc_dual_dir: proc~calc_prml_unsgnd_vlm calc_prml_unsgnd_vlm proc~initialise_geo:->proc~calc_prml_unsgnd_vlm: proc~calc_hodge_star:->num_elm: proc~calc_hodge_star:->lcl_complex: mpi_barrier mpi_barrier proc~syncwrite_log:->mpi_barrier: proc~calc_prml_sgnd_vlm:->num_elm: proc~calc_prml_sgnd_vlm:->lcl_complex: proc~determinant determinant proc~calc_prml_sgnd_vlm:->proc~determinant: proc~calc_dual_vlm:->num_elm: proc~calc_dual_vlm:->lcl_complex: proc~calc_dual_vlm_i calc_dual_vlm_i proc~calc_dual_vlm:->proc~calc_dual_vlm_i: proc~add_points_i add_points_i proc~calc_dual_vlm:->proc~add_points_i: proc~get_lcl_node_indx:->num_elm: proc~get_lcl_node_indx:->lcl_complex: proc~syncwrite_log_time:->proc~rootwrite_log: proc~syncwrite_log_time:->mpi_wtime: proc~syncwrite_log_time:->mpi_barrier: proc~calc_prml_dir:->num_elm: proc~calc_prml_dir:->lcl_complex: proc~calc_circumcenters:->num_elm: proc~calc_circumcenters:->lcl_complex: dsysv dsysv proc~calc_circumcenters:->dsysv: proc~exchange_dual_vlm:->lcl_complex: adj_proc adj_proc proc~exchange_dual_vlm:->adj_proc: mpi_waitall mpi_waitall proc~exchange_dual_vlm:->mpi_waitall: mpi_request_free mpi_request_free proc~exchange_dual_vlm:->mpi_request_free: num_send num_send proc~exchange_dual_vlm:->num_send: proc~exchange_dual_vlm:->mpi_barrier: num_recv num_recv proc~exchange_dual_vlm:->num_recv: proc~calc_dual_dir:->num_elm: proc~calc_dual_dir:->lcl_complex: proc~exchange_dual_dir exchange_dual_dir proc~calc_dual_dir:->proc~exchange_dual_dir: proc~calc_prml_unsgnd_vlm:->num_elm: proc~calc_prml_unsgnd_vlm:->lcl_complex: proc~calc_prml_unsgnd_vlm:->proc~determinant: proc~determinant:->proc~determinant: proc~calc_dual_vlm_i:->lcl_complex: proc~calc_unsgnd_vlm calc_unsgnd_vlm proc~calc_dual_vlm_i:->proc~calc_unsgnd_vlm: proc~add_points_i:->lcl_complex: proc~exchange_dual_dir:->lcl_complex: proc~exchange_dual_dir:->adj_proc: proc~exchange_dual_dir:->mpi_waitall: proc~exchange_dual_dir:->mpi_request_free: proc~exchange_dual_dir:->num_send: proc~exchange_dual_dir:->mpi_barrier: proc~exchange_dual_dir:->num_recv: proc~calc_unsgnd_vlm:->proc~determinant: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables str i k absminV maxV minV Variables Type Visibility Attributes Name Initial character(len=slen), public :: str string for writing to log file integer, public :: i loop counters integer, public :: k loop counters real(kind=iwp), public :: absminV min/max volumes real(kind=iwp), public :: maxV min/max volumes real(kind=iwp), public :: minV min/max volumes","tags":"","loc":"proc/initialise_geo.html","title":"initialise_geo – ParaGEMS"},{"text":"public subroutine set_bndry_cobndry(bndry, k, cnt, bndry_cnt, cobndry_cnt, ext_cnt, surf_cnt, local) Set [co-]boundaries: internal, external, surface Arguments Type Intent Optional Attributes Name integer, intent(inout) :: bndry (:,:) boundary work array integer, intent(in) :: k simplicial order integer, intent(inout) :: cnt counter (k-1) element integer, intent(inout) :: bndry_cnt (:) temp # local boundaries integer, intent(inout) :: cobndry_cnt (:) temp # co-boundaries integer, intent(inout) :: ext_cnt (2) counter external boundaries integer, intent(inout) :: surf_cnt counter surface boundaries logical, intent(inout) :: local (:) locality work array Calls proc~~set_bndry_cobndry~~CallsGraph proc~set_bndry_cobndry set_bndry_cobndry lcl_complex lcl_complex proc~set_bndry_cobndry:->lcl_complex: proc~elm2proc elm2proc proc~set_bndry_cobndry:->proc~elm2proc: num_elm num_elm proc~set_bndry_cobndry:->num_elm: num_pelm_pp num_pelm_pp proc~elm2proc:->num_pelm_pp: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~set_bndry_cobndry~~CalledByGraph proc~set_bndry_cobndry set_bndry_cobndry proc~calc_bndry_cobndry calc_bndry_cobndry proc~calc_bndry_cobndry:->proc~set_bndry_cobndry: proc~parallel_setup parallel_setup proc~parallel_setup:->proc~calc_bndry_cobndry: program~darcy_1f darcy_1f program~darcy_1f:->proc~parallel_setup: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->proc~parallel_setup: program~darcy darcy program~darcy:->proc~parallel_setup: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->proc~parallel_setup: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->proc~parallel_setup: program~darcy_2f darcy_2f program~darcy_2f:->proc~parallel_setup: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables i Variables Type Visibility Attributes Name Initial integer, public :: i counter","tags":"","loc":"proc/set_bndry_cobndry.html","title":"set_bndry_cobndry – ParaGEMS"},{"text":"public subroutine test_vars(cnt) Arguments Type Intent Optional Attributes Name integer, intent(inout) :: cnt Contents None","tags":"","loc":"proc/test_vars~7.html","title":"test_vars – ParaGEMS"},{"text":"Module contains routines for partitioning parallel workload Uses common_mod mpi_mod io_mod math_mod dec_mod module~~partition_mod~~UsesGraph module~partition_mod partition_mod module~dec_mod dec_mod module~partition_mod:->module~dec_mod: module~io_mod io_mod module~partition_mod:->module~io_mod: module~mpi_mod mpi_mod module~partition_mod:->module~mpi_mod: module~math_mod math_mod module~partition_mod:->module~math_mod: module~common_mod common_mod module~partition_mod:->module~common_mod: module~dec_mod:->module~mpi_mod: module~dec_mod:->module~math_mod: module~dec_mod:->module~common_mod: module~io_mod:->module~dec_mod: module~io_mod:->module~mpi_mod: module~io_mod:->module~common_mod: iso_fortran_env iso_fortran_env module~io_mod:->iso_fortran_env: ieee_arithmetic ieee_arithmetic module~io_mod:->ieee_arithmetic: module~solver_mod solver_mod module~io_mod:->module~solver_mod: module~mpi_mod:->module~common_mod: module~math_mod:->module~mpi_mod: module~math_mod:->module~common_mod: petsc petsc module~common_mod:->petsc: module~solver_mod:->module~mpi_mod: module~solver_mod:->module~common_mod: module~solver_mod:->iso_fortran_env: module~solver_mod:->ieee_arithmetic: Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Used by module~~partition_mod~~UsedByGraph module~partition_mod partition_mod program~darcy_1f darcy_1f program~darcy_1f:->module~partition_mod: module~test_partition_mod test_partition_mod module~test_partition_mod:->module~partition_mod: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->module~partition_mod: program~darcy darcy program~darcy:->module~partition_mod: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->module~partition_mod: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->module~partition_mod: program~darcy_2f darcy_2f program~darcy_2f:->module~partition_mod: Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Subroutines exchange_glb_indx exchange_prml_nodes get_connectivity get_glb_indx get_glb_indx_dual_vlm parallel_setup partition_dual Subroutines public subroutine exchange_glb_indx (k) Author Pieter Boom Date 2020/09/16 Exchange global indices to adjacent process (ghost nodes) Arguments Type Intent Optional Attributes Name integer, intent(in) :: k simplicial order public subroutine exchange_prml_nodes () Author Pieter Boom Date 2020/09/16 Exchange nodal locations to adjacent process (ghost nodes) Arguments None public subroutine get_connectivity () Author Pieter Boom Date 2020/09/16 Determine cross-process entity connections for all geometric orders Arguments None public subroutine get_glb_indx () Author Pieter Boom Date 2020/09/16 Get global entity indices from file Arguments None public subroutine get_glb_indx_dual_vlm () Author Pieter Boom Date 2020/09/16 Set global index of nodes from predetermined node index Arguments None public subroutine parallel_setup () Author Pieter Boom Date 2020/09/16 Sets up partitioning, read mesh and nodal locations, and sets connectivity Arguments None public subroutine partition_dual () Author Pieter Boom Date 2020/09/16 Partitions dual volumes across available processes\n Assumption: Mesh is a simplicial complex in TetGen format Arguments None","tags":"","loc":"module/partition_mod.html","title":"partition_mod – ParaGEMS"},{"text":"Uses common_mod mpi_mod module~~test_mpi~~UsesGraph module~test_mpi test_mpi module~common_mod common_mod module~test_mpi:->module~common_mod: module~mpi_mod mpi_mod module~test_mpi:->module~mpi_mod: petsc petsc module~common_mod:->petsc: module~mpi_mod:->module~common_mod: Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Subroutines t_end_mpi t_parallel_setup t_partition t_start_mpi Subroutines public subroutine t_end_mpi () Arguments None public subroutine t_parallel_setup () Arguments None public subroutine t_partition () Arguments None public subroutine t_start_mpi () Arguments None","tags":"","loc":"module/test_mpi.html","title":"test_mpi – ParaGEMS"},{"text":"Uses partition_mod module~~test_partition_mod~~UsesGraph module~test_partition_mod test_partition_mod module~partition_mod partition_mod module~test_partition_mod:->module~partition_mod: module~dec_mod dec_mod module~partition_mod:->module~dec_mod: module~io_mod io_mod module~partition_mod:->module~io_mod: module~mpi_mod mpi_mod module~partition_mod:->module~mpi_mod: module~math_mod math_mod module~partition_mod:->module~math_mod: module~common_mod common_mod module~partition_mod:->module~common_mod: module~dec_mod:->module~mpi_mod: module~dec_mod:->module~math_mod: module~dec_mod:->module~common_mod: module~io_mod:->module~dec_mod: module~io_mod:->module~mpi_mod: module~io_mod:->module~common_mod: iso_fortran_env iso_fortran_env module~io_mod:->iso_fortran_env: ieee_arithmetic ieee_arithmetic module~io_mod:->ieee_arithmetic: module~solver_mod solver_mod module~io_mod:->module~solver_mod: module~mpi_mod:->module~common_mod: module~math_mod:->module~mpi_mod: module~math_mod:->module~common_mod: petsc petsc module~common_mod:->petsc: module~solver_mod:->module~mpi_mod: module~solver_mod:->module~common_mod: module~solver_mod:->iso_fortran_env: module~solver_mod:->ieee_arithmetic: Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Subroutines test_vars Subroutines public subroutine test_vars (cnt) Arguments Type Intent Optional Attributes Name integer, intent(inout) :: cnt","tags":"","loc":"module/test_partition_mod.html","title":"test_partition_mod – ParaGEMS"},{"text":"Module contains routines for controlling MPI execution environment Uses common_mod module~~mpi_mod~~UsesGraph module~mpi_mod mpi_mod module~common_mod common_mod module~mpi_mod:->module~common_mod: petsc petsc module~common_mod:->petsc: Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Used by module~~mpi_mod~~UsedByGraph module~mpi_mod mpi_mod program~darcy_1f darcy_1f program~darcy_1f:->module~mpi_mod: module~darcy_mod darcy_mod program~darcy_1f:->module~darcy_mod: module~partition_mod partition_mod program~darcy_1f:->module~partition_mod: module~solver_mod solver_mod program~darcy_1f:->module~solver_mod: module~dec_mod dec_mod module~dec_mod:->module~mpi_mod: module~math_mod math_mod module~dec_mod:->module~math_mod: module~math_mod:->module~mpi_mod: module~darcy_mod:->module~mpi_mod: module~io_mod io_mod module~darcy_mod:->module~io_mod: module~darcy_mod:->module~solver_mod: module~io_mod:->module~mpi_mod: module~io_mod:->module~dec_mod: module~io_mod:->module~solver_mod: program~darcy_2f darcy_2f program~darcy_2f:->module~mpi_mod: program~darcy_2f:->module~darcy_mod: program~darcy_2f:->module~partition_mod: program~darcy_2f:->module~solver_mod: program~darcy darcy program~darcy:->module~mpi_mod: program~darcy:->module~darcy_mod: program~darcy:->module~partition_mod: program~darcy:->module~solver_mod: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->module~mpi_mod: program~darcy_crkp_1f:->module~darcy_mod: program~darcy_crkp_1f:->module~partition_mod: program~darcy_crkp_1f:->module~solver_mod: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->module~mpi_mod: program~darcy_crkp_2f~2:->module~darcy_mod: program~darcy_crkp_2f~2:->module~partition_mod: program~darcy_crkp_2f~2:->module~solver_mod: module~test_mpi test_mpi module~test_mpi:->module~mpi_mod: module~partition_mod:->module~mpi_mod: module~partition_mod:->module~dec_mod: module~partition_mod:->module~math_mod: module~partition_mod:->module~io_mod: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->module~mpi_mod: program~darcy_crkp_2f:->module~darcy_mod: program~darcy_crkp_2f:->module~partition_mod: program~darcy_crkp_2f:->module~solver_mod: module~solver_mod:->module~mpi_mod: module~test_dec_mod test_dec_mod module~test_dec_mod:->module~dec_mod: module~test_darcy_mod test_darcy_mod module~test_darcy_mod:->module~darcy_mod: module~test_io_mod test_io_mod module~test_io_mod:->module~io_mod: module~test_partition_mod test_partition_mod module~test_partition_mod:->module~partition_mod: module~test_math_mod test_math_mod module~test_math_mod:->module~math_mod: module~test_solver_mod test_solver_mod module~test_solver_mod:->module~solver_mod: Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Functions elm2proc Subroutines close_log close_unsteady_log end_mpi open_log open_unsteady_log rootwrite_log start_mpi syncwrite_log syncwrite_log_mpidata syncwrite_log_time write_log Functions public function elm2proc (id) Author Pieter Boom Date 2020/09/16 Convert partitioning element to process number (rank) Arguments Type Intent Optional Attributes Name integer, intent(in) :: id Return Value integer Subroutines public subroutine close_log () Author Pieter Boom Date 2020/09/16 Close log file from root process Arguments None public subroutine close_unsteady_log () Author Pieter Boom Date 2020/09/16 Close log file for unsteady simulations from root Arguments None public subroutine end_mpi () Author Pieter Boom Date 2020/09/16 End MPI execution environment & stop ParaGEMS Arguments None public subroutine open_log () Author Pieter Boom Date 2020/09/16 Open log file from root process and write ParaGEMS header Arguments None public subroutine open_unsteady_log () Author Pieter Boom Date 2020/09/16 Open log file for unsteady simulations from root process and write header Arguments None public subroutine rootwrite_log (msg) Author Pieter Boom Date 2020/09/16 Write unsynchronised message from root process to log file Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: msg message string public subroutine start_mpi () Author Pieter Boom Date 2020/09/16 Start MPI execution environment & get details Arguments None public subroutine syncwrite_log (msg) Author Pieter Boom Date 2020/09/16 Write uynchronised message from root process to log file Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: msg message string public subroutine syncwrite_log_mpidata () Author Pieter Boom Date 2020/09/16 Write a synchronised message with number of mpi ranks to log file Arguments None public subroutine syncwrite_log_time () Author Pieter Boom Date 2020/09/16 Write a synchronised message to log file with timings Arguments None public subroutine write_log (msg) Author Pieter Boom Date 2020/09/16 Write unsynchronised messages collated from processes to log file Arguments Type Intent Optional Attributes Name character(len=slen), intent(in) :: msg message string","tags":"","loc":"module/mpi_mod.html","title":"mpi_mod – ParaGEMS"},{"text":"Uses io_mod module~~test_io_mod~~UsesGraph module~test_io_mod test_io_mod module~io_mod io_mod module~test_io_mod:->module~io_mod: module~common_mod common_mod module~io_mod:->module~common_mod: iso_fortran_env iso_fortran_env module~io_mod:->iso_fortran_env: ieee_arithmetic ieee_arithmetic module~io_mod:->ieee_arithmetic: module~mpi_mod mpi_mod module~io_mod:->module~mpi_mod: module~dec_mod dec_mod module~io_mod:->module~dec_mod: module~solver_mod solver_mod module~io_mod:->module~solver_mod: petsc petsc module~common_mod:->petsc: module~mpi_mod:->module~common_mod: module~dec_mod:->module~common_mod: module~dec_mod:->module~mpi_mod: module~math_mod math_mod module~dec_mod:->module~math_mod: module~solver_mod:->module~common_mod: module~solver_mod:->iso_fortran_env: module~solver_mod:->ieee_arithmetic: module~solver_mod:->module~mpi_mod: module~math_mod:->module~common_mod: module~math_mod:->module~mpi_mod: Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Subroutines test_vars Subroutines public subroutine test_vars (cnt) Arguments Type Intent Optional Attributes Name integer, intent(inout) :: cnt","tags":"","loc":"module/test_io_mod.html","title":"test_io_mod – ParaGEMS"},{"text":"Module contains subroutines for reading from and writing to disk Uses common_mod mpi_mod dec_mod solver_mod ieee_arithmetic iso_fortran_env module~~io_mod~~UsesGraph module~io_mod io_mod module~common_mod common_mod module~io_mod:->module~common_mod: iso_fortran_env iso_fortran_env module~io_mod:->iso_fortran_env: ieee_arithmetic ieee_arithmetic module~io_mod:->ieee_arithmetic: module~mpi_mod mpi_mod module~io_mod:->module~mpi_mod: module~dec_mod dec_mod module~io_mod:->module~dec_mod: module~solver_mod solver_mod module~io_mod:->module~solver_mod: petsc petsc module~common_mod:->petsc: module~mpi_mod:->module~common_mod: module~dec_mod:->module~common_mod: module~dec_mod:->module~mpi_mod: module~math_mod math_mod module~dec_mod:->module~math_mod: module~solver_mod:->module~common_mod: module~solver_mod:->iso_fortran_env: module~solver_mod:->ieee_arithmetic: module~solver_mod:->module~mpi_mod: module~math_mod:->module~common_mod: module~math_mod:->module~mpi_mod: Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Used by module~~io_mod~~UsedByGraph module~io_mod io_mod module~test_io_mod test_io_mod module~test_io_mod:->module~io_mod: module~partition_mod partition_mod module~partition_mod:->module~io_mod: module~darcy_mod darcy_mod module~darcy_mod:->module~io_mod: program~darcy_1f darcy_1f program~darcy_1f:->module~partition_mod: program~darcy_1f:->module~darcy_mod: module~test_darcy_mod test_darcy_mod module~test_darcy_mod:->module~darcy_mod: module~test_partition_mod test_partition_mod module~test_partition_mod:->module~partition_mod: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->module~partition_mod: program~darcy_crkp_2f:->module~darcy_mod: program~darcy darcy program~darcy:->module~partition_mod: program~darcy:->module~darcy_mod: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->module~partition_mod: program~darcy_crkp_1f:->module~darcy_mod: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->module~partition_mod: program~darcy_crkp_2f~2:->module~darcy_mod: program~darcy_2f darcy_2f program~darcy_2f:->module~partition_mod: program~darcy_2f:->module~darcy_mod: Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Subroutines clean_PETSc_output init_write_MATLAB read_bndry_cond read_bndry_cond2 read_crack_faces read_glb_indx read_glb_indx2 read_nodes_prml read_prml_elms read_prml_elms2 root_open_file_read root_open_file_write write_centers_MATLAB write_centers_MATLAB2 write_pressure_MATLAB write_pressure_MATLAB2 write_prml_volumes_MATLAB write_prml_volumes_MATLAB2 write_solution_D0S write_solution_D0S2 write_solution_MATLAB write_solution_MATLAB2 write_unsteady_D0S write_unsteady_D0S2 Subroutines public subroutine clean_PETSc_output () Author Pieter Boom Date 2020/09/23 Clean up PETSc output format Arguments None public subroutine init_write_MATLAB () Author Pieter Boom Date 2020/09/23 Setup PETSc for MATLAB output Arguments None public subroutine read_bndry_cond () Author Pieter Boom Date 2019/08/23 Read boundary conditions from file Arguments None public subroutine read_bndry_cond2 () Author Pieter Boom Date 2019/08/23 Read boundary conditions from file Arguments None public subroutine read_crack_faces () Author Pieter Boom Date 2019/08/23 read crack faces from file Arguments None public subroutine read_glb_indx (k) Author Pieter Boom Date 2019/08/23 Read glbal indices of elements of given geometric order Arguments Type Intent Optional Attributes Name integer, intent(in) :: k simplicial order public subroutine read_glb_indx2 (k) Author Pieter Boom Date 2019/08/23 Read local node indices of highest order primal elements Arguments Type Intent Optional Attributes Name integer, intent(in) :: k simplicial order public subroutine read_nodes_prml () Author Pieter Boom Date 2019/08/23 Root process reads the location of primal nodes Arguments None public subroutine read_prml_elms () Author Pieter Boom Date 2019/08/23 Read local node indices of highest order primal elements Arguments None public subroutine read_prml_elms2 () Author Pieter Boom Date 2019/08/23 Read local node indices of highest order primal elements Arguments None public subroutine root_open_file_read (fname, unit) Author Pieter Boom Date 2019/08/23 Open given file on root process for reading Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: fname file name integer, intent(in) :: unit file unit public subroutine root_open_file_write (fname, unit) Author Pieter Boom Date 2019/08/23 Open given file on root process for writing Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: fname file name integer, intent(in) :: unit file unit public subroutine write_centers_MATLAB () Author Pieter Boom Date 2019/08/23 write barycenters to file in MATLAB format Arguments None public subroutine write_centers_MATLAB2 () Author Pieter Boom Date 2019/08/23 write barycenters to file in MATLAB format Arguments None public subroutine write_pressure_MATLAB (iter) Author Pieter Boom Date 2019/08/23 write scaled pressure solution to file in MATLAB format Arguments Type Intent Optional Attributes Name integer, intent(in), optional :: iter iteration for file name public subroutine write_pressure_MATLAB2 (iter) Author Pieter Boom Date 2019/08/23 write scaled pressure solution to file in MATLAB format Arguments Type Intent Optional Attributes Name integer, intent(in), optional :: iter iteration for file name public subroutine write_prml_volumes_MATLAB () Author Pieter Boom Date 2019/08/23 write primal volumes to file in MATLAB format Arguments None public subroutine write_prml_volumes_MATLAB2 () Author Pieter Boom Date 2019/08/23 write primary volumes to file in MATLAB format Arguments None public subroutine write_solution_D0S (sclr_name, vctr_name) Author Pieter Boom Date 2019/08/23 write vtk solution at dual 0-cells Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: sclr_name scalar variable name (label) character(len=*), intent(in) :: vctr_name vector variable name (label) public subroutine write_solution_D0S2 (sclr_name, vctr_name) Author Pieter Boom Date 2019/08/23 write vtk solution at dual 0-cells Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: sclr_name scalar variable name (label) character(len=*), intent(in) :: vctr_name vector variable name (label) public subroutine write_solution_MATLAB (iter) Author Pieter Boom Date 2019/08/23 write solution to file in MATLAB format Arguments Type Intent Optional Attributes Name integer, intent(in), optional :: iter iteration for file name public subroutine write_solution_MATLAB2 (iter) Author Pieter Boom Date 2019/08/23 write solution to file in MATLAB format Arguments Type Intent Optional Attributes Name integer, intent(in), optional :: iter iteration for file name public subroutine write_unsteady_D0S (sclr_name, vctr_name, iter) Author Pieter Boom Date 2019/08/23 write vtk solution at dual 0-cells for evolving simulations Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: sclr_name scalar variable name (label) character(len=*), intent(in) :: vctr_name vector variable name (label) integer, intent(in) :: iter integer iteration number public subroutine write_unsteady_D0S2 (sclr_name, vctr_name, iter) Author Pieter Boom Date 2019/08/23 write vtk solution at dual 0-cells for evolving simulations Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: sclr_name scalar variable name (label) character(len=*), intent(in) :: vctr_name vector variable name (label) integer, intent(in) :: iter integer iteration number","tags":"","loc":"module/io_mod.html","title":"io_mod – ParaGEMS"},{"text":"Uses solver_mod module~~test_solver_mod~~UsesGraph module~test_solver_mod test_solver_mod module~solver_mod solver_mod module~test_solver_mod:->module~solver_mod: module~common_mod common_mod module~solver_mod:->module~common_mod: iso_fortran_env iso_fortran_env module~solver_mod:->iso_fortran_env: ieee_arithmetic ieee_arithmetic module~solver_mod:->ieee_arithmetic: module~mpi_mod mpi_mod module~solver_mod:->module~mpi_mod: petsc petsc module~common_mod:->petsc: module~mpi_mod:->module~common_mod: Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Subroutines test_vars Subroutines public subroutine test_vars (cnt) Arguments Type Intent Optional Attributes Name integer, intent(inout) :: cnt","tags":"","loc":"module/test_solver_mod.html","title":"test_solver_mod – ParaGEMS"},{"text":"Module contains general routines for solvers (linear and nonlinear) Uses common_mod mpi_mod ieee_arithmetic iso_fortran_env module~~solver_mod~~UsesGraph module~solver_mod solver_mod module~common_mod common_mod module~solver_mod:->module~common_mod: iso_fortran_env iso_fortran_env module~solver_mod:->iso_fortran_env: ieee_arithmetic ieee_arithmetic module~solver_mod:->ieee_arithmetic: module~mpi_mod mpi_mod module~solver_mod:->module~mpi_mod: petsc petsc module~common_mod:->petsc: module~mpi_mod:->module~common_mod: Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Used by module~~solver_mod~~UsedByGraph module~solver_mod solver_mod program~darcy_1f darcy_1f program~darcy_1f:->module~solver_mod: module~darcy_mod darcy_mod program~darcy_1f:->module~darcy_mod: module~partition_mod partition_mod program~darcy_1f:->module~partition_mod: module~test_solver_mod test_solver_mod module~test_solver_mod:->module~solver_mod: module~darcy_mod:->module~solver_mod: module~io_mod io_mod module~darcy_mod:->module~io_mod: module~io_mod:->module~solver_mod: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->module~solver_mod: program~darcy_crkp_2f:->module~darcy_mod: program~darcy_crkp_2f:->module~partition_mod: program~darcy darcy program~darcy:->module~solver_mod: program~darcy:->module~darcy_mod: program~darcy:->module~partition_mod: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->module~solver_mod: program~darcy_crkp_1f:->module~darcy_mod: program~darcy_crkp_1f:->module~partition_mod: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->module~solver_mod: program~darcy_crkp_2f~2:->module~darcy_mod: program~darcy_crkp_2f~2:->module~partition_mod: program~darcy_2f darcy_2f program~darcy_2f:->module~solver_mod: program~darcy_2f:->module~darcy_mod: program~darcy_2f:->module~partition_mod: module~test_darcy_mod test_darcy_mod module~test_darcy_mod:->module~darcy_mod: module~partition_mod:->module~io_mod: module~test_io_mod test_io_mod module~test_io_mod:->module~io_mod: module~test_partition_mod test_partition_mod module~test_partition_mod:->module~partition_mod: Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Subroutines check_param_solver end_petsc extract_sol_KSP extract_sol_KSP2 set_defaults_solver solve_KSP solve_KSP2 start_petsc Subroutines public subroutine check_param_solver () Author Pieter Boom Date 2019/08/23 Check values set for solver Arguments None public subroutine end_petsc () Author Pieter Boom Date 2019/08/23 Finalise PETSc library Arguments None public subroutine extract_sol_KSP () Author Pieter Boom Date 2019/08/23 extract local solution values from distributed PETSc vector Read more… Arguments None public subroutine extract_sol_KSP2 () Author Pieter Boom Date 2019/08/23 extract local solution values from distributed PETSc vector Read more… Arguments None public subroutine set_defaults_solver (solver) Author Pieter Boom Date 2019/08/23 Set default values for solver Arguments Type Intent Optional Attributes Name character(len=*) :: solver public subroutine solve_KSP () Author Pieter Boom Date 2019/08/23 Solve linear system using PETSc KSP Arguments None public subroutine solve_KSP2 () Author Pieter Boom Date 2019/08/23 Solve linear system using PETSc KSP Arguments None public subroutine start_petsc () Author Pieter Boom Date 2019/08/23 Initialise PETSc library Arguments None","tags":"","loc":"module/solver_mod.html","title":"solver_mod – ParaGEMS"},{"text":"Uses math_mod module~~test_math_mod~~UsesGraph module~test_math_mod test_math_mod module~math_mod math_mod module~test_math_mod:->module~math_mod: module~common_mod common_mod module~math_mod:->module~common_mod: module~mpi_mod mpi_mod module~math_mod:->module~mpi_mod: petsc petsc module~common_mod:->petsc: module~mpi_mod:->module~common_mod: Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Subroutines test_vars Subroutines public subroutine test_vars (cnt) Arguments Type Intent Optional Attributes Name integer, intent(inout) :: cnt","tags":"","loc":"module/test_math_mod.html","title":"test_math_mod – ParaGEMS"},{"text":"Module for common math functions and routines Uses common_mod mpi_mod module~~math_mod~~UsesGraph module~math_mod math_mod module~common_mod common_mod module~math_mod:->module~common_mod: module~mpi_mod mpi_mod module~math_mod:->module~mpi_mod: petsc petsc module~common_mod:->petsc: module~mpi_mod:->module~common_mod: Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Used by module~~math_mod~~UsedByGraph module~math_mod math_mod module~dec_mod dec_mod module~dec_mod:->module~math_mod: module~partition_mod partition_mod module~partition_mod:->module~math_mod: module~partition_mod:->module~dec_mod: module~io_mod io_mod module~partition_mod:->module~io_mod: module~test_math_mod test_math_mod module~test_math_mod:->module~math_mod: module~test_dec_mod test_dec_mod module~test_dec_mod:->module~dec_mod: program~darcy_1f darcy_1f program~darcy_1f:->module~partition_mod: module~darcy_mod darcy_mod program~darcy_1f:->module~darcy_mod: module~test_partition_mod test_partition_mod module~test_partition_mod:->module~partition_mod: module~io_mod:->module~dec_mod: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->module~partition_mod: program~darcy_crkp_2f:->module~darcy_mod: program~darcy darcy program~darcy:->module~partition_mod: program~darcy:->module~darcy_mod: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->module~partition_mod: program~darcy_crkp_1f:->module~darcy_mod: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->module~partition_mod: program~darcy_crkp_2f~2:->module~darcy_mod: program~darcy_2f darcy_2f program~darcy_2f:->module~partition_mod: program~darcy_2f:->module~darcy_mod: module~test_io_mod test_io_mod module~test_io_mod:->module~io_mod: module~darcy_mod:->module~io_mod: module~test_darcy_mod test_darcy_mod module~test_darcy_mod:->module~darcy_mod: Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Functions index_in_list num_element_in_list any_element_in_list cross_product determinant matinv2 matinv3 Subroutines int_insertion_sort int_merge_rows int_merge_sort_rows Functions public function index_in_list (elm, list, init_guess) Author Pieter Boom Date 2019/08/29 Test if any element of one array is in another Arguments Type Intent Optional Attributes Name integer, intent(in) :: elm integer, intent(in) :: list (:) integer, intent(in) :: init_guess Return Value integer public function num_element_in_list (elm, node_list) Author Pieter Boom Date 2019/08/29 Test if any element of one array is in another Arguments Type Intent Optional Attributes Name integer, intent(in) :: elm (:) integer, intent(in) :: node_list (:) Return Value integer public function any_element_in_list (elm, node_list) Author Pieter Boom Date 2019/08/29 Test if any element of one array is in another Arguments Type Intent Optional Attributes Name integer, intent(in) :: elm (:) integer, intent(in) :: node_list (:) Return Value logical public function cross_product (a, b) Author Pieter Boom Date 2019/08/29 Performs a direct calculation of vector cross product Arguments Type Intent Optional Attributes Name real(kind=iwp), intent(in) :: a (3) input vectors real(kind=iwp), intent(in) :: b (3) input vectors Return Value real(kind=iwp)\n  (3) resulting vector public recursive function determinant (A, n) result(accumulation) Author Pieter Boom Date 2019/08/29 Compute determinant Arguments Type Intent Optional Attributes Name real(kind=iwp), intent(in) :: A (n,n) integer, intent(in) :: n Return Value real(kind=iwp) public function matinv2 (A) result(B) Author Pieter Boom Date 2019/08/29 Performs a direct calculation of the inverse of a 2x2 matrix. Arguments Type Intent Optional Attributes Name real(kind=iwp), intent(in) :: A (2,2) Matrix Return Value real(kind=iwp)\n  (2,2) Inverse matrix public function matinv3 (A) result(B) Author Pieter Boom Date 2019/08/29 Performs a direct calculation of the inverse of a 3×3 matrix. Arguments Type Intent Optional Attributes Name real(kind=iwp), intent(in) :: A (3,3) Matrix Return Value real(kind=iwp)\n  (3,3) Inverse matrix Subroutines public subroutine int_insertion_sort (A) Author Pieter Boom Date 2019/08/29 Insertion sort of integer array Arguments Type Intent Optional Attributes Name integer, intent(inout) :: A (:) public subroutine int_merge_rows (A, B, C, irow, frow) Author Pieter Boom Date 2019/08/23 Merge rows of an array in ascending order\n Assumption: rows are already sorted in ascending order Arguments Type Intent Optional Attributes Name integer, intent(in) :: A (:,:) integer, intent(in) :: B (:,:) integer, intent(inout) :: C (:,:) integer, intent(in) :: irow integer, intent(in) :: frow public subroutine int_merge_sort_rows (A, length, irow, frow, work) Author Pieter Boom Date 2019/08/23 Merge sort rows of integer array in ascending order\n Assumption: rows are already sorted in ascending order Arguments Type Intent Optional Attributes Name integer, intent(inout) :: A (:,:) integer :: length integer, intent(in) :: irow integer, intent(in) :: frow integer, intent(inout) :: work (:,:)","tags":"","loc":"module/math_mod.html","title":"math_mod – ParaGEMS"},{"text":"Module contains routines specifically related to Darcy flow equations Uses common_mod mpi_mod io_mod solver_mod module~~darcy_mod~~UsesGraph module~darcy_mod darcy_mod module~common_mod common_mod module~darcy_mod:->module~common_mod: module~io_mod io_mod module~darcy_mod:->module~io_mod: module~mpi_mod mpi_mod module~darcy_mod:->module~mpi_mod: module~solver_mod solver_mod module~darcy_mod:->module~solver_mod: petsc petsc module~common_mod:->petsc: module~io_mod:->module~common_mod: module~io_mod:->module~mpi_mod: module~io_mod:->module~solver_mod: module~dec_mod dec_mod module~io_mod:->module~dec_mod: iso_fortran_env iso_fortran_env module~io_mod:->iso_fortran_env: ieee_arithmetic ieee_arithmetic module~io_mod:->ieee_arithmetic: module~mpi_mod:->module~common_mod: module~solver_mod:->module~common_mod: module~solver_mod:->module~mpi_mod: module~solver_mod:->iso_fortran_env: module~solver_mod:->ieee_arithmetic: module~dec_mod:->module~common_mod: module~dec_mod:->module~mpi_mod: module~math_mod math_mod module~dec_mod:->module~math_mod: module~math_mod:->module~common_mod: module~math_mod:->module~mpi_mod: Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Used by module~~darcy_mod~~UsedByGraph module~darcy_mod darcy_mod program~darcy_1f darcy_1f program~darcy_1f:->module~darcy_mod: module~test_darcy_mod test_darcy_mod module~test_darcy_mod:->module~darcy_mod: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->module~darcy_mod: program~darcy darcy program~darcy:->module~darcy_mod: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->module~darcy_mod: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->module~darcy_mod: program~darcy_2f darcy_2f program~darcy_2f:->module~darcy_mod: Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Subroutines check_param_darcy exchange_bndry_cond finalise_darcy finalise_darcy2 get_LHS_darcy get_LHS_darcy2 get_RHS_darcy get_RHS_darcy2 identify_crack identify_crack2 identify_crack3 identify_crack4 identify_crack5 identify_crack6 identify_crack7 initialise_darcy initialise_darcy2 read_input_darcy Subroutines public subroutine check_param_darcy () Author Pieter Boom Date 2020/09/16 Check values set for solving Darcy flow simulation Arguments None public subroutine exchange_bndry_cond (k) Author Pieter Boom Date 2019/08/23 exchange boundary conditions Arguments Type Intent Optional Attributes Name integer, intent(in) :: k simplicial order public subroutine finalise_darcy () Author Pieter Boom Date 2019/08/23 clean up matrices and vectors for Darcy simulations Arguments None public subroutine finalise_darcy2 () Author Pieter Boom Date 2019/08/23 clean up matrices and vectors for Darcy simulations Arguments None public subroutine get_LHS_darcy () Author Pieter Boom Date 2019/08/23 setup left hand side matrix for darcy simulations Arguments None public subroutine get_LHS_darcy2 () Author Pieter Boom Date 2019/08/23 setup left hand side matrix for darcy simulations Arguments None public subroutine get_RHS_darcy () Author Pieter Boom Date 2019/08/23 setup right hand side vector and boundary conditions for darcy simulations Arguments None public subroutine get_RHS_darcy2 () Author Pieter Boom Date 2019/08/23 setup right hand side vector and boundary conditions for darcy simulations Arguments None public subroutine identify_crack (exit_cond) Author Pieter Boom Date 2019/08/23 identify faces to crack based on threshold value Arguments Type Intent Optional Attributes Name logical, intent(out) :: exit_cond public subroutine identify_crack2 (exit_cond) Author Pieter Boom Date 2019/08/23 identify faces to crack based on threshold value Arguments Type Intent Optional Attributes Name logical, intent(out) :: exit_cond public subroutine identify_crack3 (iter, exit_cond) Author Pieter Boom Date 2019/08/23 identify faces to crack based on max value Arguments Type Intent Optional Attributes Name integer, intent(in) :: iter logical, intent(out) :: exit_cond public subroutine identify_crack4 (iter, exit_cond) Author Pieter Boom Date 2019/08/23 identify faces to crack based on threshold value Arguments Type Intent Optional Attributes Name integer, intent(in) :: iter logical, intent(out) :: exit_cond public subroutine identify_crack5 (iter, exit_cond) Author Pieter Boom Date 2019/08/23 identify faces to crack based on max value Arguments Type Intent Optional Attributes Name integer, intent(in) :: iter logical, intent(out) :: exit_cond public subroutine identify_crack6 (iter, exit_cond) Author Pieter Boom Date 2020/12/09 Arguments Type Intent Optional Attributes Name integer, intent(in) :: iter logical, intent(out) :: exit_cond public subroutine identify_crack7 (iter, exit_cond) Author Pieter Boom Date 2019/08/23 Arguments Type Intent Optional Attributes Name integer, intent(in) :: iter logical, intent(out) :: exit_cond public subroutine initialise_darcy () Author Pieter Boom Date 2019/08/23 initialise matrices and vectors for Darcy simulations Arguments None public subroutine initialise_darcy2 () Author Pieter Boom Date 2019/08/23 initialise matrices and vectors for Darcy simulations Arguments None public subroutine read_input_darcy () Author Pieter Boom Date 2020/09/16 Read input filefor user defined parameters, and perform checks to ensure\n reasonable values are set Arguments None","tags":"","loc":"module/darcy_mod.html","title":"darcy_mod – ParaGEMS"},{"text":"Uses darcy_mod module~~test_darcy_mod~~UsesGraph module~test_darcy_mod test_darcy_mod module~darcy_mod darcy_mod module~test_darcy_mod:->module~darcy_mod: module~common_mod common_mod module~darcy_mod:->module~common_mod: module~io_mod io_mod module~darcy_mod:->module~io_mod: module~mpi_mod mpi_mod module~darcy_mod:->module~mpi_mod: module~solver_mod solver_mod module~darcy_mod:->module~solver_mod: petsc petsc module~common_mod:->petsc: module~io_mod:->module~common_mod: module~io_mod:->module~mpi_mod: module~io_mod:->module~solver_mod: module~dec_mod dec_mod module~io_mod:->module~dec_mod: iso_fortran_env iso_fortran_env module~io_mod:->iso_fortran_env: ieee_arithmetic ieee_arithmetic module~io_mod:->ieee_arithmetic: module~mpi_mod:->module~common_mod: module~solver_mod:->module~common_mod: module~solver_mod:->module~mpi_mod: module~solver_mod:->iso_fortran_env: module~solver_mod:->ieee_arithmetic: module~dec_mod:->module~common_mod: module~dec_mod:->module~mpi_mod: module~math_mod math_mod module~dec_mod:->module~math_mod: module~math_mod:->module~common_mod: module~math_mod:->module~mpi_mod: Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Subroutines test_vars Subroutines public subroutine test_vars (cnt) Arguments Type Intent Optional Attributes Name integer, intent(inout) :: cnt","tags":"","loc":"module/test_darcy_mod.html","title":"test_darcy_mod – ParaGEMS"},{"text":"Module for global variables, included library, and common code Uses petsc module~~common_mod~~UsesGraph module~common_mod common_mod petsc petsc module~common_mod:->petsc: Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Used by module~~common_mod~~UsedByGraph module~common_mod common_mod module~mpi_mod mpi_mod module~mpi_mod:->module~common_mod: program~darcy_1f darcy_1f program~darcy_1f:->module~common_mod: program~darcy_1f:->module~mpi_mod: module~darcy_mod darcy_mod program~darcy_1f:->module~darcy_mod: module~partition_mod partition_mod program~darcy_1f:->module~partition_mod: module~solver_mod solver_mod program~darcy_1f:->module~solver_mod: module~dec_mod dec_mod module~dec_mod:->module~common_mod: module~dec_mod:->module~mpi_mod: module~math_mod math_mod module~dec_mod:->module~math_mod: module~math_mod:->module~common_mod: module~math_mod:->module~mpi_mod: module~test_common_mod test_common_mod module~test_common_mod:->module~common_mod: module~darcy_mod:->module~common_mod: module~darcy_mod:->module~mpi_mod: module~io_mod io_mod module~darcy_mod:->module~io_mod: module~darcy_mod:->module~solver_mod: module~io_mod:->module~common_mod: module~io_mod:->module~mpi_mod: module~io_mod:->module~dec_mod: module~io_mod:->module~solver_mod: program~paragems_unit paragems_unit program~paragems_unit:->module~common_mod: program~paragems_unit:->module~test_common_mod: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->module~common_mod: program~darcy_crkp_2f:->module~mpi_mod: program~darcy_crkp_2f:->module~darcy_mod: program~darcy_crkp_2f:->module~partition_mod: program~darcy_crkp_2f:->module~solver_mod: program~darcy darcy program~darcy:->module~common_mod: program~darcy:->module~mpi_mod: program~darcy:->module~darcy_mod: program~darcy:->module~partition_mod: program~darcy:->module~solver_mod: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->module~common_mod: program~darcy_crkp_1f:->module~mpi_mod: program~darcy_crkp_1f:->module~darcy_mod: program~darcy_crkp_1f:->module~partition_mod: program~darcy_crkp_1f:->module~solver_mod: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->module~common_mod: program~darcy_crkp_2f~2:->module~mpi_mod: program~darcy_crkp_2f~2:->module~darcy_mod: program~darcy_crkp_2f~2:->module~partition_mod: program~darcy_crkp_2f~2:->module~solver_mod: module~test_mpi test_mpi module~test_mpi:->module~common_mod: module~test_mpi:->module~mpi_mod: module~partition_mod:->module~common_mod: module~partition_mod:->module~mpi_mod: module~partition_mod:->module~dec_mod: module~partition_mod:->module~math_mod: module~partition_mod:->module~io_mod: program~darcy_2f darcy_2f program~darcy_2f:->module~common_mod: program~darcy_2f:->module~mpi_mod: program~darcy_2f:->module~darcy_mod: program~darcy_2f:->module~partition_mod: program~darcy_2f:->module~solver_mod: module~solver_mod:->module~common_mod: module~solver_mod:->module~mpi_mod: module~test_dec_mod test_dec_mod module~test_dec_mod:->module~dec_mod: module~test_darcy_mod test_darcy_mod module~test_darcy_mod:->module~darcy_mod: module~test_io_mod test_io_mod module~test_io_mod:->module~io_mod: module~test_partition_mod test_partition_mod module~test_partition_mod:->module~partition_mod: module~test_math_mod test_math_mod module~test_math_mod:->module~math_mod: module~test_solver_mod test_solver_mod module~test_solver_mod:->module~solver_mod: Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Subroutines clean_up Subroutines public subroutine clean_up () Author Pieter Boom Date 2019/08/21 Deallocate global structures/variables Arguments None","tags":"","loc":"module/common_mod.html","title":"common_mod – ParaGEMS"},{"text":"Tests for common_mod module, primairly global variables Uses common_mod module~~test_common_mod~~UsesGraph module~test_common_mod test_common_mod module~common_mod common_mod module~test_common_mod:->module~common_mod: petsc petsc module~common_mod:->petsc: Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Used by module~~test_common_mod~~UsedByGraph module~test_common_mod test_common_mod program~paragems_unit paragems_unit program~paragems_unit:->module~test_common_mod: Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Subroutines test_vars Subroutines public subroutine test_vars (cnt) Author Pieter Boom Date 2019/08/21 deallocate global structures/variables Arguments Type Intent Optional Attributes Name integer, intent(inout) :: cnt test counter","tags":"","loc":"module/test_common_mod.html","title":"test_common_mod – ParaGEMS"},{"text":"Module containing routines for performing DEC operations Uses common_mod mpi_mod math_mod module~~dec_mod~~UsesGraph module~dec_mod dec_mod module~common_mod common_mod module~dec_mod:->module~common_mod: module~math_mod math_mod module~dec_mod:->module~math_mod: module~mpi_mod mpi_mod module~dec_mod:->module~mpi_mod: petsc petsc module~common_mod:->petsc: module~math_mod:->module~common_mod: module~math_mod:->module~mpi_mod: module~mpi_mod:->module~common_mod: Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Used by module~~dec_mod~~UsedByGraph module~dec_mod dec_mod module~io_mod io_mod module~io_mod:->module~dec_mod: module~partition_mod partition_mod module~partition_mod:->module~dec_mod: module~partition_mod:->module~io_mod: module~test_dec_mod test_dec_mod module~test_dec_mod:->module~dec_mod: program~darcy_crkp_2f darcy_crkp_2f program~darcy_crkp_2f:->module~partition_mod: module~darcy_mod darcy_mod program~darcy_crkp_2f:->module~darcy_mod: program~darcy_1f darcy_1f program~darcy_1f:->module~partition_mod: program~darcy_1f:->module~darcy_mod: module~test_io_mod test_io_mod module~test_io_mod:->module~io_mod: module~test_partition_mod test_partition_mod module~test_partition_mod:->module~partition_mod: module~darcy_mod:->module~io_mod: program~darcy darcy program~darcy:->module~partition_mod: program~darcy:->module~darcy_mod: program~darcy_crkp_1f darcy_crkp_1f program~darcy_crkp_1f:->module~partition_mod: program~darcy_crkp_1f:->module~darcy_mod: program~darcy_crkp_2f~2 darcy_crkp_2f program~darcy_crkp_2f~2:->module~partition_mod: program~darcy_crkp_2f~2:->module~darcy_mod: program~darcy_2f darcy_2f program~darcy_2f:->module~partition_mod: program~darcy_2f:->module~darcy_mod: module~test_darcy_mod test_darcy_mod module~test_darcy_mod:->module~darcy_mod: Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Functions calc_unsgnd_vlm Subroutines add_points_i allocate_bndry_cobndry build_bndry_work_array calc_barycentric_grad calc_bndry_cobndry calc_circumcenters calc_dual_dir calc_dual_vlm calc_dual_vlm_i calc_hodge_star calc_orientation calc_prml_dir calc_prml_sgnd_vlm calc_prml_unsgnd_vlm calc_whitney_C2_BC count_bndry_cobndry exchange_dual_dir exchange_dual_vlm get_lcl_node_indx initialise_geo set_bndry_cobndry Functions public function calc_unsgnd_vlm (pts, n) Author Pieter Boom Date 2019/08/23 Compute unsigned volume for given set of points Arguments Type Intent Optional Attributes Name real(kind=iwp), intent(in) :: pts (:,:) bounding points integer, intent(in) :: n # points to use Return Value real(kind=iwp) function value Subroutines public subroutine add_points_i (pts, indx, k) Author Pieter Boom Date 2019/08/23 Recursively adds points for dual volume calculation Arguments Type Intent Optional Attributes Name real(kind=iwp), intent(inout) :: pts (:,:) bounding points integer, intent(in) :: indx element index integer, intent(in) :: k simplicial order public subroutine allocate_bndry_cobndry (k, cnt, bndry_cnt, cobndry_cnt, ext_cnt, surf_cnt) Author Pieter Boom Date 2019/08/23 Allocate [co-]boundaries structures/variables Arguments Type Intent Optional Attributes Name integer, intent(in) :: k simplicial order integer, intent(in) :: cnt counter (k-1) element integer, intent(in) :: bndry_cnt (:) temp # local boundaries integer, intent(in) :: cobndry_cnt (:) temp # co-boundaries integer, intent(in) :: ext_cnt (2) counter external boundaries integer, intent(in) :: surf_cnt counter surface boundaries public subroutine build_bndry_work_array (bndry, k) Author Pieter Boom Date 2019/08/23 Build boundary data working array Arguments Type Intent Optional Attributes Name integer, intent(inout) :: bndry (:,:) boundary work array integer, intent(in) :: k simplicial order public subroutine calc_barycentric_grad (pts, n) Author Pieter Boom Date 2019/08/23 Compute barycentric gradients Arguments Type Intent Optional Attributes Name real(kind=iwp), intent(inout) :: pts (:,:) vertices of simplex integer, intent(in) :: n number of vertices public subroutine calc_bndry_cobndry () Author Pieter Boom Date 2019/08/20 Recursively compute element (co-)boundaries from highest to lowest geometric order Arguments None public subroutine calc_circumcenters (k) Author Pieter Boom Date 2019/08/23 Compute circumcenter of given elements Arguments Type Intent Optional Attributes Name integer :: k simplicial order public subroutine calc_dual_dir () Author Pieter Boom Date 2019/08/23 Compute the unit direction of dual edges Arguments None public subroutine calc_dual_vlm () Author Pieter Boom Date 2019/08/23 Compute volume for dual elements of all geometric order Arguments None public subroutine calc_dual_vlm_i (pts, sgn, indx, p_indx, k) Author Pieter Boom Date 2019/08/23 Recursively add to dual volume calculation Arguments Type Intent Optional Attributes Name real(kind=iwp), intent(inout) :: pts (:,:) bounding points real(kind=iwp), intent(inout) :: sgn (:) sign of volume elements integer, intent(in) :: indx element and parent index integer, intent(in) :: p_indx element and parent index integer, intent(in) :: k simplicial order public subroutine calc_hodge_star (k) Author Pieter Boom Date 2019/08/23 Compute hodge star and it's inverse from primal and dual volumes Arguments Type Intent Optional Attributes Name integer, intent(in) :: k geometric order public subroutine calc_orientation (elm, orientation) Author Pieter Boom Date 2019/08/23 Sort nodal indices and compute ± orientation of an element Arguments Type Intent Optional Attributes Name integer, intent(inout) :: elm (:) nodal indices of elements integer, intent(inout) :: orientation orientations of the elements public subroutine calc_prml_dir () Author Pieter Boom Date 2019/08/23 Compute the unit direction of primal edges Arguments None public subroutine calc_prml_sgnd_vlm (k) Author Pieter Boom Date 2019/08/23 Compute signed volume of primal elements Arguments Type Intent Optional Attributes Name integer :: k simplicial order public subroutine calc_prml_unsgnd_vlm (k) Author Pieter Boom Date 2019/08/23 Compute unsigned volume of primal elements Arguments Type Intent Optional Attributes Name integer :: k loop counters public subroutine calc_whitney_C2_BC () Author Pieter Boom Date 2019/08/23 Compute the Whitney interpolation of primal faces to primal volume barycentric centers Arguments None public subroutine count_bndry_cobndry (bndry, k, cnt, bndry_cnt, cobndry_cnt, ext_cnt, surf_cnt, local) Author Pieter Boom Date 2019/08/23 Count [co-]boundaries: internal, external, surface Arguments Type Intent Optional Attributes Name integer, intent(in) :: bndry (:,:) boundary work array integer, intent(in) :: k simplicial order integer, intent(inout) :: cnt counter (k-1) element integer, intent(inout) :: bndry_cnt (:) counter local boundaries integer, intent(inout) :: cobndry_cnt (:) counter co-boundaries integer, intent(inout) :: ext_cnt (2) counter external boundaries integer, intent(inout) :: surf_cnt counter surface boundaries logical, intent(inout) :: local (:) locality work array public subroutine exchange_dual_dir (k) Author Pieter Boom Date 2019/08/22 Exchange external dual edge directions between adjacent processes Arguments Type Intent Optional Attributes Name integer, intent(in) :: k simplicial order public subroutine exchange_dual_vlm (k) Author Pieter Boom Date 2019/08/22 Exchange external dual volumes between adjacent processes Arguments Type Intent Optional Attributes Name integer, intent(in) :: k simplicial order public subroutine get_lcl_node_indx () Author Pieter Boom Date 2019/08/23 Create map between global and local node indices Arguments None public subroutine initialise_geo () Author Pieter Boom Date 2019/08/23 Initialise geometric quantities for the given mesh Arguments None public subroutine set_bndry_cobndry (bndry, k, cnt, bndry_cnt, cobndry_cnt, ext_cnt, surf_cnt, local) Author Pieter Boom Date 2019/08/23 Set [co-]boundaries: internal, external, surface Arguments Type Intent Optional Attributes Name integer, intent(inout) :: bndry (:,:) boundary work array integer, intent(in) :: k simplicial order integer, intent(inout) :: cnt counter (k-1) element integer, intent(inout) :: bndry_cnt (:) temp # local boundaries integer, intent(inout) :: cobndry_cnt (:) temp # co-boundaries integer, intent(inout) :: ext_cnt (2) counter external boundaries integer, intent(inout) :: surf_cnt counter surface boundaries logical, intent(inout) :: local (:) locality work array","tags":"","loc":"module/dec_mod.html","title":"dec_mod – ParaGEMS"},{"text":"Uses dec_mod module~~test_dec_mod~~UsesGraph module~test_dec_mod test_dec_mod module~dec_mod dec_mod module~test_dec_mod:->module~dec_mod: module~common_mod common_mod module~dec_mod:->module~common_mod: module~math_mod math_mod module~dec_mod:->module~math_mod: module~mpi_mod mpi_mod module~dec_mod:->module~mpi_mod: petsc petsc module~common_mod:->petsc: module~math_mod:->module~common_mod: module~math_mod:->module~mpi_mod: module~mpi_mod:->module~common_mod: Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Subroutines test_vars Subroutines public subroutine test_vars (cnt) Arguments Type Intent Optional Attributes Name integer, intent(inout) :: cnt","tags":"","loc":"module/test_dec_mod.html","title":"test_dec_mod – ParaGEMS"},{"text":"Miniapp to to reorder nodes file for 'better' partitioning/load balancing Calls program~~simpleloadbalance~~CallsGraph program~simpleloadbalance simpleLoadBalance proc~merge_sort_rows merge_sort_rows program~simpleloadbalance:->proc~merge_sort_rows: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables fname mesh_prefix elems ie ier indx iwp je junk ke kn num_elm num_node offset unit centers nodes work Subroutines merge_rows merge_sort_rows Variables Type Attributes Name Initial character(len=128) :: fname character(len=128) :: mesh_prefix integer, ALLOCATABLE :: elems (:,:) integer :: ie integer :: ier integer, ALLOCATABLE :: indx (:) integer, parameter :: iwp = SELECTED_REAL_KIND(15, 300) integer :: je integer :: junk integer :: ke integer :: kn integer :: num_elm integer :: num_node integer :: offset integer :: unit = 10 real(kind=iwp), ALLOCATABLE :: centers (:,:) real(kind=iwp), ALLOCATABLE :: nodes (:,:) real(kind=iwp), ALLOCATABLE :: work (:,:) Subroutines subroutine merge_rows (A, B, C, irow, frow) Arguments Type Intent Optional Attributes Name real(kind=iwp), intent(in) :: A (:,:) real(kind=iwp), intent(in) :: B (:,:) real(kind=iwp), intent(inout) :: C (:,:) integer, intent(in) :: irow integer, intent(in) :: frow subroutine merge_sort_rows (A, length, irow, frow, work) Arguments Type Intent Optional Attributes Name real(kind=iwp), intent(inout) :: A (:,:) integer, intent(in) :: length integer, intent(in) :: irow integer, intent(in) :: frow real(kind=iwp), intent(inout) :: work (:,:)","tags":"","loc":"program/simpleloadbalance.html","title":"simpleLoadBalance – ParaGEMS"},{"text":"Uses common_mod test_common_mod program~~paragems_unit~~UsesGraph program~paragems_unit paragems_unit module~common_mod common_mod program~paragems_unit:->module~common_mod: module~test_common_mod test_common_mod program~paragems_unit:->module~test_common_mod: petsc petsc module~common_mod:->petsc: module~test_common_mod:->module~common_mod: Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Driver routine for ParaGEMS' unit tests Calls program~~paragems_unit~~CallsGraph program~paragems_unit paragems_unit proc~test_vars~6 test_vars program~paragems_unit:->proc~test_vars~6: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables cnt Variables Type Attributes Name Initial integer :: cnt = 0","tags":"","loc":"program/paragems_unit.html","title":"paragems_unit – ParaGEMS"},{"text":"Uses common_mod mpi_mod partition_mod darcy_mod solver_mod program~~darcy_1f~~UsesGraph program~darcy_1f darcy_1f module~common_mod common_mod program~darcy_1f:->module~common_mod: module~mpi_mod mpi_mod program~darcy_1f:->module~mpi_mod: module~partition_mod partition_mod program~darcy_1f:->module~partition_mod: module~darcy_mod darcy_mod program~darcy_1f:->module~darcy_mod: module~solver_mod solver_mod program~darcy_1f:->module~solver_mod: petsc petsc module~common_mod:->petsc: module~mpi_mod:->module~common_mod: module~partition_mod:->module~common_mod: module~partition_mod:->module~mpi_mod: module~dec_mod dec_mod module~partition_mod:->module~dec_mod: module~io_mod io_mod module~partition_mod:->module~io_mod: module~math_mod math_mod module~partition_mod:->module~math_mod: module~darcy_mod:->module~common_mod: module~darcy_mod:->module~mpi_mod: module~darcy_mod:->module~solver_mod: module~darcy_mod:->module~io_mod: module~solver_mod:->module~common_mod: module~solver_mod:->module~mpi_mod: iso_fortran_env iso_fortran_env module~solver_mod:->iso_fortran_env: ieee_arithmetic ieee_arithmetic module~solver_mod:->ieee_arithmetic: module~dec_mod:->module~common_mod: module~dec_mod:->module~mpi_mod: module~dec_mod:->module~math_mod: module~io_mod:->module~common_mod: module~io_mod:->module~mpi_mod: module~io_mod:->module~solver_mod: module~io_mod:->iso_fortran_env: module~io_mod:->ieee_arithmetic: module~io_mod:->module~dec_mod: module~math_mod:->module~common_mod: module~math_mod:->module~mpi_mod: Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Miniapp to solve one-field Darcy flow in parallel using PETSc KSP Calls program~~darcy_1f~~CallsGraph program~darcy_1f darcy_1f proc~end_mpi end_mpi program~darcy_1f:->proc~end_mpi: proc~syncwrite_log syncwrite_log program~darcy_1f:->proc~syncwrite_log: proc~rootwrite_log rootwrite_log program~darcy_1f:->proc~rootwrite_log: proc~start_mpi start_mpi program~darcy_1f:->proc~start_mpi: proc~start_petsc start_petsc program~darcy_1f:->proc~start_petsc: proc~get_rhs_darcy get_RHS_darcy program~darcy_1f:->proc~get_rhs_darcy: proc~get_lhs_darcy get_LHS_darcy program~darcy_1f:->proc~get_lhs_darcy: calc_whitney_c2_bc calc_whitney_c2_bc program~darcy_1f:->calc_whitney_c2_bc: proc~parallel_setup parallel_setup program~darcy_1f:->proc~parallel_setup: proc~read_input_darcy read_input_darcy program~darcy_1f:->proc~read_input_darcy: proc~finalise_darcy finalise_darcy program~darcy_1f:->proc~finalise_darcy: proc~clean_up clean_up program~darcy_1f:->proc~clean_up: proc~initialise_darcy initialise_darcy program~darcy_1f:->proc~initialise_darcy: proc~solve_ksp solve_KSP program~darcy_1f:->proc~solve_ksp: proc~end_petsc end_petsc program~darcy_1f:->proc~end_petsc: initialise_geo initialise_geo program~darcy_1f:->initialise_geo: write_solution_d0s write_solution_d0s program~darcy_1f:->write_solution_d0s: proc~end_mpi:->proc~syncwrite_log: proc~syncwrite_log_time syncwrite_log_time proc~end_mpi:->proc~syncwrite_log_time: mpi_finalize mpi_finalize proc~end_mpi:->mpi_finalize: proc~close_log close_log proc~end_mpi:->proc~close_log: mpi_barrier mpi_barrier proc~syncwrite_log:->mpi_barrier: proc~start_mpi:->proc~end_mpi: proc~syncwrite_log_mpidata syncwrite_log_mpidata proc~start_mpi:->proc~syncwrite_log_mpidata: proc~start_mpi:->mpi_barrier: mpi_wtime mpi_wtime proc~start_mpi:->mpi_wtime: mpi_comm_size mpi_comm_size proc~start_mpi:->mpi_comm_size: proc~open_log open_log proc~start_mpi:->proc~open_log: mpi_init mpi_init proc~start_mpi:->mpi_init: mpi_comm_rank mpi_comm_rank proc~start_mpi:->mpi_comm_rank: proc~start_petsc:->proc~end_mpi: proc~start_petsc:->proc~syncwrite_log: petscinitialize petscinitialize proc~start_petsc:->petscinitialize: proc~get_rhs_darcy:->proc~syncwrite_log: bc_press bc_press proc~get_rhs_darcy:->bc_press: proc~get_rhs_darcy:->proc~syncwrite_log_time: num_elm num_elm proc~get_rhs_darcy:->num_elm: proc~read_bndry_cond2 read_bndry_cond2 proc~get_rhs_darcy:->proc~read_bndry_cond2: vecassemblyend vecassemblyend proc~get_rhs_darcy:->vecassemblyend: lcl_complex lcl_complex proc~get_rhs_darcy:->lcl_complex: proc~get_rhs_darcy:->mpi_wtime: glb_num_elm glb_num_elm proc~get_rhs_darcy:->glb_num_elm: mpi_reduce mpi_reduce proc~get_rhs_darcy:->mpi_reduce: vecassemblybegin vecassemblybegin proc~get_rhs_darcy:->vecassemblybegin: proc~get_lhs_darcy:->proc~syncwrite_log: proc~get_lhs_darcy:->proc~syncwrite_log_time: matsetoption matsetoption proc~get_lhs_darcy:->matsetoption: proc~get_lhs_darcy:->num_elm: matassemblybegin matassemblybegin proc~get_lhs_darcy:->matassemblybegin: proc~get_lhs_darcy:->lcl_complex: proc~get_lhs_darcy:->mpi_wtime: matassemblyend matassemblyend proc~get_lhs_darcy:->matassemblyend: proc~get_lhs_darcy:->glb_num_elm: proc~parallel_setup:->proc~syncwrite_log: proc~parallel_setup:->proc~syncwrite_log_time: proc~exchange_prml_nodes exchange_prml_nodes proc~parallel_setup:->proc~exchange_prml_nodes: proc~get_glb_indx_dual_vlm get_glb_indx_dual_vlm proc~parallel_setup:->proc~get_glb_indx_dual_vlm: proc~read_prml_elms2 read_prml_elms2 proc~parallel_setup:->proc~read_prml_elms2: proc~partition_dual partition_dual proc~parallel_setup:->proc~partition_dual: proc~read_nodes_prml read_nodes_prml proc~parallel_setup:->proc~read_nodes_prml: proc~get_connectivity get_connectivity proc~parallel_setup:->proc~get_connectivity: proc~calc_bndry_cobndry calc_bndry_cobndry proc~parallel_setup:->proc~calc_bndry_cobndry: proc~get_glb_indx get_glb_indx proc~parallel_setup:->proc~get_glb_indx: proc~read_input_darcy:->proc~end_mpi: proc~read_input_darcy:->proc~syncwrite_log: proc~read_input_darcy:->proc~rootwrite_log: proc~check_param_darcy check_param_darcy proc~read_input_darcy:->proc~check_param_darcy: vecdestroy vecdestroy proc~finalise_darcy:->vecdestroy: matdestroy matdestroy proc~finalise_darcy:->matdestroy: kspdestroy kspdestroy proc~finalise_darcy:->kspdestroy: proc~initialise_darcy:->proc~syncwrite_log: matsetsizes matsetsizes proc~initialise_darcy:->matsetsizes: proc~initialise_darcy:->proc~syncwrite_log_time: vecgetvalues vecgetvalues proc~initialise_darcy:->vecgetvalues: proc~initialise_darcy:->vecdestroy: proc~initialise_darcy:->num_elm: matcreate matcreate proc~initialise_darcy:->matcreate: proc~initialise_darcy:->vecassemblyend: veccreatempi veccreatempi proc~initialise_darcy:->veccreatempi: vecsetfromoptions vecsetfromoptions proc~initialise_darcy:->vecsetfromoptions: proc~initialise_darcy:->lcl_complex: matsetfromoptions matsetfromoptions proc~initialise_darcy:->matsetfromoptions: proc~initialise_darcy:->mpi_wtime: proc~initialise_darcy:->glb_num_elm: vecgetownershiprange vecgetownershiprange proc~initialise_darcy:->vecgetownershiprange: vecduplicate vecduplicate proc~initialise_darcy:->vecduplicate: proc~initialise_darcy:->vecassemblybegin: proc~solve_ksp:->proc~syncwrite_log: kspgetresidualnorm kspgetresidualnorm proc~solve_ksp:->kspgetresidualnorm: kspsolve kspsolve proc~solve_ksp:->kspsolve: kspcreate kspcreate proc~solve_ksp:->kspcreate: kspsetup kspsetup proc~solve_ksp:->kspsetup: pcfactorsetmatorderingtype pcfactorsetmatorderingtype proc~solve_ksp:->pcfactorsetmatorderingtype: pcsettype pcsettype proc~solve_ksp:->pcsettype: kspmonitorset kspmonitorset proc~solve_ksp:->kspmonitorset: kspview kspview proc~solve_ksp:->kspview: petscviewerandformatcreate petscviewerandformatcreate proc~solve_ksp:->petscviewerandformatcreate: kspsettolerances kspsettolerances proc~solve_ksp:->kspsettolerances: kspsetoperators kspsetoperators proc~solve_ksp:->kspsetoperators: proc~solve_ksp:->mpi_wtime: kspsetfromoptions kspsetfromoptions proc~solve_ksp:->kspsetfromoptions: kspsettype kspsettype proc~solve_ksp:->kspsettype: kspgetiterationnumber kspgetiterationnumber proc~solve_ksp:->kspgetiterationnumber: kspgetpc kspgetpc proc~solve_ksp:->kspgetpc: petscfinalize petscfinalize proc~end_petsc:->petscfinalize: proc~syncwrite_log_mpidata:->mpi_barrier: proc~syncwrite_log_time:->proc~rootwrite_log: proc~syncwrite_log_time:->mpi_barrier: proc~syncwrite_log_time:->mpi_wtime: proc~exchange_prml_nodes:->mpi_barrier: proc~exchange_prml_nodes:->lcl_complex: mpi_waitall mpi_waitall proc~exchange_prml_nodes:->mpi_waitall: mpi_request_free mpi_request_free proc~exchange_prml_nodes:->mpi_request_free: num_send num_send proc~exchange_prml_nodes:->num_send: adj_proc adj_proc proc~exchange_prml_nodes:->adj_proc: num_recv num_recv proc~exchange_prml_nodes:->num_recv: proc~get_glb_indx_dual_vlm:->num_elm: proc~get_glb_indx_dual_vlm:->lcl_complex: proc~read_prml_elms2:->mpi_barrier: proc~read_prml_elms2:->num_elm: proc~read_prml_elms2:->lcl_complex: proc~read_prml_elms2:->glb_num_elm: proc~elm2proc elm2proc proc~read_prml_elms2:->proc~elm2proc: indx_offset indx_offset proc~read_prml_elms2:->indx_offset: proc~root_open_file_read root_open_file_read proc~read_prml_elms2:->proc~root_open_file_read: mpi_bcast mpi_bcast proc~read_prml_elms2:->mpi_bcast: num_pelm_pp num_pelm_pp proc~read_prml_elms2:->num_pelm_pp: proc~partition_dual:->num_elm: proc~partition_dual:->lcl_complex: proc~partition_dual:->glb_num_elm: proc~partition_dual:->indx_offset: proc~partition_dual:->mpi_bcast: proc~partition_dual:->num_pelm_pp: proc~read_nodes_prml:->num_elm: proc~read_nodes_prml:->lcl_complex: proc~read_nodes_prml:->glb_num_elm: mpi_recv mpi_recv proc~read_nodes_prml:->mpi_recv: proc~read_nodes_prml:->indx_offset: proc~read_nodes_prml:->proc~root_open_file_read: proc~read_nodes_prml:->mpi_bcast: idnint idnint proc~read_nodes_prml:->idnint: proc~read_nodes_prml:->num_pelm_pp: mpi_send mpi_send proc~read_nodes_prml:->mpi_send: proc~read_bndry_cond2:->mpi_barrier: proc~read_bndry_cond2:->num_elm: proc~read_bndry_cond2:->lcl_complex: proc~read_bndry_cond2:->glb_num_elm: proc~read_bndry_cond2:->proc~root_open_file_read: proc~read_bndry_cond2:->mpi_bcast: proc~get_connectivity:->proc~rootwrite_log: proc~get_connectivity:->mpi_barrier: proc~get_connectivity:->num_elm: proc~get_connectivity:->lcl_complex: proc~get_connectivity:->mpi_waitall: proc~get_connectivity:->mpi_request_free: proc~get_connectivity:->num_send: proc~get_connectivity:->adj_proc: proc~index_in_list index_in_list proc~get_connectivity:->proc~index_in_list: proc~get_connectivity:->num_recv: proc~calc_bndry_cobndry:->proc~syncwrite_log: proc~calc_bndry_cobndry:->proc~syncwrite_log_time: proc~calc_bndry_cobndry:->num_elm: proc~count_bndry_cobndry count_bndry_cobndry proc~calc_bndry_cobndry:->proc~count_bndry_cobndry: proc~allocate_bndry_cobndry allocate_bndry_cobndry proc~calc_bndry_cobndry:->proc~allocate_bndry_cobndry: proc~build_bndry_work_array build_bndry_work_array proc~calc_bndry_cobndry:->proc~build_bndry_work_array: proc~set_bndry_cobndry set_bndry_cobndry proc~calc_bndry_cobndry:->proc~set_bndry_cobndry: proc~get_glb_indx:->num_elm: proc~get_glb_indx:->lcl_complex: proc~read_glb_indx2 read_glb_indx2 proc~get_glb_indx:->proc~read_glb_indx2: proc~exchange_glb_indx exchange_glb_indx proc~get_glb_indx:->proc~exchange_glb_indx: proc~open_log:->mpi_bcast: proc~check_param_darcy:->proc~rootwrite_log: proc~count_bndry_cobndry:->num_elm: proc~count_bndry_cobndry:->lcl_complex: proc~any_element_in_list any_element_in_list proc~count_bndry_cobndry:->proc~any_element_in_list: proc~elm2proc:->num_pelm_pp: proc~root_open_file_read:->mpi_bcast: proc~read_glb_indx2:->proc~end_mpi: proc~read_glb_indx2:->mpi_barrier: proc~read_glb_indx2:->num_elm: proc~read_glb_indx2:->lcl_complex: proc~read_glb_indx2:->glb_num_elm: proc~read_glb_indx2:->proc~elm2proc: proc~read_glb_indx2:->indx_offset: proc~read_glb_indx2:->proc~root_open_file_read: proc~read_glb_indx2:->mpi_bcast: proc~allocate_bndry_cobndry:->num_elm: proc~allocate_bndry_cobndry:->lcl_complex: proc~build_bndry_work_array:->num_elm: proc~build_bndry_work_array:->lcl_complex: proc~set_bndry_cobndry:->num_elm: proc~set_bndry_cobndry:->lcl_complex: proc~set_bndry_cobndry:->proc~elm2proc: proc~exchange_glb_indx:->mpi_barrier: proc~exchange_glb_indx:->lcl_complex: proc~exchange_glb_indx:->mpi_waitall: proc~exchange_glb_indx:->mpi_request_free: proc~exchange_glb_indx:->num_send: proc~exchange_glb_indx:->adj_proc: proc~exchange_glb_indx:->num_recv: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents None","tags":"","loc":"program/darcy_1f.html","title":"darcy_1f – ParaGEMS"},{"text":"Uses common_mod mpi_mod partition_mod darcy_mod solver_mod program~~darcy~~UsesGraph program~darcy darcy module~common_mod common_mod program~darcy:->module~common_mod: module~mpi_mod mpi_mod program~darcy:->module~mpi_mod: module~partition_mod partition_mod program~darcy:->module~partition_mod: module~darcy_mod darcy_mod program~darcy:->module~darcy_mod: module~solver_mod solver_mod program~darcy:->module~solver_mod: petsc petsc module~common_mod:->petsc: module~mpi_mod:->module~common_mod: module~partition_mod:->module~common_mod: module~partition_mod:->module~mpi_mod: module~dec_mod dec_mod module~partition_mod:->module~dec_mod: module~io_mod io_mod module~partition_mod:->module~io_mod: module~math_mod math_mod module~partition_mod:->module~math_mod: module~darcy_mod:->module~common_mod: module~darcy_mod:->module~mpi_mod: module~darcy_mod:->module~solver_mod: module~darcy_mod:->module~io_mod: module~solver_mod:->module~common_mod: module~solver_mod:->module~mpi_mod: iso_fortran_env iso_fortran_env module~solver_mod:->iso_fortran_env: ieee_arithmetic ieee_arithmetic module~solver_mod:->ieee_arithmetic: module~dec_mod:->module~common_mod: module~dec_mod:->module~mpi_mod: module~dec_mod:->module~math_mod: module~io_mod:->module~common_mod: module~io_mod:->module~mpi_mod: module~io_mod:->module~solver_mod: module~io_mod:->iso_fortran_env: module~io_mod:->ieee_arithmetic: module~io_mod:->module~dec_mod: module~math_mod:->module~common_mod: module~math_mod:->module~mpi_mod: Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Miniapp to solve two-field Darcy flow in parallel using PETSc KSP Calls program~~darcy~~CallsGraph program~darcy darcy proc~end_mpi end_mpi program~darcy:->proc~end_mpi: proc~syncwrite_log syncwrite_log program~darcy:->proc~syncwrite_log: proc~rootwrite_log rootwrite_log program~darcy:->proc~rootwrite_log: proc~start_mpi start_mpi program~darcy:->proc~start_mpi: proc~start_petsc start_petsc program~darcy:->proc~start_petsc: proc~get_rhs_darcy get_RHS_darcy program~darcy:->proc~get_rhs_darcy: proc~get_lhs_darcy get_LHS_darcy program~darcy:->proc~get_lhs_darcy: calc_whitney_c2_bc calc_whitney_c2_bc program~darcy:->calc_whitney_c2_bc: proc~parallel_setup parallel_setup program~darcy:->proc~parallel_setup: proc~read_input_darcy read_input_darcy program~darcy:->proc~read_input_darcy: proc~finalise_darcy finalise_darcy program~darcy:->proc~finalise_darcy: proc~clean_up clean_up program~darcy:->proc~clean_up: proc~initialise_darcy initialise_darcy program~darcy:->proc~initialise_darcy: proc~solve_ksp solve_KSP program~darcy:->proc~solve_ksp: proc~end_petsc end_petsc program~darcy:->proc~end_petsc: initialise_geo initialise_geo program~darcy:->initialise_geo: write_solution_d0s write_solution_d0s program~darcy:->write_solution_d0s: proc~end_mpi:->proc~syncwrite_log: proc~syncwrite_log_time syncwrite_log_time proc~end_mpi:->proc~syncwrite_log_time: mpi_finalize mpi_finalize proc~end_mpi:->mpi_finalize: proc~close_log close_log proc~end_mpi:->proc~close_log: mpi_barrier mpi_barrier proc~syncwrite_log:->mpi_barrier: proc~start_mpi:->proc~end_mpi: proc~syncwrite_log_mpidata syncwrite_log_mpidata proc~start_mpi:->proc~syncwrite_log_mpidata: proc~start_mpi:->mpi_barrier: mpi_wtime mpi_wtime proc~start_mpi:->mpi_wtime: mpi_comm_size mpi_comm_size proc~start_mpi:->mpi_comm_size: proc~open_log open_log proc~start_mpi:->proc~open_log: mpi_init mpi_init proc~start_mpi:->mpi_init: mpi_comm_rank mpi_comm_rank proc~start_mpi:->mpi_comm_rank: proc~start_petsc:->proc~end_mpi: proc~start_petsc:->proc~syncwrite_log: petscinitialize petscinitialize proc~start_petsc:->petscinitialize: proc~get_rhs_darcy:->proc~syncwrite_log: bc_press bc_press proc~get_rhs_darcy:->bc_press: proc~get_rhs_darcy:->proc~syncwrite_log_time: num_elm num_elm proc~get_rhs_darcy:->num_elm: proc~read_bndry_cond2 read_bndry_cond2 proc~get_rhs_darcy:->proc~read_bndry_cond2: vecassemblyend vecassemblyend proc~get_rhs_darcy:->vecassemblyend: lcl_complex lcl_complex proc~get_rhs_darcy:->lcl_complex: proc~get_rhs_darcy:->mpi_wtime: glb_num_elm glb_num_elm proc~get_rhs_darcy:->glb_num_elm: mpi_reduce mpi_reduce proc~get_rhs_darcy:->mpi_reduce: vecassemblybegin vecassemblybegin proc~get_rhs_darcy:->vecassemblybegin: proc~get_lhs_darcy:->proc~syncwrite_log: proc~get_lhs_darcy:->proc~syncwrite_log_time: matsetoption matsetoption proc~get_lhs_darcy:->matsetoption: proc~get_lhs_darcy:->num_elm: matassemblybegin matassemblybegin proc~get_lhs_darcy:->matassemblybegin: proc~get_lhs_darcy:->lcl_complex: proc~get_lhs_darcy:->mpi_wtime: matassemblyend matassemblyend proc~get_lhs_darcy:->matassemblyend: proc~get_lhs_darcy:->glb_num_elm: proc~parallel_setup:->proc~syncwrite_log: proc~parallel_setup:->proc~syncwrite_log_time: proc~exchange_prml_nodes exchange_prml_nodes proc~parallel_setup:->proc~exchange_prml_nodes: proc~get_glb_indx_dual_vlm get_glb_indx_dual_vlm proc~parallel_setup:->proc~get_glb_indx_dual_vlm: proc~read_prml_elms2 read_prml_elms2 proc~parallel_setup:->proc~read_prml_elms2: proc~partition_dual partition_dual proc~parallel_setup:->proc~partition_dual: proc~read_nodes_prml read_nodes_prml proc~parallel_setup:->proc~read_nodes_prml: proc~get_connectivity get_connectivity proc~parallel_setup:->proc~get_connectivity: proc~calc_bndry_cobndry calc_bndry_cobndry proc~parallel_setup:->proc~calc_bndry_cobndry: proc~get_glb_indx get_glb_indx proc~parallel_setup:->proc~get_glb_indx: proc~read_input_darcy:->proc~end_mpi: proc~read_input_darcy:->proc~syncwrite_log: proc~read_input_darcy:->proc~rootwrite_log: proc~check_param_darcy check_param_darcy proc~read_input_darcy:->proc~check_param_darcy: vecdestroy vecdestroy proc~finalise_darcy:->vecdestroy: matdestroy matdestroy proc~finalise_darcy:->matdestroy: kspdestroy kspdestroy proc~finalise_darcy:->kspdestroy: proc~initialise_darcy:->proc~syncwrite_log: matsetsizes matsetsizes proc~initialise_darcy:->matsetsizes: proc~initialise_darcy:->proc~syncwrite_log_time: vecgetvalues vecgetvalues proc~initialise_darcy:->vecgetvalues: proc~initialise_darcy:->vecdestroy: proc~initialise_darcy:->num_elm: matcreate matcreate proc~initialise_darcy:->matcreate: proc~initialise_darcy:->vecassemblyend: veccreatempi veccreatempi proc~initialise_darcy:->veccreatempi: vecsetfromoptions vecsetfromoptions proc~initialise_darcy:->vecsetfromoptions: proc~initialise_darcy:->lcl_complex: matsetfromoptions matsetfromoptions proc~initialise_darcy:->matsetfromoptions: proc~initialise_darcy:->mpi_wtime: proc~initialise_darcy:->glb_num_elm: vecgetownershiprange vecgetownershiprange proc~initialise_darcy:->vecgetownershiprange: vecduplicate vecduplicate proc~initialise_darcy:->vecduplicate: proc~initialise_darcy:->vecassemblybegin: proc~solve_ksp:->proc~syncwrite_log: kspgetresidualnorm kspgetresidualnorm proc~solve_ksp:->kspgetresidualnorm: kspsolve kspsolve proc~solve_ksp:->kspsolve: kspcreate kspcreate proc~solve_ksp:->kspcreate: kspsetup kspsetup proc~solve_ksp:->kspsetup: pcfactorsetmatorderingtype pcfactorsetmatorderingtype proc~solve_ksp:->pcfactorsetmatorderingtype: pcsettype pcsettype proc~solve_ksp:->pcsettype: kspmonitorset kspmonitorset proc~solve_ksp:->kspmonitorset: kspview kspview proc~solve_ksp:->kspview: petscviewerandformatcreate petscviewerandformatcreate proc~solve_ksp:->petscviewerandformatcreate: kspsettolerances kspsettolerances proc~solve_ksp:->kspsettolerances: kspsetoperators kspsetoperators proc~solve_ksp:->kspsetoperators: proc~solve_ksp:->mpi_wtime: kspsetfromoptions kspsetfromoptions proc~solve_ksp:->kspsetfromoptions: kspsettype kspsettype proc~solve_ksp:->kspsettype: kspgetiterationnumber kspgetiterationnumber proc~solve_ksp:->kspgetiterationnumber: kspgetpc kspgetpc proc~solve_ksp:->kspgetpc: petscfinalize petscfinalize proc~end_petsc:->petscfinalize: proc~syncwrite_log_mpidata:->mpi_barrier: proc~syncwrite_log_time:->proc~rootwrite_log: proc~syncwrite_log_time:->mpi_barrier: proc~syncwrite_log_time:->mpi_wtime: proc~exchange_prml_nodes:->mpi_barrier: proc~exchange_prml_nodes:->lcl_complex: mpi_waitall mpi_waitall proc~exchange_prml_nodes:->mpi_waitall: mpi_request_free mpi_request_free proc~exchange_prml_nodes:->mpi_request_free: num_send num_send proc~exchange_prml_nodes:->num_send: adj_proc adj_proc proc~exchange_prml_nodes:->adj_proc: num_recv num_recv proc~exchange_prml_nodes:->num_recv: proc~get_glb_indx_dual_vlm:->num_elm: proc~get_glb_indx_dual_vlm:->lcl_complex: proc~read_prml_elms2:->mpi_barrier: proc~read_prml_elms2:->num_elm: proc~read_prml_elms2:->lcl_complex: proc~read_prml_elms2:->glb_num_elm: proc~elm2proc elm2proc proc~read_prml_elms2:->proc~elm2proc: indx_offset indx_offset proc~read_prml_elms2:->indx_offset: proc~root_open_file_read root_open_file_read proc~read_prml_elms2:->proc~root_open_file_read: mpi_bcast mpi_bcast proc~read_prml_elms2:->mpi_bcast: num_pelm_pp num_pelm_pp proc~read_prml_elms2:->num_pelm_pp: proc~partition_dual:->num_elm: proc~partition_dual:->lcl_complex: proc~partition_dual:->glb_num_elm: proc~partition_dual:->indx_offset: proc~partition_dual:->mpi_bcast: proc~partition_dual:->num_pelm_pp: proc~read_nodes_prml:->num_elm: proc~read_nodes_prml:->lcl_complex: proc~read_nodes_prml:->glb_num_elm: mpi_recv mpi_recv proc~read_nodes_prml:->mpi_recv: proc~read_nodes_prml:->indx_offset: proc~read_nodes_prml:->proc~root_open_file_read: proc~read_nodes_prml:->mpi_bcast: idnint idnint proc~read_nodes_prml:->idnint: proc~read_nodes_prml:->num_pelm_pp: mpi_send mpi_send proc~read_nodes_prml:->mpi_send: proc~read_bndry_cond2:->mpi_barrier: proc~read_bndry_cond2:->num_elm: proc~read_bndry_cond2:->lcl_complex: proc~read_bndry_cond2:->glb_num_elm: proc~read_bndry_cond2:->proc~root_open_file_read: proc~read_bndry_cond2:->mpi_bcast: proc~get_connectivity:->proc~rootwrite_log: proc~get_connectivity:->mpi_barrier: proc~get_connectivity:->num_elm: proc~get_connectivity:->lcl_complex: proc~get_connectivity:->mpi_waitall: proc~get_connectivity:->mpi_request_free: proc~get_connectivity:->num_send: proc~get_connectivity:->adj_proc: proc~index_in_list index_in_list proc~get_connectivity:->proc~index_in_list: proc~get_connectivity:->num_recv: proc~calc_bndry_cobndry:->proc~syncwrite_log: proc~calc_bndry_cobndry:->proc~syncwrite_log_time: proc~calc_bndry_cobndry:->num_elm: proc~count_bndry_cobndry count_bndry_cobndry proc~calc_bndry_cobndry:->proc~count_bndry_cobndry: proc~allocate_bndry_cobndry allocate_bndry_cobndry proc~calc_bndry_cobndry:->proc~allocate_bndry_cobndry: proc~build_bndry_work_array build_bndry_work_array proc~calc_bndry_cobndry:->proc~build_bndry_work_array: proc~set_bndry_cobndry set_bndry_cobndry proc~calc_bndry_cobndry:->proc~set_bndry_cobndry: proc~get_glb_indx:->num_elm: proc~get_glb_indx:->lcl_complex: proc~read_glb_indx2 read_glb_indx2 proc~get_glb_indx:->proc~read_glb_indx2: proc~exchange_glb_indx exchange_glb_indx proc~get_glb_indx:->proc~exchange_glb_indx: proc~open_log:->mpi_bcast: proc~check_param_darcy:->proc~rootwrite_log: proc~count_bndry_cobndry:->num_elm: proc~count_bndry_cobndry:->lcl_complex: proc~any_element_in_list any_element_in_list proc~count_bndry_cobndry:->proc~any_element_in_list: proc~elm2proc:->num_pelm_pp: proc~root_open_file_read:->mpi_bcast: proc~read_glb_indx2:->proc~end_mpi: proc~read_glb_indx2:->mpi_barrier: proc~read_glb_indx2:->num_elm: proc~read_glb_indx2:->lcl_complex: proc~read_glb_indx2:->glb_num_elm: proc~read_glb_indx2:->proc~elm2proc: proc~read_glb_indx2:->indx_offset: proc~read_glb_indx2:->proc~root_open_file_read: proc~read_glb_indx2:->mpi_bcast: proc~allocate_bndry_cobndry:->num_elm: proc~allocate_bndry_cobndry:->lcl_complex: proc~build_bndry_work_array:->num_elm: proc~build_bndry_work_array:->lcl_complex: proc~set_bndry_cobndry:->num_elm: proc~set_bndry_cobndry:->lcl_complex: proc~set_bndry_cobndry:->proc~elm2proc: proc~exchange_glb_indx:->mpi_barrier: proc~exchange_glb_indx:->lcl_complex: proc~exchange_glb_indx:->mpi_waitall: proc~exchange_glb_indx:->mpi_request_free: proc~exchange_glb_indx:->num_send: proc~exchange_glb_indx:->adj_proc: proc~exchange_glb_indx:->num_recv: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents None","tags":"","loc":"program/darcy.html","title":"darcy – ParaGEMS"},{"text":"Uses common_mod mpi_mod partition_mod darcy_mod solver_mod program~~darcy_crkp_2f~~UsesGraph program~darcy_crkp_2f darcy_crkp_2f module~common_mod common_mod program~darcy_crkp_2f:->module~common_mod: module~mpi_mod mpi_mod program~darcy_crkp_2f:->module~mpi_mod: module~partition_mod partition_mod program~darcy_crkp_2f:->module~partition_mod: module~darcy_mod darcy_mod program~darcy_crkp_2f:->module~darcy_mod: module~solver_mod solver_mod program~darcy_crkp_2f:->module~solver_mod: petsc petsc module~common_mod:->petsc: module~mpi_mod:->module~common_mod: module~partition_mod:->module~common_mod: module~partition_mod:->module~mpi_mod: module~dec_mod dec_mod module~partition_mod:->module~dec_mod: module~io_mod io_mod module~partition_mod:->module~io_mod: module~math_mod math_mod module~partition_mod:->module~math_mod: module~darcy_mod:->module~common_mod: module~darcy_mod:->module~mpi_mod: module~darcy_mod:->module~solver_mod: module~darcy_mod:->module~io_mod: module~solver_mod:->module~common_mod: module~solver_mod:->module~mpi_mod: iso_fortran_env iso_fortran_env module~solver_mod:->iso_fortran_env: ieee_arithmetic ieee_arithmetic module~solver_mod:->ieee_arithmetic: module~dec_mod:->module~common_mod: module~dec_mod:->module~mpi_mod: module~dec_mod:->module~math_mod: module~io_mod:->module~common_mod: module~io_mod:->module~mpi_mod: module~io_mod:->module~solver_mod: module~io_mod:->iso_fortran_env: module~io_mod:->ieee_arithmetic: module~io_mod:->module~dec_mod: module~math_mod:->module~common_mod: module~math_mod:->module~mpi_mod: Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Miniapp to solve two-field Darcy flow in parallel using PETSc KSP with cracking Calls program~~darcy_crkp_2f~~CallsGraph program~darcy_crkp_2f darcy_crkp_2f proc~end_mpi end_mpi program~darcy_crkp_2f:->proc~end_mpi: proc~open_unsteady_log open_unsteady_log program~darcy_crkp_2f:->proc~open_unsteady_log: write_unsteady_d0s2 write_unsteady_d0s2 program~darcy_crkp_2f:->write_unsteady_d0s2: proc~rootwrite_log rootwrite_log program~darcy_crkp_2f:->proc~rootwrite_log: initialise_geo initialise_geo program~darcy_crkp_2f:->initialise_geo: proc~start_mpi start_mpi program~darcy_crkp_2f:->proc~start_mpi: write_pressure_matlab write_pressure_matlab program~darcy_crkp_2f:->write_pressure_matlab: proc~initialise_darcy2 initialise_darcy2 program~darcy_crkp_2f:->proc~initialise_darcy2: write_centers_matlab write_centers_matlab program~darcy_crkp_2f:->write_centers_matlab: proc~get_rhs_darcy2 get_RHS_darcy2 program~darcy_crkp_2f:->proc~get_rhs_darcy2: proc~clean_up clean_up program~darcy_crkp_2f:->proc~clean_up: proc~solve_ksp2 solve_KSP2 program~darcy_crkp_2f:->proc~solve_ksp2: proc~end_petsc end_petsc program~darcy_crkp_2f:->proc~end_petsc: proc~parallel_setup parallel_setup program~darcy_crkp_2f:->proc~parallel_setup: proc~get_lhs_darcy2 get_LHS_darcy2 program~darcy_crkp_2f:->proc~get_lhs_darcy2: calc_whitney_c2_bc calc_whitney_c2_bc program~darcy_crkp_2f:->calc_whitney_c2_bc: proc~read_input_darcy read_input_darcy program~darcy_crkp_2f:->proc~read_input_darcy: proc~finalise_darcy finalise_darcy program~darcy_crkp_2f:->proc~finalise_darcy: write_solution_matlab write_solution_matlab program~darcy_crkp_2f:->write_solution_matlab: proc~extract_sol_ksp2 extract_sol_KSP2 program~darcy_crkp_2f:->proc~extract_sol_ksp2: proc~syncwrite_log syncwrite_log program~darcy_crkp_2f:->proc~syncwrite_log: proc~start_petsc start_petsc program~darcy_crkp_2f:->proc~start_petsc: proc~identify_crack5 identify_crack5 program~darcy_crkp_2f:->proc~identify_crack5: proc~identify_crack7 identify_crack7 program~darcy_crkp_2f:->proc~identify_crack7: proc~identify_crack6 identify_crack6 program~darcy_crkp_2f:->proc~identify_crack6: proc~close_unsteady_log close_unsteady_log program~darcy_crkp_2f:->proc~close_unsteady_log: proc~identify_crack4 identify_crack4 program~darcy_crkp_2f:->proc~identify_crack4: write_prml_volumes_matlab write_prml_volumes_matlab program~darcy_crkp_2f:->write_prml_volumes_matlab: proc~end_mpi:->proc~syncwrite_log: proc~syncwrite_log_time syncwrite_log_time proc~end_mpi:->proc~syncwrite_log_time: mpi_finalize mpi_finalize proc~end_mpi:->mpi_finalize: proc~close_log close_log proc~end_mpi:->proc~close_log: mpi_bcast mpi_bcast proc~open_unsteady_log:->mpi_bcast: proc~start_mpi:->proc~end_mpi: proc~syncwrite_log_mpidata syncwrite_log_mpidata proc~start_mpi:->proc~syncwrite_log_mpidata: mpi_barrier mpi_barrier proc~start_mpi:->mpi_barrier: mpi_comm_rank mpi_comm_rank proc~start_mpi:->mpi_comm_rank: mpi_wtime mpi_wtime proc~start_mpi:->mpi_wtime: mpi_comm_size mpi_comm_size proc~start_mpi:->mpi_comm_size: proc~open_log open_log proc~start_mpi:->proc~open_log: mpi_init mpi_init proc~start_mpi:->mpi_init: proc~initialise_darcy2:->proc~syncwrite_log: proc~initialise_darcy2:->proc~syncwrite_log_time: num_elm num_elm proc~initialise_darcy2:->num_elm: lcl_complex lcl_complex proc~initialise_darcy2:->lcl_complex: proc~initialise_darcy2:->mpi_wtime: proc~get_rhs_darcy2:->proc~syncwrite_log: bc_press bc_press proc~get_rhs_darcy2:->bc_press: proc~get_rhs_darcy2:->proc~syncwrite_log_time: proc~get_rhs_darcy2:->num_elm: matcreatenest matcreatenest proc~get_rhs_darcy2:->matcreatenest: proc~read_bndry_cond2 read_bndry_cond2 proc~get_rhs_darcy2:->proc~read_bndry_cond2: proc~get_rhs_darcy2:->lcl_complex: matgetownershiprange matgetownershiprange proc~get_rhs_darcy2:->matgetownershiprange: vecsetfromoptions vecsetfromoptions proc~get_rhs_darcy2:->vecsetfromoptions: vecassemblyend vecassemblyend proc~get_rhs_darcy2:->vecassemblyend: proc~get_rhs_darcy2:->mpi_wtime: glb_num_elm glb_num_elm proc~get_rhs_darcy2:->glb_num_elm: asub asub proc~get_rhs_darcy2:->asub: vecdestroy vecdestroy proc~get_rhs_darcy2:->vecdestroy: vecduplicate vecduplicate proc~get_rhs_darcy2:->vecduplicate: isg isg proc~get_rhs_darcy2:->isg: vecassemblybegin vecassemblybegin proc~get_rhs_darcy2:->vecassemblybegin: proc~solve_ksp2:->proc~syncwrite_log: kspgetconvergedreason kspgetconvergedreason proc~solve_ksp2:->kspgetconvergedreason: kspsolve kspsolve proc~solve_ksp2:->kspsolve: kspcreate kspcreate proc~solve_ksp2:->kspcreate: kspsetup kspsetup proc~solve_ksp2:->kspsetup: kspview kspview proc~solve_ksp2:->kspview: sub2pc_id sub2pc_id proc~solve_ksp2:->sub2pc_id: sub2ksp_id sub2ksp_id proc~solve_ksp2:->sub2ksp_id: kspgetpc kspgetpc proc~solve_ksp2:->kspgetpc: subksp_id subksp_id proc~solve_ksp2:->subksp_id: kspgetresidualnorm kspgetresidualnorm proc~solve_ksp2:->kspgetresidualnorm: pcsettype pcsettype proc~solve_ksp2:->pcsettype: subpc_id subpc_id proc~solve_ksp2:->subpc_id: kspmonitorset kspmonitorset proc~solve_ksp2:->kspmonitorset: petscviewerandformatcreate petscviewerandformatcreate proc~solve_ksp2:->petscviewerandformatcreate: kspsettolerances kspsettolerances proc~solve_ksp2:->kspsettolerances: kspsetoperators kspsetoperators proc~solve_ksp2:->kspsetoperators: pcsetup pcsetup proc~solve_ksp2:->pcsetup: proc~solve_ksp2:->mpi_wtime: kspsetfromoptions kspsetfromoptions proc~solve_ksp2:->kspsetfromoptions: kspsettype kspsettype proc~solve_ksp2:->kspsettype: pcfieldsplitsetschurfacttype pcfieldsplitsetschurfacttype proc~solve_ksp2:->pcfieldsplitsetschurfacttype: kspgetiterationnumber kspgetiterationnumber proc~solve_ksp2:->kspgetiterationnumber: pcfieldsplitsetschurpre pcfieldsplitsetschurpre proc~solve_ksp2:->pcfieldsplitsetschurpre: pcfieldsplitgetsubksp pcfieldsplitgetsubksp proc~solve_ksp2:->pcfieldsplitgetsubksp: pcfieldsplitsettype pcfieldsplitsettype proc~solve_ksp2:->pcfieldsplitsettype: proc~solve_ksp2:->isg: kspdestroy kspdestroy proc~solve_ksp2:->kspdestroy: petscfinalize petscfinalize proc~end_petsc:->petscfinalize: proc~parallel_setup:->proc~syncwrite_log: proc~parallel_setup:->proc~syncwrite_log_time: proc~exchange_prml_nodes exchange_prml_nodes proc~parallel_setup:->proc~exchange_prml_nodes: proc~get_glb_indx_dual_vlm get_glb_indx_dual_vlm proc~parallel_setup:->proc~get_glb_indx_dual_vlm: proc~read_prml_elms2 read_prml_elms2 proc~parallel_setup:->proc~read_prml_elms2: proc~partition_dual partition_dual proc~parallel_setup:->proc~partition_dual: proc~read_nodes_prml read_nodes_prml proc~parallel_setup:->proc~read_nodes_prml: proc~get_connectivity get_connectivity proc~parallel_setup:->proc~get_connectivity: proc~calc_bndry_cobndry calc_bndry_cobndry proc~parallel_setup:->proc~calc_bndry_cobndry: proc~get_glb_indx get_glb_indx proc~parallel_setup:->proc~get_glb_indx: proc~get_lhs_darcy2:->proc~syncwrite_log: proc~get_lhs_darcy2:->proc~syncwrite_log_time: proc~get_lhs_darcy2:->num_elm: proc~get_lhs_darcy2:->matcreatenest: proc~get_lhs_darcy2:->lcl_complex: matnestgetiss matnestgetiss proc~get_lhs_darcy2:->matnestgetiss: proc~get_lhs_darcy2:->mpi_wtime: proc~get_lhs_darcy2:->glb_num_elm: matsetoption matsetoption proc~get_lhs_darcy2:->matsetoption: proc~get_lhs_darcy2:->asub: proc~read_input_darcy:->proc~end_mpi: proc~read_input_darcy:->proc~rootwrite_log: proc~read_input_darcy:->proc~syncwrite_log: proc~check_param_darcy check_param_darcy proc~read_input_darcy:->proc~check_param_darcy: matdestroy matdestroy proc~finalise_darcy:->matdestroy: proc~finalise_darcy:->vecdestroy: proc~finalise_darcy:->kspdestroy: proc~extract_sol_ksp2:->proc~syncwrite_log_time: proc~extract_sol_ksp2:->num_elm: isdestroy isdestroy proc~extract_sol_ksp2:->isdestroy: vecscatterbegin vecscatterbegin proc~extract_sol_ksp2:->vecscatterbegin: vecscattercreate vecscattercreate proc~extract_sol_ksp2:->vecscattercreate: proc~extract_sol_ksp2:->lcl_complex: vecscatterend vecscatterend proc~extract_sol_ksp2:->vecscatterend: vecscatterdestroy vecscatterdestroy proc~extract_sol_ksp2:->vecscatterdestroy: proc~extract_sol_ksp2:->vecdestroy: proc~extract_sol_ksp2:->isg: proc~syncwrite_log:->mpi_barrier: proc~start_petsc:->proc~end_mpi: proc~start_petsc:->proc~syncwrite_log: petscinitialize petscinitialize proc~start_petsc:->petscinitialize: proc~identify_crack5:->proc~syncwrite_log: proc~identify_crack5:->proc~syncwrite_log_time: proc~identify_crack5:->num_elm: proc~identify_crack5:->lcl_complex: proc~identify_crack5:->vecassemblyend: proc~identify_crack5:->mpi_wtime: proc~identify_crack5:->asub: mpi_reduce mpi_reduce proc~identify_crack5:->mpi_reduce: proc~identify_crack5:->vecassemblybegin: proc~identify_crack7:->proc~syncwrite_log: proc~identify_crack7:->proc~syncwrite_log_time: proc~identify_crack7:->num_elm: proc~identify_crack7:->lcl_complex: proc~identify_crack7:->vecassemblyend: proc~identify_crack7:->mpi_wtime: proc~identify_crack7:->asub: proc~identify_crack7:->mpi_reduce: proc~identify_crack7:->vecassemblybegin: proc~identify_crack6:->proc~syncwrite_log: proc~identify_crack6:->proc~syncwrite_log_time: proc~identify_crack6:->num_elm: mpi_allreduce mpi_allreduce proc~identify_crack6:->mpi_allreduce: proc~identify_crack6:->lcl_complex: proc~identify_crack6:->mpi_bcast: proc~identify_crack6:->vecassemblyend: proc~identify_crack6:->mpi_wtime: proc~identify_crack6:->glb_num_elm: vecsetvalues vecsetvalues proc~identify_crack6:->vecsetvalues: proc~identify_crack6:->asub: proc~identify_crack6:->mpi_reduce: proc~identify_crack6:->vecassemblybegin: proc~identify_crack4:->proc~syncwrite_log: proc~identify_crack4:->proc~syncwrite_log_time: proc~identify_crack4:->num_elm: proc~identify_crack4:->mpi_allreduce: proc~identify_crack4:->lcl_complex: proc~identify_crack4:->vecassemblyend: proc~identify_crack4:->mpi_wtime: proc~identify_crack4:->asub: proc~identify_crack4:->vecassemblybegin: proc~syncwrite_log_mpidata:->mpi_barrier: proc~syncwrite_log_time:->proc~rootwrite_log: proc~syncwrite_log_time:->mpi_barrier: proc~syncwrite_log_time:->mpi_wtime: proc~exchange_prml_nodes:->mpi_barrier: proc~exchange_prml_nodes:->lcl_complex: mpi_waitall mpi_waitall proc~exchange_prml_nodes:->mpi_waitall: mpi_request_free mpi_request_free proc~exchange_prml_nodes:->mpi_request_free: num_send num_send proc~exchange_prml_nodes:->num_send: adj_proc adj_proc proc~exchange_prml_nodes:->adj_proc: num_recv num_recv proc~exchange_prml_nodes:->num_recv: proc~get_glb_indx_dual_vlm:->num_elm: proc~get_glb_indx_dual_vlm:->lcl_complex: proc~check_param_darcy:->proc~rootwrite_log: proc~read_prml_elms2:->mpi_barrier: proc~read_prml_elms2:->num_elm: proc~read_prml_elms2:->lcl_complex: proc~read_prml_elms2:->mpi_bcast: proc~read_prml_elms2:->glb_num_elm: proc~elm2proc elm2proc proc~read_prml_elms2:->proc~elm2proc: indx_offset indx_offset proc~read_prml_elms2:->indx_offset: proc~root_open_file_read root_open_file_read proc~read_prml_elms2:->proc~root_open_file_read: num_pelm_pp num_pelm_pp proc~read_prml_elms2:->num_pelm_pp: proc~partition_dual:->num_elm: proc~partition_dual:->lcl_complex: proc~partition_dual:->mpi_bcast: proc~partition_dual:->glb_num_elm: proc~partition_dual:->indx_offset: proc~partition_dual:->num_pelm_pp: proc~read_nodes_prml:->num_elm: proc~read_nodes_prml:->lcl_complex: proc~read_nodes_prml:->mpi_bcast: proc~read_nodes_prml:->glb_num_elm: mpi_recv mpi_recv proc~read_nodes_prml:->mpi_recv: proc~read_nodes_prml:->indx_offset: proc~read_nodes_prml:->proc~root_open_file_read: idnint idnint proc~read_nodes_prml:->idnint: proc~read_nodes_prml:->num_pelm_pp: mpi_send mpi_send proc~read_nodes_prml:->mpi_send: proc~read_bndry_cond2:->mpi_barrier: proc~read_bndry_cond2:->num_elm: proc~read_bndry_cond2:->lcl_complex: proc~read_bndry_cond2:->mpi_bcast: proc~read_bndry_cond2:->glb_num_elm: proc~read_bndry_cond2:->proc~root_open_file_read: proc~get_connectivity:->proc~rootwrite_log: proc~get_connectivity:->mpi_barrier: proc~get_connectivity:->num_elm: proc~get_connectivity:->lcl_complex: proc~get_connectivity:->mpi_waitall: proc~get_connectivity:->mpi_request_free: proc~get_connectivity:->num_send: proc~get_connectivity:->adj_proc: proc~index_in_list index_in_list proc~get_connectivity:->proc~index_in_list: proc~get_connectivity:->num_recv: proc~calc_bndry_cobndry:->proc~syncwrite_log: proc~calc_bndry_cobndry:->proc~syncwrite_log_time: proc~calc_bndry_cobndry:->num_elm: proc~count_bndry_cobndry count_bndry_cobndry proc~calc_bndry_cobndry:->proc~count_bndry_cobndry: proc~allocate_bndry_cobndry allocate_bndry_cobndry proc~calc_bndry_cobndry:->proc~allocate_bndry_cobndry: proc~build_bndry_work_array build_bndry_work_array proc~calc_bndry_cobndry:->proc~build_bndry_work_array: proc~set_bndry_cobndry set_bndry_cobndry proc~calc_bndry_cobndry:->proc~set_bndry_cobndry: proc~get_glb_indx:->num_elm: proc~get_glb_indx:->lcl_complex: proc~read_glb_indx2 read_glb_indx2 proc~get_glb_indx:->proc~read_glb_indx2: proc~exchange_glb_indx exchange_glb_indx proc~get_glb_indx:->proc~exchange_glb_indx: proc~open_log:->mpi_bcast: proc~count_bndry_cobndry:->num_elm: proc~count_bndry_cobndry:->lcl_complex: proc~any_element_in_list any_element_in_list proc~count_bndry_cobndry:->proc~any_element_in_list: proc~elm2proc:->num_pelm_pp: proc~root_open_file_read:->mpi_bcast: proc~allocate_bndry_cobndry:->num_elm: proc~allocate_bndry_cobndry:->lcl_complex: proc~read_glb_indx2:->proc~end_mpi: proc~read_glb_indx2:->mpi_barrier: proc~read_glb_indx2:->num_elm: proc~read_glb_indx2:->lcl_complex: proc~read_glb_indx2:->mpi_bcast: proc~read_glb_indx2:->glb_num_elm: proc~read_glb_indx2:->proc~elm2proc: proc~read_glb_indx2:->indx_offset: proc~read_glb_indx2:->proc~root_open_file_read: proc~build_bndry_work_array:->num_elm: proc~build_bndry_work_array:->lcl_complex: proc~set_bndry_cobndry:->num_elm: proc~set_bndry_cobndry:->lcl_complex: proc~set_bndry_cobndry:->proc~elm2proc: proc~exchange_glb_indx:->mpi_barrier: proc~exchange_glb_indx:->lcl_complex: proc~exchange_glb_indx:->mpi_waitall: proc~exchange_glb_indx:->mpi_request_free: proc~exchange_glb_indx:->num_send: proc~exchange_glb_indx:->adj_proc: proc~exchange_glb_indx:->num_recv: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables fname msg crck_id i exit_cond Variables Type Attributes Name Initial character(len=slen) :: fname character(len=slen) :: msg integer :: crck_id integer :: i logical :: exit_cond","tags":"","loc":"program/darcy_crkp_2f.html","title":"darcy_crkp_2f – ParaGEMS"},{"text":"Uses common_mod mpi_mod partition_mod darcy_mod solver_mod program~~darcy_crkp_2f~2~~UsesGraph program~darcy_crkp_2f~2 darcy_crkp_2f module~common_mod common_mod program~darcy_crkp_2f~2:->module~common_mod: module~mpi_mod mpi_mod program~darcy_crkp_2f~2:->module~mpi_mod: module~partition_mod partition_mod program~darcy_crkp_2f~2:->module~partition_mod: module~darcy_mod darcy_mod program~darcy_crkp_2f~2:->module~darcy_mod: module~solver_mod solver_mod program~darcy_crkp_2f~2:->module~solver_mod: petsc petsc module~common_mod:->petsc: module~mpi_mod:->module~common_mod: module~partition_mod:->module~common_mod: module~partition_mod:->module~mpi_mod: module~dec_mod dec_mod module~partition_mod:->module~dec_mod: module~io_mod io_mod module~partition_mod:->module~io_mod: module~math_mod math_mod module~partition_mod:->module~math_mod: module~darcy_mod:->module~common_mod: module~darcy_mod:->module~mpi_mod: module~darcy_mod:->module~solver_mod: module~darcy_mod:->module~io_mod: module~solver_mod:->module~common_mod: module~solver_mod:->module~mpi_mod: iso_fortran_env iso_fortran_env module~solver_mod:->iso_fortran_env: ieee_arithmetic ieee_arithmetic module~solver_mod:->ieee_arithmetic: module~dec_mod:->module~common_mod: module~dec_mod:->module~mpi_mod: module~dec_mod:->module~math_mod: module~io_mod:->module~common_mod: module~io_mod:->module~mpi_mod: module~io_mod:->module~solver_mod: module~io_mod:->iso_fortran_env: module~io_mod:->ieee_arithmetic: module~io_mod:->module~dec_mod: module~math_mod:->module~common_mod: module~math_mod:->module~mpi_mod: Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Miniapp to solve two-field Darcy flow in parallel using PETSc KSP with cracking Calls program~~darcy_crkp_2f~2~~CallsGraph program~darcy_crkp_2f~2 darcy_crkp_2f proc~end_mpi end_mpi program~darcy_crkp_2f~2:->proc~end_mpi: proc~extract_sol_ksp extract_sol_KSP program~darcy_crkp_2f~2:->proc~extract_sol_ksp: proc~open_unsteady_log open_unsteady_log program~darcy_crkp_2f~2:->proc~open_unsteady_log: write_centers_matlab2 write_centers_matlab2 program~darcy_crkp_2f~2:->write_centers_matlab2: proc~rootwrite_log rootwrite_log program~darcy_crkp_2f~2:->proc~rootwrite_log: initialise_geo initialise_geo program~darcy_crkp_2f~2:->initialise_geo: proc~solve_ksp solve_KSP program~darcy_crkp_2f~2:->proc~solve_ksp: proc~start_mpi start_mpi program~darcy_crkp_2f~2:->proc~start_mpi: write_solution_matlab2 write_solution_matlab2 program~darcy_crkp_2f~2:->write_solution_matlab2: proc~clean_up clean_up program~darcy_crkp_2f~2:->proc~clean_up: proc~end_petsc end_petsc program~darcy_crkp_2f~2:->proc~end_petsc: proc~parallel_setup parallel_setup program~darcy_crkp_2f~2:->proc~parallel_setup: proc~get_rhs_darcy get_RHS_darcy program~darcy_crkp_2f~2:->proc~get_rhs_darcy: calc_whitney_c2_bc calc_whitney_c2_bc program~darcy_crkp_2f~2:->calc_whitney_c2_bc: proc~identify_crack identify_crack program~darcy_crkp_2f~2:->proc~identify_crack: proc~read_input_darcy read_input_darcy program~darcy_crkp_2f~2:->proc~read_input_darcy: proc~finalise_darcy finalise_darcy program~darcy_crkp_2f~2:->proc~finalise_darcy: proc~initialise_darcy initialise_darcy program~darcy_crkp_2f~2:->proc~initialise_darcy: write_pressure_matlab2 write_pressure_matlab2 program~darcy_crkp_2f~2:->write_pressure_matlab2: write_prml_volumes_matlab2 write_prml_volumes_matlab2 program~darcy_crkp_2f~2:->write_prml_volumes_matlab2: proc~syncwrite_log syncwrite_log program~darcy_crkp_2f~2:->proc~syncwrite_log: proc~start_petsc start_petsc program~darcy_crkp_2f~2:->proc~start_petsc: proc~get_lhs_darcy get_LHS_darcy program~darcy_crkp_2f~2:->proc~get_lhs_darcy: write_unsteady_d0s write_unsteady_d0s program~darcy_crkp_2f~2:->write_unsteady_d0s: proc~close_unsteady_log close_unsteady_log program~darcy_crkp_2f~2:->proc~close_unsteady_log: proc~identify_crack3 identify_crack3 program~darcy_crkp_2f~2:->proc~identify_crack3: proc~end_mpi:->proc~syncwrite_log: proc~syncwrite_log_time syncwrite_log_time proc~end_mpi:->proc~syncwrite_log_time: mpi_finalize mpi_finalize proc~end_mpi:->mpi_finalize: proc~close_log close_log proc~end_mpi:->proc~close_log: proc~extract_sol_ksp:->proc~syncwrite_log_time: num_elm num_elm proc~extract_sol_ksp:->num_elm: isdestroy isdestroy proc~extract_sol_ksp:->isdestroy: vecscatterend vecscatterend proc~extract_sol_ksp:->vecscatterend: lcl_complex lcl_complex proc~extract_sol_ksp:->lcl_complex: vecscatterdestroy vecscatterdestroy proc~extract_sol_ksp:->vecscatterdestroy: vecscattercreate vecscattercreate proc~extract_sol_ksp:->vecscattercreate: glb_num_elm glb_num_elm proc~extract_sol_ksp:->glb_num_elm: vecscatterbegin vecscatterbegin proc~extract_sol_ksp:->vecscatterbegin: vecdestroy vecdestroy proc~extract_sol_ksp:->vecdestroy: mpi_bcast mpi_bcast proc~open_unsteady_log:->mpi_bcast: proc~solve_ksp:->proc~syncwrite_log: kspgetresidualnorm kspgetresidualnorm proc~solve_ksp:->kspgetresidualnorm: kspcreate kspcreate proc~solve_ksp:->kspcreate: kspsetup kspsetup proc~solve_ksp:->kspsetup: kspview kspview proc~solve_ksp:->kspview: pcfactorsetmatorderingtype pcfactorsetmatorderingtype proc~solve_ksp:->pcfactorsetmatorderingtype: kspsettype kspsettype proc~solve_ksp:->kspsettype: kspsolve kspsolve proc~solve_ksp:->kspsolve: pcsettype pcsettype proc~solve_ksp:->pcsettype: kspmonitorset kspmonitorset proc~solve_ksp:->kspmonitorset: kspsetoperators kspsetoperators proc~solve_ksp:->kspsetoperators: petscviewerandformatcreate petscviewerandformatcreate proc~solve_ksp:->petscviewerandformatcreate: kspsettolerances kspsettolerances proc~solve_ksp:->kspsettolerances: mpi_wtime mpi_wtime proc~solve_ksp:->mpi_wtime: kspsetfromoptions kspsetfromoptions proc~solve_ksp:->kspsetfromoptions: kspgetpc kspgetpc proc~solve_ksp:->kspgetpc: kspgetiterationnumber kspgetiterationnumber proc~solve_ksp:->kspgetiterationnumber: proc~start_mpi:->proc~end_mpi: proc~syncwrite_log_mpidata syncwrite_log_mpidata proc~start_mpi:->proc~syncwrite_log_mpidata: mpi_barrier mpi_barrier proc~start_mpi:->mpi_barrier: proc~open_log open_log proc~start_mpi:->proc~open_log: mpi_comm_rank mpi_comm_rank proc~start_mpi:->mpi_comm_rank: proc~start_mpi:->mpi_wtime: mpi_comm_size mpi_comm_size proc~start_mpi:->mpi_comm_size: mpi_init mpi_init proc~start_mpi:->mpi_init: petscfinalize petscfinalize proc~end_petsc:->petscfinalize: proc~parallel_setup:->proc~syncwrite_log: proc~parallel_setup:->proc~syncwrite_log_time: proc~exchange_prml_nodes exchange_prml_nodes proc~parallel_setup:->proc~exchange_prml_nodes: proc~get_glb_indx_dual_vlm get_glb_indx_dual_vlm proc~parallel_setup:->proc~get_glb_indx_dual_vlm: proc~read_prml_elms2 read_prml_elms2 proc~parallel_setup:->proc~read_prml_elms2: proc~partition_dual partition_dual proc~parallel_setup:->proc~partition_dual: proc~read_nodes_prml read_nodes_prml proc~parallel_setup:->proc~read_nodes_prml: proc~get_connectivity get_connectivity proc~parallel_setup:->proc~get_connectivity: proc~calc_bndry_cobndry calc_bndry_cobndry proc~parallel_setup:->proc~calc_bndry_cobndry: proc~get_glb_indx get_glb_indx proc~parallel_setup:->proc~get_glb_indx: proc~get_rhs_darcy:->proc~syncwrite_log: bc_press bc_press proc~get_rhs_darcy:->bc_press: proc~get_rhs_darcy:->proc~syncwrite_log_time: proc~get_rhs_darcy:->num_elm: vecassemblyend vecassemblyend proc~get_rhs_darcy:->vecassemblyend: proc~get_rhs_darcy:->lcl_complex: proc~get_rhs_darcy:->mpi_wtime: proc~get_rhs_darcy:->glb_num_elm: vecassemblybegin vecassemblybegin proc~get_rhs_darcy:->vecassemblybegin: proc~read_bndry_cond2 read_bndry_cond2 proc~get_rhs_darcy:->proc~read_bndry_cond2: mpi_reduce mpi_reduce proc~get_rhs_darcy:->mpi_reduce: proc~identify_crack:->proc~syncwrite_log: proc~identify_crack:->proc~syncwrite_log_time: proc~identify_crack:->num_elm: mpi_allreduce mpi_allreduce proc~identify_crack:->mpi_allreduce: proc~identify_crack:->vecassemblyend: proc~identify_crack:->lcl_complex: proc~identify_crack:->mpi_wtime: proc~identify_crack:->vecassemblybegin: proc~read_input_darcy:->proc~end_mpi: proc~read_input_darcy:->proc~rootwrite_log: proc~read_input_darcy:->proc~syncwrite_log: proc~check_param_darcy check_param_darcy proc~read_input_darcy:->proc~check_param_darcy: matdestroy matdestroy proc~finalise_darcy:->matdestroy: proc~finalise_darcy:->vecdestroy: kspdestroy kspdestroy proc~finalise_darcy:->kspdestroy: proc~initialise_darcy:->proc~syncwrite_log: matcreate matcreate proc~initialise_darcy:->matcreate: proc~initialise_darcy:->proc~syncwrite_log_time: vecgetvalues vecgetvalues proc~initialise_darcy:->vecgetvalues: proc~initialise_darcy:->num_elm: proc~initialise_darcy:->vecassemblyend: veccreatempi veccreatempi proc~initialise_darcy:->veccreatempi: vecsetfromoptions vecsetfromoptions proc~initialise_darcy:->vecsetfromoptions: proc~initialise_darcy:->lcl_complex: proc~initialise_darcy:->mpi_wtime: proc~initialise_darcy:->glb_num_elm: matsetsizes matsetsizes proc~initialise_darcy:->matsetsizes: vecduplicate vecduplicate proc~initialise_darcy:->vecduplicate: proc~initialise_darcy:->vecdestroy: proc~initialise_darcy:->vecassemblybegin: vecgetownershiprange vecgetownershiprange proc~initialise_darcy:->vecgetownershiprange: matsetfromoptions matsetfromoptions proc~initialise_darcy:->matsetfromoptions: proc~syncwrite_log:->mpi_barrier: proc~start_petsc:->proc~end_mpi: proc~start_petsc:->proc~syncwrite_log: petscinitialize petscinitialize proc~start_petsc:->petscinitialize: proc~get_lhs_darcy:->proc~syncwrite_log: proc~get_lhs_darcy:->proc~syncwrite_log_time: matsetoption matsetoption proc~get_lhs_darcy:->matsetoption: proc~get_lhs_darcy:->num_elm: matassemblybegin matassemblybegin proc~get_lhs_darcy:->matassemblybegin: proc~get_lhs_darcy:->lcl_complex: proc~get_lhs_darcy:->mpi_wtime: matassemblyend matassemblyend proc~get_lhs_darcy:->matassemblyend: proc~get_lhs_darcy:->glb_num_elm: proc~identify_crack3:->proc~syncwrite_log: proc~identify_crack3:->proc~syncwrite_log_time: proc~identify_crack3:->num_elm: proc~identify_crack3:->vecassemblyend: proc~identify_crack3:->lcl_complex: proc~identify_crack3:->mpi_wtime: proc~identify_crack3:->vecassemblybegin: matzerorowscolumns matzerorowscolumns proc~identify_crack3:->matzerorowscolumns: proc~identify_crack3:->mpi_reduce: proc~syncwrite_log_mpidata:->mpi_barrier: proc~syncwrite_log_time:->proc~rootwrite_log: proc~syncwrite_log_time:->mpi_barrier: proc~syncwrite_log_time:->mpi_wtime: proc~exchange_prml_nodes:->mpi_barrier: proc~exchange_prml_nodes:->lcl_complex: mpi_waitall mpi_waitall proc~exchange_prml_nodes:->mpi_waitall: mpi_request_free mpi_request_free proc~exchange_prml_nodes:->mpi_request_free: num_send num_send proc~exchange_prml_nodes:->num_send: adj_proc adj_proc proc~exchange_prml_nodes:->adj_proc: num_recv num_recv proc~exchange_prml_nodes:->num_recv: proc~get_glb_indx_dual_vlm:->num_elm: proc~get_glb_indx_dual_vlm:->lcl_complex: proc~open_log:->mpi_bcast: proc~read_prml_elms2:->mpi_barrier: proc~read_prml_elms2:->num_elm: proc~read_prml_elms2:->mpi_bcast: proc~read_prml_elms2:->lcl_complex: proc~read_prml_elms2:->glb_num_elm: proc~elm2proc elm2proc proc~read_prml_elms2:->proc~elm2proc: indx_offset indx_offset proc~read_prml_elms2:->indx_offset: proc~root_open_file_read root_open_file_read proc~read_prml_elms2:->proc~root_open_file_read: num_pelm_pp num_pelm_pp proc~read_prml_elms2:->num_pelm_pp: proc~partition_dual:->num_elm: proc~partition_dual:->mpi_bcast: proc~partition_dual:->lcl_complex: proc~partition_dual:->glb_num_elm: proc~partition_dual:->indx_offset: proc~partition_dual:->num_pelm_pp: proc~read_nodes_prml:->num_elm: proc~read_nodes_prml:->mpi_bcast: proc~read_nodes_prml:->lcl_complex: proc~read_nodes_prml:->glb_num_elm: mpi_recv mpi_recv proc~read_nodes_prml:->mpi_recv: proc~read_nodes_prml:->indx_offset: proc~read_nodes_prml:->proc~root_open_file_read: idnint idnint proc~read_nodes_prml:->idnint: proc~read_nodes_prml:->num_pelm_pp: mpi_send mpi_send proc~read_nodes_prml:->mpi_send: proc~get_connectivity:->proc~rootwrite_log: proc~get_connectivity:->mpi_barrier: proc~get_connectivity:->num_elm: proc~get_connectivity:->lcl_complex: proc~get_connectivity:->mpi_waitall: proc~get_connectivity:->mpi_request_free: proc~get_connectivity:->num_send: proc~get_connectivity:->adj_proc: proc~index_in_list index_in_list proc~get_connectivity:->proc~index_in_list: proc~get_connectivity:->num_recv: proc~calc_bndry_cobndry:->proc~syncwrite_log: proc~calc_bndry_cobndry:->proc~syncwrite_log_time: proc~calc_bndry_cobndry:->num_elm: proc~count_bndry_cobndry count_bndry_cobndry proc~calc_bndry_cobndry:->proc~count_bndry_cobndry: proc~allocate_bndry_cobndry allocate_bndry_cobndry proc~calc_bndry_cobndry:->proc~allocate_bndry_cobndry: proc~build_bndry_work_array build_bndry_work_array proc~calc_bndry_cobndry:->proc~build_bndry_work_array: proc~set_bndry_cobndry set_bndry_cobndry proc~calc_bndry_cobndry:->proc~set_bndry_cobndry: proc~get_glb_indx:->num_elm: proc~get_glb_indx:->lcl_complex: proc~read_glb_indx2 read_glb_indx2 proc~get_glb_indx:->proc~read_glb_indx2: proc~exchange_glb_indx exchange_glb_indx proc~get_glb_indx:->proc~exchange_glb_indx: proc~check_param_darcy:->proc~rootwrite_log: proc~read_bndry_cond2:->mpi_barrier: proc~read_bndry_cond2:->num_elm: proc~read_bndry_cond2:->mpi_bcast: proc~read_bndry_cond2:->lcl_complex: proc~read_bndry_cond2:->glb_num_elm: proc~read_bndry_cond2:->proc~root_open_file_read: proc~count_bndry_cobndry:->num_elm: proc~count_bndry_cobndry:->lcl_complex: proc~any_element_in_list any_element_in_list proc~count_bndry_cobndry:->proc~any_element_in_list: proc~elm2proc:->num_pelm_pp: proc~root_open_file_read:->mpi_bcast: proc~allocate_bndry_cobndry:->num_elm: proc~allocate_bndry_cobndry:->lcl_complex: proc~read_glb_indx2:->proc~end_mpi: proc~read_glb_indx2:->mpi_barrier: proc~read_glb_indx2:->num_elm: proc~read_glb_indx2:->mpi_bcast: proc~read_glb_indx2:->lcl_complex: proc~read_glb_indx2:->glb_num_elm: proc~read_glb_indx2:->proc~elm2proc: proc~read_glb_indx2:->indx_offset: proc~read_glb_indx2:->proc~root_open_file_read: proc~build_bndry_work_array:->num_elm: proc~build_bndry_work_array:->lcl_complex: proc~set_bndry_cobndry:->num_elm: proc~set_bndry_cobndry:->lcl_complex: proc~set_bndry_cobndry:->proc~elm2proc: proc~exchange_glb_indx:->mpi_barrier: proc~exchange_glb_indx:->lcl_complex: proc~exchange_glb_indx:->mpi_waitall: proc~exchange_glb_indx:->mpi_request_free: proc~exchange_glb_indx:->num_send: proc~exchange_glb_indx:->adj_proc: proc~exchange_glb_indx:->num_recv: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Variables fname msg crck_id exit_cond Variables Type Attributes Name Initial character(len=slen) :: fname character(len=slen) :: msg integer :: crck_id logical :: exit_cond","tags":"","loc":"program/darcy_crkp_2f~2.html","title":"darcy_crkp_2f – ParaGEMS"},{"text":"Uses common_mod mpi_mod partition_mod darcy_mod solver_mod program~~darcy_crkp_1f~~UsesGraph program~darcy_crkp_1f darcy_crkp_1f module~common_mod common_mod program~darcy_crkp_1f:->module~common_mod: module~mpi_mod mpi_mod program~darcy_crkp_1f:->module~mpi_mod: module~partition_mod partition_mod program~darcy_crkp_1f:->module~partition_mod: module~darcy_mod darcy_mod program~darcy_crkp_1f:->module~darcy_mod: module~solver_mod solver_mod program~darcy_crkp_1f:->module~solver_mod: petsc petsc module~common_mod:->petsc: module~mpi_mod:->module~common_mod: module~partition_mod:->module~common_mod: module~partition_mod:->module~mpi_mod: module~dec_mod dec_mod module~partition_mod:->module~dec_mod: module~io_mod io_mod module~partition_mod:->module~io_mod: module~math_mod math_mod module~partition_mod:->module~math_mod: module~darcy_mod:->module~common_mod: module~darcy_mod:->module~mpi_mod: module~darcy_mod:->module~solver_mod: module~darcy_mod:->module~io_mod: module~solver_mod:->module~common_mod: module~solver_mod:->module~mpi_mod: iso_fortran_env iso_fortran_env module~solver_mod:->iso_fortran_env: ieee_arithmetic ieee_arithmetic module~solver_mod:->ieee_arithmetic: module~dec_mod:->module~common_mod: module~dec_mod:->module~mpi_mod: module~dec_mod:->module~math_mod: module~io_mod:->module~common_mod: module~io_mod:->module~mpi_mod: module~io_mod:->module~solver_mod: module~io_mod:->iso_fortran_env: module~io_mod:->ieee_arithmetic: module~io_mod:->module~dec_mod: module~math_mod:->module~common_mod: module~math_mod:->module~mpi_mod: Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Miniapp to solve one-field Darcy flow in parallel using PETSc KSP with cracking Calls program~~darcy_crkp_1f~~CallsGraph program~darcy_crkp_1f darcy_crkp_1f proc~end_mpi end_mpi program~darcy_crkp_1f:->proc~end_mpi: proc~syncwrite_log syncwrite_log program~darcy_crkp_1f:->proc~syncwrite_log: proc~rootwrite_log rootwrite_log program~darcy_crkp_1f:->proc~rootwrite_log: proc~start_mpi start_mpi program~darcy_crkp_1f:->proc~start_mpi: proc~start_petsc start_petsc program~darcy_crkp_1f:->proc~start_petsc: proc~get_rhs_darcy get_RHS_darcy program~darcy_crkp_1f:->proc~get_rhs_darcy: proc~get_lhs_darcy get_LHS_darcy program~darcy_crkp_1f:->proc~get_lhs_darcy: calc_whitney_c2_bc calc_whitney_c2_bc program~darcy_crkp_1f:->calc_whitney_c2_bc: proc~parallel_setup parallel_setup program~darcy_crkp_1f:->proc~parallel_setup: proc~read_input_darcy read_input_darcy program~darcy_crkp_1f:->proc~read_input_darcy: proc~finalise_darcy finalise_darcy program~darcy_crkp_1f:->proc~finalise_darcy: proc~clean_up clean_up program~darcy_crkp_1f:->proc~clean_up: proc~initialise_darcy initialise_darcy program~darcy_crkp_1f:->proc~initialise_darcy: proc~solve_ksp solve_KSP program~darcy_crkp_1f:->proc~solve_ksp: proc~end_petsc end_petsc program~darcy_crkp_1f:->proc~end_petsc: initialise_geo initialise_geo program~darcy_crkp_1f:->initialise_geo: write_solution_d0s write_solution_d0s program~darcy_crkp_1f:->write_solution_d0s: proc~end_mpi:->proc~syncwrite_log: proc~syncwrite_log_time syncwrite_log_time proc~end_mpi:->proc~syncwrite_log_time: mpi_finalize mpi_finalize proc~end_mpi:->mpi_finalize: proc~close_log close_log proc~end_mpi:->proc~close_log: mpi_barrier mpi_barrier proc~syncwrite_log:->mpi_barrier: proc~start_mpi:->proc~end_mpi: proc~syncwrite_log_mpidata syncwrite_log_mpidata proc~start_mpi:->proc~syncwrite_log_mpidata: proc~start_mpi:->mpi_barrier: mpi_wtime mpi_wtime proc~start_mpi:->mpi_wtime: mpi_comm_size mpi_comm_size proc~start_mpi:->mpi_comm_size: proc~open_log open_log proc~start_mpi:->proc~open_log: mpi_init mpi_init proc~start_mpi:->mpi_init: mpi_comm_rank mpi_comm_rank proc~start_mpi:->mpi_comm_rank: proc~start_petsc:->proc~end_mpi: proc~start_petsc:->proc~syncwrite_log: petscinitialize petscinitialize proc~start_petsc:->petscinitialize: proc~get_rhs_darcy:->proc~syncwrite_log: bc_press bc_press proc~get_rhs_darcy:->bc_press: proc~get_rhs_darcy:->proc~syncwrite_log_time: num_elm num_elm proc~get_rhs_darcy:->num_elm: proc~read_bndry_cond2 read_bndry_cond2 proc~get_rhs_darcy:->proc~read_bndry_cond2: vecassemblyend vecassemblyend proc~get_rhs_darcy:->vecassemblyend: lcl_complex lcl_complex proc~get_rhs_darcy:->lcl_complex: proc~get_rhs_darcy:->mpi_wtime: glb_num_elm glb_num_elm proc~get_rhs_darcy:->glb_num_elm: mpi_reduce mpi_reduce proc~get_rhs_darcy:->mpi_reduce: vecassemblybegin vecassemblybegin proc~get_rhs_darcy:->vecassemblybegin: proc~get_lhs_darcy:->proc~syncwrite_log: proc~get_lhs_darcy:->proc~syncwrite_log_time: matsetoption matsetoption proc~get_lhs_darcy:->matsetoption: proc~get_lhs_darcy:->num_elm: matassemblybegin matassemblybegin proc~get_lhs_darcy:->matassemblybegin: proc~get_lhs_darcy:->lcl_complex: proc~get_lhs_darcy:->mpi_wtime: matassemblyend matassemblyend proc~get_lhs_darcy:->matassemblyend: proc~get_lhs_darcy:->glb_num_elm: proc~parallel_setup:->proc~syncwrite_log: proc~parallel_setup:->proc~syncwrite_log_time: proc~exchange_prml_nodes exchange_prml_nodes proc~parallel_setup:->proc~exchange_prml_nodes: proc~get_glb_indx_dual_vlm get_glb_indx_dual_vlm proc~parallel_setup:->proc~get_glb_indx_dual_vlm: proc~read_prml_elms2 read_prml_elms2 proc~parallel_setup:->proc~read_prml_elms2: proc~partition_dual partition_dual proc~parallel_setup:->proc~partition_dual: proc~read_nodes_prml read_nodes_prml proc~parallel_setup:->proc~read_nodes_prml: proc~get_connectivity get_connectivity proc~parallel_setup:->proc~get_connectivity: proc~calc_bndry_cobndry calc_bndry_cobndry proc~parallel_setup:->proc~calc_bndry_cobndry: proc~get_glb_indx get_glb_indx proc~parallel_setup:->proc~get_glb_indx: proc~read_input_darcy:->proc~end_mpi: proc~read_input_darcy:->proc~syncwrite_log: proc~read_input_darcy:->proc~rootwrite_log: proc~check_param_darcy check_param_darcy proc~read_input_darcy:->proc~check_param_darcy: vecdestroy vecdestroy proc~finalise_darcy:->vecdestroy: matdestroy matdestroy proc~finalise_darcy:->matdestroy: kspdestroy kspdestroy proc~finalise_darcy:->kspdestroy: proc~initialise_darcy:->proc~syncwrite_log: matsetsizes matsetsizes proc~initialise_darcy:->matsetsizes: proc~initialise_darcy:->proc~syncwrite_log_time: vecgetvalues vecgetvalues proc~initialise_darcy:->vecgetvalues: proc~initialise_darcy:->vecdestroy: proc~initialise_darcy:->num_elm: matcreate matcreate proc~initialise_darcy:->matcreate: proc~initialise_darcy:->vecassemblyend: veccreatempi veccreatempi proc~initialise_darcy:->veccreatempi: vecsetfromoptions vecsetfromoptions proc~initialise_darcy:->vecsetfromoptions: proc~initialise_darcy:->lcl_complex: matsetfromoptions matsetfromoptions proc~initialise_darcy:->matsetfromoptions: proc~initialise_darcy:->mpi_wtime: proc~initialise_darcy:->glb_num_elm: vecgetownershiprange vecgetownershiprange proc~initialise_darcy:->vecgetownershiprange: vecduplicate vecduplicate proc~initialise_darcy:->vecduplicate: proc~initialise_darcy:->vecassemblybegin: proc~solve_ksp:->proc~syncwrite_log: kspgetresidualnorm kspgetresidualnorm proc~solve_ksp:->kspgetresidualnorm: kspsolve kspsolve proc~solve_ksp:->kspsolve: kspcreate kspcreate proc~solve_ksp:->kspcreate: kspsetup kspsetup proc~solve_ksp:->kspsetup: pcfactorsetmatorderingtype pcfactorsetmatorderingtype proc~solve_ksp:->pcfactorsetmatorderingtype: pcsettype pcsettype proc~solve_ksp:->pcsettype: kspmonitorset kspmonitorset proc~solve_ksp:->kspmonitorset: kspview kspview proc~solve_ksp:->kspview: petscviewerandformatcreate petscviewerandformatcreate proc~solve_ksp:->petscviewerandformatcreate: kspsettolerances kspsettolerances proc~solve_ksp:->kspsettolerances: kspsetoperators kspsetoperators proc~solve_ksp:->kspsetoperators: proc~solve_ksp:->mpi_wtime: kspsetfromoptions kspsetfromoptions proc~solve_ksp:->kspsetfromoptions: kspsettype kspsettype proc~solve_ksp:->kspsettype: kspgetiterationnumber kspgetiterationnumber proc~solve_ksp:->kspgetiterationnumber: kspgetpc kspgetpc proc~solve_ksp:->kspgetpc: petscfinalize petscfinalize proc~end_petsc:->petscfinalize: proc~syncwrite_log_mpidata:->mpi_barrier: proc~syncwrite_log_time:->proc~rootwrite_log: proc~syncwrite_log_time:->mpi_barrier: proc~syncwrite_log_time:->mpi_wtime: proc~exchange_prml_nodes:->mpi_barrier: proc~exchange_prml_nodes:->lcl_complex: mpi_waitall mpi_waitall proc~exchange_prml_nodes:->mpi_waitall: mpi_request_free mpi_request_free proc~exchange_prml_nodes:->mpi_request_free: num_send num_send proc~exchange_prml_nodes:->num_send: adj_proc adj_proc proc~exchange_prml_nodes:->adj_proc: num_recv num_recv proc~exchange_prml_nodes:->num_recv: proc~get_glb_indx_dual_vlm:->num_elm: proc~get_glb_indx_dual_vlm:->lcl_complex: proc~read_prml_elms2:->mpi_barrier: proc~read_prml_elms2:->num_elm: proc~read_prml_elms2:->lcl_complex: proc~read_prml_elms2:->glb_num_elm: proc~elm2proc elm2proc proc~read_prml_elms2:->proc~elm2proc: indx_offset indx_offset proc~read_prml_elms2:->indx_offset: proc~root_open_file_read root_open_file_read proc~read_prml_elms2:->proc~root_open_file_read: mpi_bcast mpi_bcast proc~read_prml_elms2:->mpi_bcast: num_pelm_pp num_pelm_pp proc~read_prml_elms2:->num_pelm_pp: proc~partition_dual:->num_elm: proc~partition_dual:->lcl_complex: proc~partition_dual:->glb_num_elm: proc~partition_dual:->indx_offset: proc~partition_dual:->mpi_bcast: proc~partition_dual:->num_pelm_pp: proc~read_nodes_prml:->num_elm: proc~read_nodes_prml:->lcl_complex: proc~read_nodes_prml:->glb_num_elm: mpi_recv mpi_recv proc~read_nodes_prml:->mpi_recv: proc~read_nodes_prml:->indx_offset: proc~read_nodes_prml:->proc~root_open_file_read: proc~read_nodes_prml:->mpi_bcast: idnint idnint proc~read_nodes_prml:->idnint: proc~read_nodes_prml:->num_pelm_pp: mpi_send mpi_send proc~read_nodes_prml:->mpi_send: proc~read_bndry_cond2:->mpi_barrier: proc~read_bndry_cond2:->num_elm: proc~read_bndry_cond2:->lcl_complex: proc~read_bndry_cond2:->glb_num_elm: proc~read_bndry_cond2:->proc~root_open_file_read: proc~read_bndry_cond2:->mpi_bcast: proc~get_connectivity:->proc~rootwrite_log: proc~get_connectivity:->mpi_barrier: proc~get_connectivity:->num_elm: proc~get_connectivity:->lcl_complex: proc~get_connectivity:->mpi_waitall: proc~get_connectivity:->mpi_request_free: proc~get_connectivity:->num_send: proc~get_connectivity:->adj_proc: proc~index_in_list index_in_list proc~get_connectivity:->proc~index_in_list: proc~get_connectivity:->num_recv: proc~calc_bndry_cobndry:->proc~syncwrite_log: proc~calc_bndry_cobndry:->proc~syncwrite_log_time: proc~calc_bndry_cobndry:->num_elm: proc~count_bndry_cobndry count_bndry_cobndry proc~calc_bndry_cobndry:->proc~count_bndry_cobndry: proc~allocate_bndry_cobndry allocate_bndry_cobndry proc~calc_bndry_cobndry:->proc~allocate_bndry_cobndry: proc~build_bndry_work_array build_bndry_work_array proc~calc_bndry_cobndry:->proc~build_bndry_work_array: proc~set_bndry_cobndry set_bndry_cobndry proc~calc_bndry_cobndry:->proc~set_bndry_cobndry: proc~get_glb_indx:->num_elm: proc~get_glb_indx:->lcl_complex: proc~read_glb_indx2 read_glb_indx2 proc~get_glb_indx:->proc~read_glb_indx2: proc~exchange_glb_indx exchange_glb_indx proc~get_glb_indx:->proc~exchange_glb_indx: proc~open_log:->mpi_bcast: proc~check_param_darcy:->proc~rootwrite_log: proc~count_bndry_cobndry:->num_elm: proc~count_bndry_cobndry:->lcl_complex: proc~any_element_in_list any_element_in_list proc~count_bndry_cobndry:->proc~any_element_in_list: proc~elm2proc:->num_pelm_pp: proc~root_open_file_read:->mpi_bcast: proc~read_glb_indx2:->proc~end_mpi: proc~read_glb_indx2:->mpi_barrier: proc~read_glb_indx2:->num_elm: proc~read_glb_indx2:->lcl_complex: proc~read_glb_indx2:->glb_num_elm: proc~read_glb_indx2:->proc~elm2proc: proc~read_glb_indx2:->indx_offset: proc~read_glb_indx2:->proc~root_open_file_read: proc~read_glb_indx2:->mpi_bcast: proc~allocate_bndry_cobndry:->num_elm: proc~allocate_bndry_cobndry:->lcl_complex: proc~build_bndry_work_array:->num_elm: proc~build_bndry_work_array:->lcl_complex: proc~set_bndry_cobndry:->num_elm: proc~set_bndry_cobndry:->lcl_complex: proc~set_bndry_cobndry:->proc~elm2proc: proc~exchange_glb_indx:->mpi_barrier: proc~exchange_glb_indx:->lcl_complex: proc~exchange_glb_indx:->mpi_waitall: proc~exchange_glb_indx:->mpi_request_free: proc~exchange_glb_indx:->num_send: proc~exchange_glb_indx:->adj_proc: proc~exchange_glb_indx:->num_recv: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents None","tags":"","loc":"program/darcy_crkp_1f.html","title":"darcy_crkp_1f – ParaGEMS"},{"text":"Uses common_mod mpi_mod partition_mod darcy_mod solver_mod program~~darcy_2f~~UsesGraph program~darcy_2f darcy_2f module~common_mod common_mod program~darcy_2f:->module~common_mod: module~mpi_mod mpi_mod program~darcy_2f:->module~mpi_mod: module~partition_mod partition_mod program~darcy_2f:->module~partition_mod: module~darcy_mod darcy_mod program~darcy_2f:->module~darcy_mod: module~solver_mod solver_mod program~darcy_2f:->module~solver_mod: petsc petsc module~common_mod:->petsc: module~mpi_mod:->module~common_mod: module~partition_mod:->module~common_mod: module~partition_mod:->module~mpi_mod: module~dec_mod dec_mod module~partition_mod:->module~dec_mod: module~io_mod io_mod module~partition_mod:->module~io_mod: module~math_mod math_mod module~partition_mod:->module~math_mod: module~darcy_mod:->module~common_mod: module~darcy_mod:->module~mpi_mod: module~darcy_mod:->module~solver_mod: module~darcy_mod:->module~io_mod: module~solver_mod:->module~common_mod: module~solver_mod:->module~mpi_mod: iso_fortran_env iso_fortran_env module~solver_mod:->iso_fortran_env: ieee_arithmetic ieee_arithmetic module~solver_mod:->ieee_arithmetic: module~dec_mod:->module~common_mod: module~dec_mod:->module~mpi_mod: module~dec_mod:->module~math_mod: module~io_mod:->module~common_mod: module~io_mod:->module~mpi_mod: module~io_mod:->module~solver_mod: module~io_mod:->iso_fortran_env: module~io_mod:->ieee_arithmetic: module~io_mod:->module~dec_mod: module~math_mod:->module~common_mod: module~math_mod:->module~mpi_mod: Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Miniapp to solve two-field Darcy flow in parallel using PETSc KSP Calls program~~darcy_2f~~CallsGraph program~darcy_2f darcy_2f proc~end_mpi end_mpi program~darcy_2f:->proc~end_mpi: proc~rootwrite_log rootwrite_log program~darcy_2f:->proc~rootwrite_log: initialise_geo initialise_geo program~darcy_2f:->initialise_geo: proc~start_mpi start_mpi program~darcy_2f:->proc~start_mpi: write_pressure_matlab write_pressure_matlab program~darcy_2f:->write_pressure_matlab: proc~initialise_darcy2 initialise_darcy2 program~darcy_2f:->proc~initialise_darcy2: write_centers_matlab write_centers_matlab program~darcy_2f:->write_centers_matlab: proc~get_rhs_darcy2 get_RHS_darcy2 program~darcy_2f:->proc~get_rhs_darcy2: proc~clean_up clean_up program~darcy_2f:->proc~clean_up: proc~solve_ksp2 solve_KSP2 program~darcy_2f:->proc~solve_ksp2: proc~end_petsc end_petsc program~darcy_2f:->proc~end_petsc: proc~parallel_setup parallel_setup program~darcy_2f:->proc~parallel_setup: write_solution_d0s2 write_solution_d0s2 program~darcy_2f:->write_solution_d0s2: proc~get_lhs_darcy2 get_LHS_darcy2 program~darcy_2f:->proc~get_lhs_darcy2: calc_whitney_c2_bc calc_whitney_c2_bc program~darcy_2f:->calc_whitney_c2_bc: proc~read_input_darcy read_input_darcy program~darcy_2f:->proc~read_input_darcy: proc~finalise_darcy finalise_darcy program~darcy_2f:->proc~finalise_darcy: write_solution_matlab write_solution_matlab program~darcy_2f:->write_solution_matlab: proc~extract_sol_ksp2 extract_sol_KSP2 program~darcy_2f:->proc~extract_sol_ksp2: proc~syncwrite_log syncwrite_log program~darcy_2f:->proc~syncwrite_log: proc~start_petsc start_petsc program~darcy_2f:->proc~start_petsc: write_prml_volumes_matlab write_prml_volumes_matlab program~darcy_2f:->write_prml_volumes_matlab: proc~end_mpi:->proc~syncwrite_log: proc~syncwrite_log_time syncwrite_log_time proc~end_mpi:->proc~syncwrite_log_time: mpi_finalize mpi_finalize proc~end_mpi:->mpi_finalize: proc~close_log close_log proc~end_mpi:->proc~close_log: proc~start_mpi:->proc~end_mpi: proc~syncwrite_log_mpidata syncwrite_log_mpidata proc~start_mpi:->proc~syncwrite_log_mpidata: mpi_barrier mpi_barrier proc~start_mpi:->mpi_barrier: mpi_comm_rank mpi_comm_rank proc~start_mpi:->mpi_comm_rank: mpi_wtime mpi_wtime proc~start_mpi:->mpi_wtime: mpi_comm_size mpi_comm_size proc~start_mpi:->mpi_comm_size: proc~open_log open_log proc~start_mpi:->proc~open_log: mpi_init mpi_init proc~start_mpi:->mpi_init: proc~initialise_darcy2:->proc~syncwrite_log: proc~initialise_darcy2:->proc~syncwrite_log_time: num_elm num_elm proc~initialise_darcy2:->num_elm: lcl_complex lcl_complex proc~initialise_darcy2:->lcl_complex: proc~initialise_darcy2:->mpi_wtime: proc~get_rhs_darcy2:->proc~syncwrite_log: bc_press bc_press proc~get_rhs_darcy2:->bc_press: proc~get_rhs_darcy2:->proc~syncwrite_log_time: proc~get_rhs_darcy2:->num_elm: matcreatenest matcreatenest proc~get_rhs_darcy2:->matcreatenest: proc~read_bndry_cond2 read_bndry_cond2 proc~get_rhs_darcy2:->proc~read_bndry_cond2: proc~get_rhs_darcy2:->lcl_complex: matgetownershiprange matgetownershiprange proc~get_rhs_darcy2:->matgetownershiprange: vecsetfromoptions vecsetfromoptions proc~get_rhs_darcy2:->vecsetfromoptions: vecassemblyend vecassemblyend proc~get_rhs_darcy2:->vecassemblyend: proc~get_rhs_darcy2:->mpi_wtime: glb_num_elm glb_num_elm proc~get_rhs_darcy2:->glb_num_elm: asub asub proc~get_rhs_darcy2:->asub: vecdestroy vecdestroy proc~get_rhs_darcy2:->vecdestroy: vecduplicate vecduplicate proc~get_rhs_darcy2:->vecduplicate: isg isg proc~get_rhs_darcy2:->isg: vecassemblybegin vecassemblybegin proc~get_rhs_darcy2:->vecassemblybegin: proc~solve_ksp2:->proc~syncwrite_log: kspgetconvergedreason kspgetconvergedreason proc~solve_ksp2:->kspgetconvergedreason: kspsolve kspsolve proc~solve_ksp2:->kspsolve: kspcreate kspcreate proc~solve_ksp2:->kspcreate: kspsetup kspsetup proc~solve_ksp2:->kspsetup: kspview kspview proc~solve_ksp2:->kspview: sub2pc_id sub2pc_id proc~solve_ksp2:->sub2pc_id: sub2ksp_id sub2ksp_id proc~solve_ksp2:->sub2ksp_id: kspgetpc kspgetpc proc~solve_ksp2:->kspgetpc: subksp_id subksp_id proc~solve_ksp2:->subksp_id: kspgetresidualnorm kspgetresidualnorm proc~solve_ksp2:->kspgetresidualnorm: pcsettype pcsettype proc~solve_ksp2:->pcsettype: kspmonitorset kspmonitorset proc~solve_ksp2:->kspmonitorset: subpc_id subpc_id proc~solve_ksp2:->subpc_id: petscviewerandformatcreate petscviewerandformatcreate proc~solve_ksp2:->petscviewerandformatcreate: kspsettolerances kspsettolerances proc~solve_ksp2:->kspsettolerances: kspsetoperators kspsetoperators proc~solve_ksp2:->kspsetoperators: pcsetup pcsetup proc~solve_ksp2:->pcsetup: proc~solve_ksp2:->mpi_wtime: kspsetfromoptions kspsetfromoptions proc~solve_ksp2:->kspsetfromoptions: kspsettype kspsettype proc~solve_ksp2:->kspsettype: pcfieldsplitsetschurfacttype pcfieldsplitsetschurfacttype proc~solve_ksp2:->pcfieldsplitsetschurfacttype: kspgetiterationnumber kspgetiterationnumber proc~solve_ksp2:->kspgetiterationnumber: pcfieldsplitsetschurpre pcfieldsplitsetschurpre proc~solve_ksp2:->pcfieldsplitsetschurpre: pcfieldsplitgetsubksp pcfieldsplitgetsubksp proc~solve_ksp2:->pcfieldsplitgetsubksp: pcfieldsplitsettype pcfieldsplitsettype proc~solve_ksp2:->pcfieldsplitsettype: proc~solve_ksp2:->isg: kspdestroy kspdestroy proc~solve_ksp2:->kspdestroy: petscfinalize petscfinalize proc~end_petsc:->petscfinalize: proc~parallel_setup:->proc~syncwrite_log: proc~parallel_setup:->proc~syncwrite_log_time: proc~exchange_prml_nodes exchange_prml_nodes proc~parallel_setup:->proc~exchange_prml_nodes: proc~get_glb_indx_dual_vlm get_glb_indx_dual_vlm proc~parallel_setup:->proc~get_glb_indx_dual_vlm: proc~read_prml_elms2 read_prml_elms2 proc~parallel_setup:->proc~read_prml_elms2: proc~partition_dual partition_dual proc~parallel_setup:->proc~partition_dual: proc~read_nodes_prml read_nodes_prml proc~parallel_setup:->proc~read_nodes_prml: proc~get_connectivity get_connectivity proc~parallel_setup:->proc~get_connectivity: proc~calc_bndry_cobndry calc_bndry_cobndry proc~parallel_setup:->proc~calc_bndry_cobndry: proc~get_glb_indx get_glb_indx proc~parallel_setup:->proc~get_glb_indx: proc~get_lhs_darcy2:->proc~syncwrite_log: proc~get_lhs_darcy2:->proc~syncwrite_log_time: proc~get_lhs_darcy2:->num_elm: proc~get_lhs_darcy2:->matcreatenest: proc~get_lhs_darcy2:->lcl_complex: matnestgetiss matnestgetiss proc~get_lhs_darcy2:->matnestgetiss: proc~get_lhs_darcy2:->mpi_wtime: proc~get_lhs_darcy2:->glb_num_elm: matsetoption matsetoption proc~get_lhs_darcy2:->matsetoption: proc~get_lhs_darcy2:->asub: proc~read_input_darcy:->proc~end_mpi: proc~read_input_darcy:->proc~rootwrite_log: proc~read_input_darcy:->proc~syncwrite_log: proc~check_param_darcy check_param_darcy proc~read_input_darcy:->proc~check_param_darcy: matdestroy matdestroy proc~finalise_darcy:->matdestroy: proc~finalise_darcy:->vecdestroy: proc~finalise_darcy:->kspdestroy: proc~extract_sol_ksp2:->proc~syncwrite_log_time: proc~extract_sol_ksp2:->num_elm: isdestroy isdestroy proc~extract_sol_ksp2:->isdestroy: vecscatterbegin vecscatterbegin proc~extract_sol_ksp2:->vecscatterbegin: vecscattercreate vecscattercreate proc~extract_sol_ksp2:->vecscattercreate: proc~extract_sol_ksp2:->lcl_complex: vecscatterend vecscatterend proc~extract_sol_ksp2:->vecscatterend: vecscatterdestroy vecscatterdestroy proc~extract_sol_ksp2:->vecscatterdestroy: proc~extract_sol_ksp2:->vecdestroy: proc~extract_sol_ksp2:->isg: proc~syncwrite_log:->mpi_barrier: proc~start_petsc:->proc~end_mpi: proc~start_petsc:->proc~syncwrite_log: petscinitialize petscinitialize proc~start_petsc:->petscinitialize: proc~syncwrite_log_mpidata:->mpi_barrier: proc~syncwrite_log_time:->proc~rootwrite_log: proc~syncwrite_log_time:->mpi_barrier: proc~syncwrite_log_time:->mpi_wtime: proc~exchange_prml_nodes:->mpi_barrier: proc~exchange_prml_nodes:->lcl_complex: mpi_waitall mpi_waitall proc~exchange_prml_nodes:->mpi_waitall: mpi_request_free mpi_request_free proc~exchange_prml_nodes:->mpi_request_free: num_send num_send proc~exchange_prml_nodes:->num_send: adj_proc adj_proc proc~exchange_prml_nodes:->adj_proc: num_recv num_recv proc~exchange_prml_nodes:->num_recv: proc~get_glb_indx_dual_vlm:->num_elm: proc~get_glb_indx_dual_vlm:->lcl_complex: proc~check_param_darcy:->proc~rootwrite_log: proc~read_prml_elms2:->mpi_barrier: proc~read_prml_elms2:->num_elm: proc~read_prml_elms2:->lcl_complex: proc~read_prml_elms2:->glb_num_elm: proc~elm2proc elm2proc proc~read_prml_elms2:->proc~elm2proc: indx_offset indx_offset proc~read_prml_elms2:->indx_offset: proc~root_open_file_read root_open_file_read proc~read_prml_elms2:->proc~root_open_file_read: mpi_bcast mpi_bcast proc~read_prml_elms2:->mpi_bcast: num_pelm_pp num_pelm_pp proc~read_prml_elms2:->num_pelm_pp: proc~partition_dual:->num_elm: proc~partition_dual:->lcl_complex: proc~partition_dual:->glb_num_elm: proc~partition_dual:->indx_offset: proc~partition_dual:->mpi_bcast: proc~partition_dual:->num_pelm_pp: proc~read_nodes_prml:->num_elm: proc~read_nodes_prml:->lcl_complex: proc~read_nodes_prml:->glb_num_elm: mpi_recv mpi_recv proc~read_nodes_prml:->mpi_recv: proc~read_nodes_prml:->indx_offset: proc~read_nodes_prml:->proc~root_open_file_read: proc~read_nodes_prml:->mpi_bcast: idnint idnint proc~read_nodes_prml:->idnint: proc~read_nodes_prml:->num_pelm_pp: mpi_send mpi_send proc~read_nodes_prml:->mpi_send: proc~read_bndry_cond2:->mpi_barrier: proc~read_bndry_cond2:->num_elm: proc~read_bndry_cond2:->lcl_complex: proc~read_bndry_cond2:->glb_num_elm: proc~read_bndry_cond2:->proc~root_open_file_read: proc~read_bndry_cond2:->mpi_bcast: proc~get_connectivity:->proc~rootwrite_log: proc~get_connectivity:->mpi_barrier: proc~get_connectivity:->num_elm: proc~get_connectivity:->lcl_complex: proc~get_connectivity:->mpi_waitall: proc~get_connectivity:->mpi_request_free: proc~get_connectivity:->num_send: proc~get_connectivity:->adj_proc: proc~index_in_list index_in_list proc~get_connectivity:->proc~index_in_list: proc~get_connectivity:->num_recv: proc~calc_bndry_cobndry:->proc~syncwrite_log: proc~calc_bndry_cobndry:->proc~syncwrite_log_time: proc~calc_bndry_cobndry:->num_elm: proc~count_bndry_cobndry count_bndry_cobndry proc~calc_bndry_cobndry:->proc~count_bndry_cobndry: proc~allocate_bndry_cobndry allocate_bndry_cobndry proc~calc_bndry_cobndry:->proc~allocate_bndry_cobndry: proc~build_bndry_work_array build_bndry_work_array proc~calc_bndry_cobndry:->proc~build_bndry_work_array: proc~set_bndry_cobndry set_bndry_cobndry proc~calc_bndry_cobndry:->proc~set_bndry_cobndry: proc~get_glb_indx:->num_elm: proc~get_glb_indx:->lcl_complex: proc~read_glb_indx2 read_glb_indx2 proc~get_glb_indx:->proc~read_glb_indx2: proc~exchange_glb_indx exchange_glb_indx proc~get_glb_indx:->proc~exchange_glb_indx: proc~open_log:->mpi_bcast: proc~count_bndry_cobndry:->num_elm: proc~count_bndry_cobndry:->lcl_complex: proc~any_element_in_list any_element_in_list proc~count_bndry_cobndry:->proc~any_element_in_list: proc~elm2proc:->num_pelm_pp: proc~root_open_file_read:->mpi_bcast: proc~read_glb_indx2:->proc~end_mpi: proc~read_glb_indx2:->mpi_barrier: proc~read_glb_indx2:->num_elm: proc~read_glb_indx2:->lcl_complex: proc~read_glb_indx2:->glb_num_elm: proc~read_glb_indx2:->proc~elm2proc: proc~read_glb_indx2:->indx_offset: proc~read_glb_indx2:->proc~root_open_file_read: proc~read_glb_indx2:->mpi_bcast: proc~allocate_bndry_cobndry:->num_elm: proc~allocate_bndry_cobndry:->lcl_complex: proc~build_bndry_work_array:->num_elm: proc~build_bndry_work_array:->lcl_complex: proc~set_bndry_cobndry:->num_elm: proc~set_bndry_cobndry:->lcl_complex: proc~set_bndry_cobndry:->proc~elm2proc: proc~exchange_glb_indx:->mpi_barrier: proc~exchange_glb_indx:->lcl_complex: proc~exchange_glb_indx:->mpi_waitall: proc~exchange_glb_indx:->mpi_request_free: proc~exchange_glb_indx:->num_send: proc~exchange_glb_indx:->adj_proc: proc~exchange_glb_indx:->num_recv: Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents None","tags":"","loc":"program/darcy_2f.html","title":"darcy_2f – ParaGEMS"},{"text":"ParaGEMS is a parallel math library for discrete exterior calculus, along with miniApps for various applications in science and engineering.","tags":"","loc":"page//index.html","title":"README – ParaGEMS"},{"text":"","tags":"","loc":"page/0about/index.html","title":"0. About – ParaGEMS"},{"text":"Prerequisites: Compilers: ParaGEMS has been compiled and tested using using both Intel (17.0 and 18.0) and GCC (8, 9 and 10) compilers. It may be possible to compile with other compilers, but this is has not be tested. NOTE: on Apple Macs you CANNOT use the built-in GCC compiler - you MUST install a complete set of GCC compilers from, for example, Homebrew. Make sure you update your PATH environment variable to point to this new installation MPI: ParaGEMS has been compiled and tested with OpenMPI 4.0.1 Python: ParaGEMS has been compiled and tested using PETSc 3.12 and python 2.7. It has also been compiled and tested with PETSc 3.14 and python 3. BLAS and LAPACK: An installation of BLAS and LAPACK is required for compiling both PETSC and ParaGEMS. PETSc: ParaGEMS has been compiled and tested using PETSc 3.12 (recommended). It has also been compiled and tested with PETSc 3.14. PETSC must be compiled with the Hypre package (--download-hypre). Installation: Enter the ParaGEMS directory, which is referred to as $(PARAGEMS_HOME) Explore the machine configuration files in config/ directory. These can be adapted for your own system. Pay particular attention to the PATH described for your PETSc installation directory. Save your configuration as [machine].inc Compilation is executed for your [machine] with the command:\n  ./make_paragemms MACHINE=[machine] If successful, the compiled ParaGEMS executables can then be found in the $(PARAGEMS_HOME)/bin directory","tags":"","loc":"page/1installation/index.html","title":"Installation – ParaGEMS"},{"text":"","tags":"","loc":"page/2formating/index.html","title":"2. Formatting Guide – ParaGEMS"},{"text":"","tags":"","loc":"page/3examples/index.html","title":"3. Examples – ParaGEMS"},{"text":"","tags":"","loc":"page/4publications/index.html","title":"4. Publications – ParaGEMS"},{"text":"","tags":"","loc":"page/5download/index.html","title":"5. Download – ParaGEMS"},{"text":"","tags":"","loc":"page/6contact/index.html","title":"6. Contact – ParaGEMS"}]}